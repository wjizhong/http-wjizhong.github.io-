# 分解成分信号
## 一、主成分分析
### 1.1 主成分分析

在多元统计分析中,主成分分析(Principal components analysis,PCA)是一种分析、简化数据集的技术。主成分分析经常用于减少数据集的维数,同时保持数据集中的对方差贡献最大的特征。这是通过保留低阶主成分,忽略高阶主成分做到的。这样低阶成分往往能够保留住数据的最重要方面。但是,这也不是一定的,要视具体应用而定。由于主成分分析依赖所给数据,所以数据的准确性对分析结果影响很大。

主成分分析[由卡尔·皮尔逊于1901年发明](http://pca.narod.ru/pearson1901.pdf),用于分析数据及建立数理模型。其方法主要是通过对协方差矩阵进行特征分解,以得出数据的主成分(即特征向量)与它们的权值(即特征值)。`PCA`是最简单的以特征量分析多元统计分布的方法。其结果可以理解为对原数据中的方差做出解释:哪一个方向上的数据值对方差的影响最大?换而言之,`PCA`提供了一种降低数据维度的有效办法;如果分析者在原数据中除掉最小的特征值所对应的成分,那么所得的低维度数据必定是最优化的(也即,这样降低维度必定是失去讯息最少的方法)。主成分分析在分析复杂数据时尤为有用,比如人脸识别。

PCA是最简单的以特征量分析多元统计分布的方法。通常情况下,这种运算可以被看作是揭露数据的内部结构,从而更好的解释数据的变量的方法。如果一个多元数据集能够在一个高维数据空间坐标系中被显现出来,那么PCA就能够提供一幅比较低维度的图像,这幅图像即为在讯息最多的点上原对象的一个‘投影’。这样就可以利用少量的主成分使得数据的维度降低了。

PCA跟因子分析密切相关,并且已经有很多混合这两种分析的统计包。而真实要素分析则是假定底层结构,求得微小差异矩阵的特征向量。

主成分分析(Principal Component Analysis)是最常用的一种降维方法,是利用在正交属性空间上的样本点,用一个超平面对所有样本进行恰当的表达。

![](http://img.blog.csdn.net/20160518114416222)

如果这里我们可以用直线的斜率和截距(k,b)来表示超平面的一些点,点在直线上的投影点(这时可以把直线看作一维的)来表示。若存在这样的超平面,则应具有的性质为

- 最近重构性:样本点到这个超平面的距离都足够近
- 最大可分性: 样本点在这个超平面上的投影能尽可能分开

假设$D=\{x_1,x_2,\dots,x_m\}$为样本集,大小为$m$.

**最近重构性:**

假定对数据样本进行了中心化,即$\Sigma_ix_i=0$,投影变换后得到的新坐标系为$\{\omega_1,\omega_2,\dots,\omega_d\}$,其中$\omega_i$是标准正交基向量:$\|\omega_i\|_2=1,\omega_i^T\omega_j=0(i\neq j)$,若丢弃新坐标系中的部分向量,则可将维度降为$d'<d$,假设现在丢弃后的坐标系为$W=\{\omega_1,\omega_2,\dots,\omega_{d'}\}$,则样本点在低维坐标系中的投影为$z_i=\{z_{i1};z_{i2};\dots;z_{id'}\}$,其中$z_{ij}=\omega_j^Tx_i$是$x_i$在新坐标系下第$j$维的坐标,若基于$z_i$来重构$x_i$,则得到$\hat{x}_i=\Sigma_{j=1}^{d'}z_{ij}\omega_j$。

对整个训练集,原样本点$x_i$与基于投影重构的样本点$\hat{x}_i$之间的距离为

$$
\begin{aligned}
    & \sum\limits_{i=1}^{m}||\sum\limits_{j=1}^{d'}z_{ij}\omega_j-x_i||_2^2 \\
    & = \sum\limits_{i=1}^{m}||Wz_i-x_i||_2^2 \\
    & = \sum\limits_{i=1}^mz_i^Tz_i-2\sum\limits_{i=1}^mz_i^TW^Tx_i+const(\sum x_i^Tx_i) \\
    & = -tr(W^T(\sum\limits_{i=1}^mx_ix_i^T)W)
\end{aligned}
$$

最后一步利用了$tr(AB)=tr(BA)$和$z_i=W^Tx_i$。根据最近重构性,可得优化目标函数

$$
\begin{aligned}
    &\min\limits_{W}\quad -tr(W^TXX^TW) \\
    &s.t.\quad W^TW=I
\end{aligned}
$$

其中$\omega_j$是标准正交基,$\sum_ix_ix_i^T$是协方差矩阵。

**最大可分性:**

样本点$x_i$在新空间上的投影为$W^Tx_i$,若所有样本点能近可能分开,则应该使投影后样本的方差最大,即$\sum_ix_i^TWW^Tx_i$,由$x_i^TWW^Tx_i=tr(W^Tx_ix_i^TW)$,可知优化目标函数为

$$
\begin{aligned}
    &\max\limits_W \quad tr(W^TXX^TW) \\
    &s.t. \quad W^TW=I
\end{aligned}
$$

**求解优化函数:**

$$
\begin{aligned}
    &\max\limits_W \quad tr(W^TXX^TW) \\
    &s.t. \quad W^TW=I
\end{aligned}
$$

假设$d'=1$,构造Lagrange函数,可以得到

$$f(\omega)=\omega^TXX^T\omega+\lambda(\omega^T\omega)$$

对$\omega$求导,得到$XX^T\omega=\lambda\omega$,可以知道$\omega$为$XX^T$的特征向量。

假设$d'>1$,构造Lagrange函数,可以得到

$$
f(\omega_1,\cdots,\omega_{d'})=\sum\limits_{i=1}^{d'}\omega_i^TXX^T\omega_i+\lambda_{ij}(\omega_i^T\omega_j-\delta_{ij})
$$

对$\omega_i$求导,得到

$$
XX^T\omega_i=\lambda_{ii}\omega_i+(\lambda_{ij}+\lambda_{ji})\omega_j
$$

从PCA的目标,我们知道,我们要确定一个超平面来表示$x_1,\cdots,x_n$,但是表示这个超平面的基向量可以不唯一,但是这个超平面必须是唯一的,由上面公式可知表示这个超平面的基向量可以互相转换,而且$XX^T$在这个超平面内恰有$d'$个特征向量,所以我们可以用$XX^T$的特征向量来表示这个超平面。即优化函数的解为

$$
XX^T\omega=\lambda \omega
$$

**`python`实现**

输入:样本集$D=\{x_1,\dots ,x_m\}$,低维空间维数$d'$

算法过程:

 1. 对所有样本进行中心化$x_i\leftarrow \frac{1}{m}\sum_{i=1}^mx_i$;
 2. 计算样本的协方差$XX^T$;
 3. 对协方差$XX^T$做特征值分解;
 4. 取最大的$d'$个特征值所对应的特征向量$\omega_1,\omega_2,\dots,\omega_{d'}$;

输出:投影矩阵$W=\{\omega_1,\dots,\omega_{d'}\}$

```python
import numpy as np
import matplotlib.pyplot as plt

# PCA获取主特征向量和均值
def PCA(date,args=1):
    mean=np.sum(date,1)/mat.shape[1];
    temp_mat=date-np.outer(mean,np.ones(mat.shape[1]));    # 中心化的矩阵
    date_shape=temp_mat.shape
    if date_shape[0]>date_shape[1]:
        covar_mat=np.dot(temp_mat.T,temp_mat);      # 计算协方差
        eigval,eigvec = np.linalg.eig(covar_mat);   # 计算特征值和特征向量
        eigvec=np.dot(covar_mat,eigvec)
    else:
        covar_mat=np.dot(temp_mat,temp_mat.T)
        eigval,eigvec = np.linalg.eig(covar_mat);

    indices=np.argsort(eigval)[::-1];           # 得到特征值降序排列的下标
    eigval=eigval[indices];                     # 得到排序的特征值
    eigvec=eigvec[:,indices];                     # 得到排序的特征向量
    if args>=1:
        return eigvec[:,0:int(args)],mean
    else:
        temp_sum,d,eig_sum=0,0,np.sum(eigval)
        while temp_sum/eig_sum<args:
            temp_sum=temp_sum+eigval[d]
            d=d+1
        return eigvec[:,0:d],mean;

# 降维获取相应的坐标
def dime_reduction(date,eigvec,mean):
    return np.dot(vec.T,date-np.outer(mean,np.ones(date.shape[1])))

# 测试数据
x,y=np.random.multivariate_normal([10,10],[[2,1],[1,2]],1000).T
mat=np.array([list(x),list(y)]);
vec,mean=PCA(mat,1);
indice=dime_reduction(mat,vec,mean)
PCA_date=np.outer(vec,indice)+np.outer(mean,np.ones(mat.shape[1]))
plt.plot(mat[0],mat[1],'r.')
plt.plot(PCA_date[0],PCA_date[1],'g')
plt.show()
```

其中$d'$还可以由其他方法获得,如设置重构阈值t=95%,s使得$\frac{\sum_{i=1}^{d'}\lambda_i}{\sum_{i=1}^d\lambda_i}>t$成立的最小$d'$.

*注意:* 降维导致了舍弃了一部分信息,这往往是必要的,一是舍弃这部分信息后,使得样本的采样密度增大(降维的重要动机);二是最小的特征值所对应的特征值往往与噪声有关,将他们舍弃在一定程度上起到去噪的作用.


**PCA数学理解:**

PCA的数学定义是:一个正交化线性变换,把数据变换到一个新的坐标系统中,使得这一数据的任何投影的第一大方差在第一个坐标(称为第一主成分)上,第二大方差在第二个坐标(第二主成分)上,依次类推。

定义一个$n×m$的矩阵,$X^T$为去平均值(以平均值为中心移动至原点)的数据,其行为数据样本,列为数据类别(注意:这里定义的是$X^T$,而不是$X$)。则$X$的奇异值分解为$X = W\Sigma V^T$,其中$m×m$矩阵$W$是$XX^T$的本征矢量矩阵,$\Sigma$是$m×n$的非负矩形对角矩阵,$V$是$n×n$的$X^TX$的本征矢量矩阵。据此,

$$
\begin{aligned}
    {\boldsymbol {Y}}^{\top }&={\boldsymbol {X}}^{\top }{\boldsymbol {W}}\\
    &={\boldsymbol {V}}{\boldsymbol {\Sigma }}^{\top }{\boldsymbol {W}}^{\top }{\boldsymbol {W}}\\
    &={\boldsymbol {V}}{\boldsymbol {\Sigma }}^{\top }\end{aligned}
$$

当$m<n-1$时,$V$在通常情况下不是唯一定义的,而$Y$则是唯一定义的。$W$是一个正交矩阵,$Y^TW^T=X^T$,且$Y^T$的第一列由第一主成分组成,第二列由第二主成分组成,依此类推。

为了得到一种降低数据维度的有效办法,我们可以利用$W_L$把$X$映射到一个只应用前面L个向量的低维空间中去:

$$
\mathbf {Y} =\mathbf {W_{L}} ^{\top }\mathbf {X} =\mathbf {\Sigma _{L}} \mathbf {V} ^{\top }
$$

其中$\mathbf {\Sigma _{L}} =\mathbf {I} _{L\times m}\mathbf {\Sigma }$,且 $\mathbf {I} _{L\times m}$为 $L\times m$的单位矩阵。

$X$的单向量矩阵$W$相当于协方差矩阵的本征矢量$C = X X^T$,
$\mathbf {X} \mathbf {X} ^{\top }=\mathbf {W} \mathbf {\Sigma } \mathbf {\Sigma } ^{\top }\mathbf {W} ^{\top }$

在欧几里得空间给定一组点数,第一主成分对应于通过多维空间平均点的一条线,同时保证各个点到这条直线距离的平方和最小。去除掉第一主成分后,用同样的方法得到第二主成分。依此类推。在$\Sigma$中的奇异值均为矩阵$XX^T$的本征值的平方根。每一个本征值都与跟它们相关的方差是成正比的,而且所有本征值的总和等于所有点到它们的多维空间平均点距离的平方和。PCA提供了一种降低维度的有效办法,本质上,它利用正交变换将围绕平均点的点集中尽可能多的变量投影到第一维中去,因此,降低维度必定是失去讯息最少的方法。PCA具有保持子空间拥有最大方差的最优正交变换的特性。然而,当与离散余弦变换相比时,它需要更大的计算需求代价。非线性降维技术相对于PCA来说则需要更高的计算要求。

PCA对变量的缩放很敏感。如果我们只有两个变量,而且它们具有相同的样本方差,并且成正相关,那么PCA将涉及两个变量的主成分的旋转。但是,如果把第一个变量的所有值都乘以100,那么第一主成分就几乎和这个变量一样,另一个变量只提供了很小的贡献,第二主成分也将和第二个原始变量几乎一致。这就意味着当不同的变量代表不同的单位(如温度和质量)时,PCA是一种比较武断的分析方法。但是在Pearson的题为 "On Lines and Planes of Closest Fit to Systems of Points in Space"的原始文件里,是假设在欧几里得空间里不考虑这些。一种使PCA不那么武断的方法是使用变量缩放以得到单位方差。


通常,为了确保第一主成分描述的是最大方差的方向,我们会使用平均减法进行主成分分析。如果不执行平均减法,第一主成分有可能或多或少的对应于数据的平均值。另外,为了找到近似数据的最小均方误差,我们必须选取一个零均值。

假设零经验均值,数据集$X$ 的主成分$w_1$可以被定义为:

$$
\mathbf{w}_1=\underset{\Vert \mathbf{w} \Vert = 1}{\operatorname{\arg\,max}}\,\operatorname{Var}\{ \mathbf{w}^\top \mathbf{X} \}
    = \underset{\Vert \mathbf{w} \Vert = 1}{\operatorname{\arg\,max}}\,E\left\{ \left( \mathbf{w}^\top \mathbf{X}\right)^2 \right\}
$$

为了得到第k个主成分,必须先从X中减去前面的$k-1$个主成分:

$$
\mathbf{\hat{X}}_{k-1}=\mathbf{X}-\sum_{i = 1}^{k - 1}\mathbf{w}_i \mathbf{w}_i^\top \mathbf{X}
$$

然后把求得的第k个主成分带入数据集,得到新的数据集,继续寻找主成分。

$$
\mathbf{w}_k= \underset{\Vert \mathbf{w} \Vert = 1}{\operatorname{arg\,max}}\,E\left\{\left( \mathbf{w}^\top \mathbf{\hat{X}}_{k-1}\right)^2 \right\}.
$$

PCA相当于在气象学中使用的经验正交函数(EOF),同时也类似于一个线性隐层神经网络。隐含层K个神经元的权重向量收敛后,将形成一个由前K个主成分跨越空间的基础。但是与PCA不同的是,这种技术并不一定会产生正交向量。

PCA是一种很流行且主要的的模式识别技术。然而,它并不能最优化类别可分离性 。另一种不考虑这一点的方法是线性判别分析。

## 1.2 增量PCA (Incremental PCA)
PCA 对象非常有用, 但 针对大型数据集的应用, 仍然具有一定的限制。 最大的限制是 PCA 仅支持批处理，这意味着所有要处理的数据必须放在主内存。 IncrementalPCA 对象使用不同的处理形式, 即允许部分计算以小型批处理方式处理数据的方法进行, 而得到和 PCA 算法差不多的结果。 IncrementalPCA 可以通过以下方式实现核外（out-of-core）主成分分析：

基于从本地硬盘或网络数据库中连续获取的数据块之上, 使用 partial_fit 方法。
在 memory mapped file (通过 numpy.memmap 创建)上使用 fit 方法。
IncrementalPCA 类为了增量式的更新 explainedvariance_ratio ，仅需要存储估计出的分量和噪声方差。 这就是为什么内存使用量依赖于每个批次的样本数量，而不是数据集中需要处理的样本总量。

在应用SVD之前，IncrementalPCA就像PCA一样,为每个特征聚集而不是缩放输入数据。

http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_incremental_pca_0011.pnghttp://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_incremental_pca_0021.png

示例

Incremental PCA


2.5.1.3. 基于随机化SVD的PCA
通过丢弃具有较低奇异值的奇异向量的分量，将数据降维到低维空间并保留大部分方差信息是非常有意义的。

例如，如果我们使用64x64像素的灰度级图像进行人脸识别，数据的维数为4096，在这样大的数据上训练含RBF内核的支持向量机是很慢的。此外我们知道这些数据固有维度远低于4096，因为人脸的所有照片都看起来有点相似。样本位于较低维度的流体上（例如约200维）。 PCA算法可以用于线性变换数据，同时降低维数并同时保留大部分可描述的方差信息。

在这种情况下，使用可选参数 svd_solver='randomized' 的 PCA 是非常有用的。既然我们将要丢弃大部分奇异值，那么仅仅就实际转换中所需的奇异向量进行计算就可以使得 PCA 计算过程变得异常有效。

例如：以下显示了来自 Olivetti 数据集的 16 个样本肖像（以 0.0 为中心）。右侧是前 16 个奇异向量重画的肖像。因为我们只需要使用大小为 n_{samples} = 400 和 n_{features} = 64 \times 64 = 4096 的数据集的前 16 个奇异向量, 使得计算时间小于 1 秒。

orig_img pca_img

注意：使用可选参数 svd_solver='randomized' ，在 PCA 中我们还需要给出输入低维空间大小 n_components 。

我们注意到, 如果 n_{\max} = \max(n_{\mathrm{samples}}, n_{\mathrm{features}}) 且 n_{\min} = \min(n_{\mathrm{samples}}, n_{\mathrm{features}}), 对于PCA中的实现，随机 PCA 的时间复杂度是：O(n_{\max}^2 \cdot n_{\mathrm{components}}), 而不是 O(n_{\max}^2 \cdot n_{\min}) 。

就内部实现的方法而言, 随机 PCA 的内存占用量和 2 \cdot n_{\max} \cdot n_{\mathrm{components}}, 而不是 n_{\max}\cdot n_{\min} 成正比。

注意：选择参数 svd_solver='randomized' 的 PCA 的 inverse_transform 的实现, 并不是对应 transform 的逆变换（即使 参数设置为默认的 whiten=False）

示例:

Faces recognition example using eigenfaces and SVMs
Faces dataset decompositions
参考资料:

“Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions” Halko, et al., 2009
2.5.1.4. 核 PCA
KernelPCA 是 PCA 的扩展，通过使用核方法实现非线性降维（dimensionality reduction） (参阅 成对的矩阵, 类别和核函数)。 它具有许多应用，包括去噪, 压缩和结构化预测（ structured prediction ） (kernel dependency estimation（内核依赖估计）)。 KernelPCA 支持 transform 和 inverse_transform 。

http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_kernel_pca_0011.png

示例:

Kernel PCA
2.5.1.5. 稀疏主成分分析 ( SparsePCA 和 MiniBatchSparsePCA )
SparsePCA 是 PCA 的一个变体，目的是提取能最大程度得重建数据的稀疏分量集合。

小批量稀疏 PCA ( MiniBatchSparsePCA ) 是一个 SparsePCA 的变体，它速度更快但准确度有所降低。对于给定的迭代次数，通过迭代该组特征的小块来达到速度的增加。

Principal component analysis（主成分分析） (PCA) 的缺点在于，通过该方法提取的成分具有独占的密度表达式，即当表示为原始变量的线性组合时，它们具有非零系数，使之难以解释。在许多情况下，真正的基础分量可以被更自然地想象为稀疏向量; 例如在面部识别中，每个分量可能自然地映射到面部的某个部分。

稀疏的主成分产生更节约、可解释的表达式，明确强调了样本之间的差异性来自哪些原始特征。

以下示例说明了使用稀疏 PCA 提取 Olivetti 人脸数据集中的 16 个分量。可以看出正则化项产生了许多零。此外，数据的自然结构导致了非零系数垂直相邻 （vertically adjacent）。该模型并不具备纯数学意义的执行: 每个分量都是一个向量 h \in \mathbf{R}^{4096}, 除非人性化地的可视化为 64x64 像素的图像，否则没有垂直相邻性的概念。下面显示的分量看起来局部化（appear local)是数据的内在结构的影响，这种局部模式使重建误差最小化。有一种考虑到邻接性和不同结构类型的导致稀疏的规范（sparsity-inducing norms）,参见 [Jen09] 对这种方法进行了解。有关如何使用稀疏 PCA 的更多详细信息，请参阅下面的示例部分。

pca_img spca_img

请注意，有多种不同的计算稀疏PCA 问题的公式。 这里使用的方法基于 [Mrl09] 。对应优化问题的解决是一个带有惩罚项（L1范数的） \ell_1 的 PCA 问题（dictionary learning（字典学习））:

(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} & \frac{1}{2}||X-UV||_2^2+\alpha||V||_1 \\\text{subject to\,} & ||U_k||_2 = 1 \text{ for all }0 \leq k < n_{components}

稀疏推导（sparsity-inducing） \ell_1 规范也可以当训练样本很少时,避免从噪声中拟合分量。可以通过超参数 alpha 来调整惩罚程度（或称稀疏度）。值较小会导致温和的正则化因式分解，而较大的值将许多系数缩小到零。

注意 虽然本着在线算法的精神， MiniBatchSparsePCA 类不实现 partial_fit , 因为在线算法是以特征为导向，而不是以样本为导向。

示例:

Faces dataset decompositions
参考资料:

[Mrl09] “Online Dictionary Learning for Sparse Coding” J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009
[Jen09] “Structured Sparse Principal Component Analysis” R. Jenatton, G. Obozinski, F. Bach, 2009




