<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="author" content="wjizhong">
    <link rel="canonical" href="https://wjizhong.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">
    <link rel="shortcut icon" href="https://pic.pngsucai.com/00/18/26/4a7884c36067e596.jpg">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>深度学习基础 - 图像/视频算法</title>
    <link href="../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../js/jquery-3.2.1.min.js"></script>
    <script src="../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840", url: "#_top", level:1, children: [
              {title: "\u4e00\u3001\u8bc4\u4f30\u6307\u6807", url: "#_2", level:2, children: [
                  {title: "1.1 \u56de\u5f52(Regression)\u7b97\u6cd5\u6307\u6807", url: "#11-regression", level:3, children: [
                  ]},
                  {title: "1.2 \u5e38\u89c1\u7684\u8ddd\u79bb", url: "#12", level:3, children: [
                  ]},
                  {title: "1.3 \u5206\u7c7b(Classification)\u6307\u6807", url: "#13-classification", level:3, children: [
                  ]},
                  {title: "5.3 \u805a\u7c7b\u6307\u6807", url: "#53", level:3, children: [
                  ]}, 
              ]},
              {title: "\u4e09\u3001\u7ecf\u5178\u7f51\u7edc", url: "#_3", level:2, children: [
                  {title: "3.1 lenet\u7f51\u7edc", url: "#31-lenet", level:3, children: [
                  ]}, 
              ]},
          ]},
        ];
    </script>
    <script src="../../js/base.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    <style>
blockquote{
    font-size: 99%;
}
</style>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  CommonHTML: {
    scale: 100
  }
});
</script>
<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    
    
      
    

    

    <h1 id="_1">深度学习基础</h1>
<h2 id="_2">一、评估指标</h2>
<p>代价函数:<span><span class="MathJax_Preview">f(\theta,y)</span><script type="math/tex">f(\theta,y)</script></span>,又称Cost function,loss function objective function。一般用在训练过程中,用来定义预测值和真实值之间的距离(也就是衡量模型在训练集上的性能),作为模型调整参数的反馈。代价函数越小,模型性能越好。</p>
<p>评判指标:<span><span class="MathJax_Preview">f(\hat y,y)</span><script type="math/tex">f(\hat y,y)</script></span>,一般用于训练和测试过程中,用于评估模型好坏。评判指标越大(或越小),模型越好。</p>
<p>本质上代价函数和评判指标都是一家人,只他们的应用场景不同,分工不同。代价函数是用来优化模型参数的,评价指标是用来评判模型好坏的。</p>
<p>作为代价函数所具备的条件:</p>
<pre><code>函数光滑且可导:可用梯度下降求解极值
函数为凸函数:可用梯度下降求解最优解
......
</code></pre>

<p>例如我们经常使用的分类器评判指标AUC就不能直接被优化,因此我们常采用交叉熵来代替AUC进行优化。一般情况下,交叉熵越小,AUC就会越大。</p>
<h3 id="11-regression">1.1 回归(Regression)算法指标</h3>
<p><img src="http://pic3.zhimg.com/v2-32c48e2a576d9c2157a071f05e2d6d7a_r.jpg" style="width: 35%"></p>
<ul>
<li><strong>平均绝对误差(Mean Absolute Error)</strong></li>
</ul>
<p>平均绝对误差MAE(Mean Absolute Error)又被称为l1范数损失(l1-norm loss):</p>
<div>
<div class="MathJax_Preview">{\rm MAE}(y, \hat{y})=\frac{1}{m}\sum\limits_{i=1}^{n}|y_i-\hat{y}_i|</div>
<script type="math/tex; mode=display">{\rm MAE}(y, \hat{y})=\frac{1}{m}\sum\limits_{i=1}^{n}|y_i-\hat{y}_i|</script>
</div>
<p>MAE不足:MAE虽能较好衡量回归模型的好坏,但是绝对值的存在导致函数不光滑,在某些点上不能求导,可以考虑将绝对值改为残差的平方,这就是均方误差。</p>
<pre><code class="python">def mean_absolute_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average'):
    &quot;&quot;&quot;Mean absolute error regression loss
    Parameters
        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
            Ground truth (correct) target values.
        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
            Estimated target values.
        sample_weight : array-like of shape (n_samples,), optional
            Sample weights.
        multioutput : string in ['raw_values', 'uniform_average'] or array-like of shape (n_outputs)
            Defines aggregating of multiple output values.
            'raw_values' : Returns a full set of errors in case of multioutput input.
            'uniform_average' : Errors of all outputs are averaged with uniform weight.
    Returns
        loss : float or ndarray of floats
            If multioutput is 'raw_values', then mean absolute error is returned for each output separately.
            If multioutput is 'uniform_average' or an ndarray of weights, then the weighted average of all output errors is returned.
            MAE output is non-negative floating point. The best value is 0.0.
    Examples
        &gt;&gt;&gt; from sklearn.metrics import mean_absolute_error
        &gt;&gt;&gt; y_true = [3, -0.5, 2, 7]
        &gt;&gt;&gt; y_pred = [2.5, 0.0, 2, 8]
        &gt;&gt;&gt; mean_absolute_error(y_true, y_pred)
        0.5
        &gt;&gt;&gt; y_true = [[0.5, 1], [-1, 1], [7, -6]]
        &gt;&gt;&gt; y_pred = [[0, 2], [-1, 2], [8, -5]]
        &gt;&gt;&gt; mean_absolute_error(y_true, y_pred)
        0.75
        &gt;&gt;&gt; mean_absolute_error(y_true, y_pred, multioutput='raw_values')
        array([0.5, 1. ])
        &gt;&gt;&gt; mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])
        0.85...
    &quot;&quot;&quot;
    y_type, y_true, y_pred, multioutput = _check_reg_targets(y_true, y_pred, multioutput)
    check_consistent_length(y_true, y_pred, sample_weight)
    output_errors = np.average(np.abs(y_pred - y_true), weights=sample_weight, axis=0)
    if isinstance(multioutput, str):
        if multioutput == 'raw_values':
            return output_errors
        elif multioutput == 'uniform_average':
            # pass None as weights to np.average: uniform mean
            multioutput = None

    return np.average(output_errors, weights=multioutput)
</code></pre>

<ul>
<li><strong>均方误差(Mean Squared Error)</strong></li>
</ul>
<p>均方误差MSE(Mean Squared Error)又被称为l2范数损失(l2-norm loss):</p>
<div>
<div class="MathJax_Preview">{\rm MSE}(y, \hat{y})=\frac{1}{m}\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2</div>
<script type="math/tex; mode=display">{\rm MSE}(y, \hat{y})=\frac{1}{m}\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2</script>
</div>
<p>MSE和方差的性质比较类似,与我们的目标变量的量纲不一致,为了保证量纲一致性,我们需要对MSE进行开方得到RMSE。</p>
<ul>
<li><strong>均方根误差(Root Mean Squared Error)</strong></li>
</ul>
<p>开方之后的MSE称为RMSE,是标准差的表兄弟,如下式所示:</p>
<div>
<div class="MathJax_Preview">{\rm RMSE}(y, \hat{y})=\sqrt {\frac{1}{m}\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2}</div>
<script type="math/tex; mode=display">{\rm RMSE}(y, \hat{y})=\sqrt {\frac{1}{m}\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2}</script>
</div>
<p>上面的几种衡量标准的取值大小与具体的应用场景有关系,很难定义统一的规则来衡量模型的好坏。比如说利用机器学习算法预测上海的房价RMSE在2000元,我们是可以接受的,但是当四五线城市的房价RMSE为2000元,我们还可以接受吗?</p>
<pre><code class="python">def mean_squared_error(y_true, y_pred, *, sample_weight=None,multioutput='uniform_average', squared=True):
    &quot;&quot;&quot;Mean squared error regression loss
    Parameters
        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
            Ground truth (correct) target values.
        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
            Estimated target values.
        sample_weight : array-like of shape (n_samples,), optional
            Sample weights.
        multioutput : string in ['raw_values', 'uniform_average'] or array-like of shape (n_outputs)
            Defines aggregating of multiple output values.
                Array-like value defines weights used to average errors.
                'raw_values' : Returns a full set of errors in case of multioutput input.
                'uniform_average' : Errors of all outputs are averaged with uniform weight.
        squared : boolean value, optional (default = True)
            If True returns MSE value, if False returns RMSE value.
    Returns
        loss : float or ndarray of floats
            A non-negative floating point value (the best value is 0.0), or an array of floating point values, one for each individual target.
    Examples
        &gt;&gt;&gt; from sklearn.metrics import mean_squared_error
        &gt;&gt;&gt; y_true = [3, -0.5, 2, 7]
        &gt;&gt;&gt; y_pred = [2.5, 0.0, 2, 8]
        &gt;&gt;&gt; mean_squared_error(y_true, y_pred)
        0.375
        &gt;&gt;&gt; y_true = [3, -0.5, 2, 7]
        &gt;&gt;&gt; y_pred = [2.5, 0.0, 2, 8]
        &gt;&gt;&gt; mean_squared_error(y_true, y_pred, squared=False)
        0.612...
        &gt;&gt;&gt; y_true = [[0.5, 1],[-1, 1],[7, -6]]
        &gt;&gt;&gt; y_pred = [[0, 2],[-1, 2],[8, -5]]
        &gt;&gt;&gt; mean_squared_error(y_true, y_pred)
        0.708...
        &gt;&gt;&gt; mean_squared_error(y_true, y_pred, multioutput='raw_values')
        array([0.41666667, 1.        ])
        &gt;&gt;&gt; mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])
        0.825...
    &quot;&quot;&quot;
    y_type, y_true, y_pred, multioutput = _check_reg_targets(y_true, y_pred, multioutput)
    check_consistent_length(y_true, y_pred, sample_weight)
    output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)
    if isinstance(multioutput, str):
        if multioutput == 'raw_values':
            return output_errors if squared else np.sqrt(output_errors)
        elif multioutput == 'uniform_average':
            # pass None as weights to np.average: uniform mean
            multioutput = None

    mse = np.average(output_errors, weights=multioutput)
    return mse if squared else np.sqrt(mse)
</code></pre>

<ul>
<li><strong>决定系数(Coefficient of determination)</strong></li>
</ul>
<p>变量之所以有价值,就是因为变量是变化的。什么意思呢?比如说一组因变量为[0,0,0,0,0],显然该因变量的结果是一个常数0,我们也没有必要建模对该因变量进行预测。假如一组的因变量为[1,3,7,10,12],该因变量是变化的,也就是有变异,因此需要通过建立回归模型进行预测。这里的变异可以理解为一组数据的方差不为0。</p>
<p>决定系数又称为<span><span class="MathJax_Preview">R^2</span><script type="math/tex">R^2</script></span> score,反应因变量的全部变异能通过回归关系被自变量解释的比例。</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
    &amp; \text{SST} = \sum \limits_i^m(y_i - \bar y)^2 \qquad \text{SST = total sum of squares} \\
    &amp; \text{SSR} = \sum \limits_i^m(\hat y_i - \bar y)^2 \qquad \text{SSR = sum of due to regression} \\
    &amp; \text{SSE} = \sum \limits_i^m(\hat y_i - y_i)^2 \qquad \text{SSE = sum of due to erros} \\
    &amp; \text{SST = SSR + SSE} \\
    &amp; R^2(y,\hat{y})= \frac{\rm SSR}{\rm SST}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
    & \text{SST} = \sum \limits_i^m(y_i - \bar y)^2 \qquad \text{SST = total sum of squares} \\
    & \text{SSR} = \sum \limits_i^m(\hat y_i - \bar y)^2 \qquad \text{SSR = sum of due to regression} \\
    & \text{SSE} = \sum \limits_i^m(\hat y_i - y_i)^2 \qquad \text{SSE = sum of due to erros} \\
    & \text{SST = SSR + SSE} \\
    & R^2(y,\hat{y})= \frac{\rm SSR}{\rm SST}
\end{aligned}
</script>
</div>
<p>如果结果是0,就说明模型预测不能预测因变量。如果结果是1。就说明是函数关系。如果结果是0~1之间的数,就是我们模型的好坏程度。化简上面的公式,分子就变成了我们的均方误差MSE,下面分母就变成了方差:</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
    R^2(y,\hat{y}) &amp;= 1 - \frac{\rm SSE}{\rm SST}=1-\frac{\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2}{\sum\limits_{i=1}^{m}(y_i-\bar{y})^2} \\
    &amp;=1-\frac{\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2/m}{\sum\limits_{i=1}^{m}(y_i-\bar{y})^2/m}= 1 - \frac{\rm MSE(\hat y, y)}{\rm Var(y)} 
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
    R^2(y,\hat{y}) &= 1 - \frac{\rm SSE}{\rm SST}=1-\frac{\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2}{\sum\limits_{i=1}^{m}(y_i-\bar{y})^2} \\
    &=1-\frac{\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2/m}{\sum\limits_{i=1}^{m}(y_i-\bar{y})^2/m}= 1 - \frac{\rm MSE(\hat y, y)}{\rm Var(y)} 
\end{aligned}
</script>
</div>
<p>以上的评估指标是基于误差的均值对进行评估的,均值对异常点(outliers)较敏感,如果样本中有一些异常值出现,会对以上指标的值有较大影响,即均值是非鲁棒的。</p>
<ul>
<li><strong>解决评估指标鲁棒性问题</strong></li>
</ul>
<p>我们通常用一下两种方法解决评估指标的鲁棒性问题:</p>
<blockquote>
<p>剔除异常值:设定一个相对误差<span><span class="MathJax_Preview">\frac{|y_i-\hat{y_i}|}{y_i}</span><script type="math/tex">\frac{|y_i-\hat{y_i}|}{y_i}</script></span>,当该值超过一定的阈值时,则认为其是一个异常点,剔除这个异常点,将异常点剔除之后。再计算平均误差来对模型进行评价。</p>
<p>使用误差的分位数来代替:如利用中位数来代替平均数。例如MAPE:<span><span class="MathJax_Preview">MAPE=median(|y_i-\hat{y_i}|/y_i)</span><script type="math/tex">MAPE=median(|y_i-\hat{y_i}|/y_i)</script></span>,MAPE是一个相对误差的中位数,当然也可以使用别的分位数。</p>
</blockquote>
<p>https://zhuanlan.zhihu.com/p/38529433</p>
<h3 id="12">1.2 常见的距离</h3>
<p>在机器学习里,我们的运算一般都是基于向量的,一条用户具有100个特征,那么他对应的就是一个100维的向量,通过计算两个用户对应向量之间的距离值大小,有时候能反映出这两个用户的相似程度。这在后面的KNN算法和K-means算法中很明显。</p>
<p>一般而言,定义一个距离函数<span><span class="MathJax_Preview">d(x,y)</span><script type="math/tex">d(x,y)</script></span>,需要满足下面几个准则:</p>
<blockquote>
<ol>
<li>
<p><span><span class="MathJax_Preview">d(x,x)=0</span><script type="math/tex">d(x,x)=0</script></span>,到自己的距离为0</p>
</li>
<li>
<p><span><span class="MathJax_Preview">d(x,y)&gt;= 0</span><script type="math/tex">d(x,y)>= 0</script></span>,距离非负</p>
</li>
<li>
<p><span><span class="MathJax_Preview">d(x,y)=d(y,x)</span><script type="math/tex">d(x,y)=d(y,x)</script></span>,对称性,如果A到B距离是a,那么B到A的距离也应该是a</p>
</li>
<li>
<p><span><span class="MathJax_Preview">d(x,k)+d(k,y)&gt;= d(x,y)</span><script type="math/tex">d(x,k)+d(k,y)>= d(x,y)</script></span>,三角形法则:(两边之和大于第三边)</p>
</li>
</ol>
</blockquote>
<p>设有两个n维变量<span><span class="MathJax_Preview">A=\left[ x_{11}, x_{12},...,x_{1n} \right]</span><script type="math/tex">A=\left[ x_{11}, x_{12},...,x_{1n} \right]</script></span>和<span><span class="MathJax_Preview">B=\left[ x_{21} ,x_{22} ,...,x_{2n} \right]</span><script type="math/tex">B=\left[ x_{21} ,x_{22} ,...,x_{2n} \right]</script></span>,则一些常用的距离公式定义如下:</p>
<ul>
<li><strong>曼哈顿距离</strong></li>
</ul>
<p>曼哈顿距离也称为城市街区距离,数学定义如下:</p>
<div>
<div class="MathJax_Preview">d_{12} =\sum_{k=1}^{n}{\left| x_{1k}-x_{2k} \right| } </div>
<script type="math/tex; mode=display">d_{12} =\sum_{k=1}^{n}{\left| x_{1k}-x_{2k} \right| } </script>
</div>
<p>曼哈顿距离的python实现:</p>
<pre><code class="python">from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print(sum(abs(vector1-vector2)))
</code></pre>

<ul>
<li><strong>欧氏距离</strong></li>
</ul>
<p>欧氏距离其实就是L2范数,数学定义如下:</p>
<div>
<div class="MathJax_Preview">
d_{12} =\sqrt{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{2} } } 
</div>
<script type="math/tex; mode=display">
d_{12} =\sqrt{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{2} } } 
</script>
</div>
<p>欧氏距离的Python实现:</p>
<pre><code class="python">from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print(sqrt((vector1-vector2)*(vector1-vector2).T))
</code></pre>

<ul>
<li><strong>切比雪夫距离</strong></li>
</ul>
<p>切比雪夫距离就是<span><span class="MathJax_Preview">L_{\infty}</span><script type="math/tex">L_{\infty}</script></span>,即无穷范数,数学表达式如下:</p>
<div>
<div class="MathJax_Preview">
d_{12} =max\left( \left| x_{1k}-x_{2k} \right| \right)
</div>
<script type="math/tex; mode=display">
d_{12} =max\left( \left| x_{1k}-x_{2k} \right| \right)
</script>
</div>
<p>切比雪夫距离额Python实现如下:</p>
<pre><code class="python">from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print(sqrt(abs(vector1-vector2).max))
</code></pre>

<ul>
<li><strong>闵可夫斯基距离</strong></li>
</ul>
<p>从严格意义上讲,闵可夫斯基距离不是一种距离,而是一组距离的定义:</p>
<div>
<div class="MathJax_Preview">
d_{12} =\sqrt[p]{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{p} } } 
</div>
<script type="math/tex; mode=display">
d_{12} =\sqrt[p]{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{p} } } 
</script>
</div>
<p>该距离最常用的p是2和1,前者是欧几里得距离(Euclidean distance),后者是曼哈顿距离(Manhattan distance)。假设在曼哈顿街区乘坐出租车从P点到Q点,白色表示高楼大厦,灰色表示街道:</p>
<p><img alt="" src="http://images.cnitblog.com/blog/533521/201308/07220530-1c87c470c5984305932cb5f5fc91656f.png" /></p>
<p>绿色的斜线表示欧几里得距离,在现实中是不可能的。其他三条折线表示了曼哈顿距离,这三条折线的长度是相等的。当p趋近于无穷大时,闵可夫斯基距离转化成切比雪夫距离(Chebyshev distance)。</p>
<p>我们知道平面上到原点欧几里得距离(p=2)为1的点所组成的形状是一个圆,当p取其他数值的时候呢?</p>
<p><img alt="" src="http://images.cnitblog.com/blog/533521/201308/07220559-ae662025d1394f90bfd62f7c21c3d895.png" /></p>
<p>注意,当p&lt;1时,闵可夫斯基距离不再符合三角形法则,举个例子:当p&lt;1,(0,0)到(1,1)的距离等于(1+1)^{1/p}&gt;2,而(0,1)到这两个点的距离都是1。</p>
<p>闵可夫斯基距离比较直观,但是它与数据的分布有关,具有一定的局限性,如果x方向的幅值远远大于y方向的值,这个距离公式就会过度放大x维度的作用。所以,在计算距离之前,我们可能还需要对数据进行z-transform 处理,即减去均值,除以标准差:</p>
<div>
<div class="MathJax_Preview">
(x_1,y_1)\rightarrow (\frac{x_1-\mu_x}{\sigma_x},\frac{y_1-\mu_y}{\sigma_y})
</div>
<script type="math/tex; mode=display">
(x_1,y_1)\rightarrow (\frac{x_1-\mu_x}{\sigma_x},\frac{y_1-\mu_y}{\sigma_y})
</script>
</div>
<p>其中<span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span>为该维度上的均值,<span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>为该维度上的标准差。</p>
<p>可以看到,上述处理开始体现数据的统计特性了。这种方法在假设数据各个维度不相关的情况下利用数据分布的特性计算出不同的距离。如果维度相互之间数据相关(例如:身高较高的信息很有可能会带来体重较重的信息,因为两者是有关联的),这时候就要用到马氏距离(Mahalanobis distance)了。</p>
<p>可以看到,上述处理开始体现数据的统计特性了。这种方法在假设数据各个维度不相关的情况下利用数据分布的特性计算出不同的距离。如果维度相互之间数据相关(例如:身高较高的信息很有可能会带来体重较重的信息,因为两者是有关联的),这时候就要用到马氏距离（Mahalanobis distance)了。</p>
<ul>
<li><strong>马氏距离</strong></li>
</ul>
<p>马氏距离实际上是利用Cholesky transformation来消除不同维度之间的相关性和尺度不同的性质。假设样本点(列向量)之间的协方差对称矩阵是<span><span class="MathJax_Preview">\Sigma</span><script type="math/tex">\Sigma</script></span>, 通过Cholesky Decomposition(实际上是对称矩阵LU分解的一种特殊形式)可以转化为下三角矩阵和上三角矩阵的乘积:<span><span class="MathJax_Preview">\Sigma=LL^T</span><script type="math/tex">\Sigma=LL^T</script></span>。消除不同维度之间的相关性和尺度不同,只需要对样本点x做如下处理:<span><span class="MathJax_Preview">z=L^{-1}(x-\mu)</span><script type="math/tex">z=L^{-1}(x-\mu)</script></span>。处理之后的欧几里得距离就是原样本的马氏距离):</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
z^Tz &amp;=(L^{-1}(x-\mu))^T(L^{-1}(x-\mu)) \\
     &amp;=(x-\mu)^T\Sigma^{-1}(x-\mu)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
z^Tz &=(L^{-1}(x-\mu))^T(L^{-1}(x-\mu)) \\
     &=(x-\mu)^T\Sigma^{-1}(x-\mu)
\end{aligned}
</script>
</div>
<p><strong>马氏距离的问题</strong>:</p>
<blockquote>
<p>协方差矩阵必须满秩:里面有求逆矩阵的过程,不满秩不行,要求数据要有原维度个特征值,如果没有可以考虑先进行PCA,这种情况下PCA不会损失信息</p>
<p>不能处理非线性流形(manifold)上的问题:只对线性空间有效,如果要处理流形,只能在局部定义,可以用来建立KNN图</p>
</blockquote>
<p>python代码:</p>
<pre><code class="python">import numpy as np
import pylab as pl
import scipy.spatial.distance as dist

def plotSamples(x, y, z=None):
    stars = np.matrix([[3., -2., 0.], [3., 2., 0.]])
    if z is not None:
        x, y = z * np.matrix([x, y])
        stars = z * stars
    pl.scatter(x, y, s=10)    # 画 gaussian 随机点
    pl.scatter(np.array(stars[0]), np.array(stars[1]), s=200, marker='*', color='r')  # 画三个指定点
    pl.axhline(linewidth=2, color='g') # 画 x 轴
    pl.axvline(linewidth=2, color='g')  # 画 y 轴

    pl.axis('equal')
    pl.axis([-5, 5, -5, 5])
    pl.show()

# 产生高斯分布的随机点
mean = [0, 0]      # 平均值
cov = [[2, 1], [1, 2]]   # 协方差
x, y = np.random.multivariate_normal(mean, cov, 1000).T
plotSamples(x, y)

covMat = np.matrix(np.cov(x, y))    # 求 x 与 y 的协方差矩阵
Z = np.linalg.cholesky(covMat).I  # 仿射矩阵
plotSamples(x, y, Z)

# 求马氏距离 
print('\n到原点的马氏距离分别是:')
print(dist.mahalanobis([0,0], [3,3], covMat.I), dist.mahalanobis([0,0], [-2,2], covMat.I))

# 求变换后的欧几里得距离
dots = (Z * np.matrix([[3, -2, 0], [3, 2, 0]])).T
print('\n变换后到原点的欧几里得距离分别是:')
print(dist.minkowski([0, 0], np.array(dots[0]), 2), dist.minkowski([0, 0], np.array(dots[1]), 2))
</code></pre>

<ul>
<li><strong>夹角余弦</strong></li>
</ul>
<p>夹角余弦的取值范围为[-1,1],可以用来衡量两个向量方向的差异;夹角余弦越大,表示两个向量的夹角越小;当两个向量的方向重合时,夹角余弦取最大值1;当两个向量的方向完全相反时,夹角余弦取最小值-1。</p>
<p>机器学习中用这一概念来衡量样本向量之间的差异,其数学表达式如下:</p>
<div>
<div class="MathJax_Preview">
cos\theta =\frac{AB}{\left| A \right| \left|B \right| } =\frac{\sum_{k=1}^{n}{x_{1k}x_{2k} } }{\sqrt{\sum_{k=1}^{n}{x_{1k}^{2} } } \sqrt{\sum_{k=1}^{n}{x_{2k}^{2} } } } 
</div>
<script type="math/tex; mode=display">
cos\theta =\frac{AB}{\left| A \right| \left|B \right| } =\frac{\sum_{k=1}^{n}{x_{1k}x_{2k} } }{\sqrt{\sum_{k=1}^{n}{x_{1k}^{2} } } \sqrt{\sum_{k=1}^{n}{x_{2k}^{2} } } } 
</script>
</div>
<p>夹角余弦的python实现:</p>
<pre><code class="python">from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print(dot(vector1,vector2)/(linalg.norm(vector1)*linalg.norm(vector2)))
</code></pre>

<ul>
<li><strong>汉明距离</strong></li>
</ul>
<p>汉明距离定义的是两个字符串中不相同位数的数目。例如:字符串‘1111’与‘1001’之间的汉明距离为2。信息编码中一般应使得编码间的汉明距离尽可能的小。</p>
<p>汉明距离的python实现:</p>
<pre><code class="python">from numpy import *
matV = mat([1,1,1,1],[1,0,0,1])
smstr = nonzero(matV[0]-matV[1])
print(smstr)
</code></pre>

<ul>
<li><strong>杰卡德相似系数</strong></li>
</ul>
<p>两个集合A和B的交集元素在A和B的并集中所占的比例称为两个集合的杰卡德相似系数,用符号<span><span class="MathJax_Preview">J(A,B)</span><script type="math/tex">J(A,B)</script></span>表示,数学表达式为:</p>
<div>
<div class="MathJax_Preview">
J\left( A,B \right) =\frac{\left| A\cap B\right| }{\left|A\cup B \right| } 
</div>
<script type="math/tex; mode=display">
J\left( A,B \right) =\frac{\left| A\cap B\right| }{\left|A\cup B \right| } 
</script>
</div>
<p>杰卡德相似系数是衡量两个集合的相似度的一种指标。一般可以将其用在衡量样本的相似度上。</p>
<ul>
<li><strong>杰卡德距离</strong></li>
</ul>
<p>与杰卡德相似系数相反的概念是杰卡德距离,其定义式为:</p>
<div>
<div class="MathJax_Preview">
J_{\sigma} =1-J\left( A,B \right) =\frac{\left| A\cup B \right| -\left| A\cap B \right| }{\left| A\cup B \right| } 
</div>
<script type="math/tex; mode=display">
J_{\sigma} =1-J\left( A,B \right) =\frac{\left| A\cup B \right| -\left| A\cap B \right| }{\left| A\cup B \right| } 
</script>
</div>
<p>杰卡德距离的python实现:</p>
<pre><code class="python">from numpy import *
import scipy.spatial.distance as dist
matV = mat([1,1,1,1],[1,0,0,1])
print(dist.pdist(matV,'jaccard'))
</code></pre>

<h3 id="13-classification">1.3 分类(Classification)指标</h3>
<p><img src="http://pic1.zhimg.com/v2-49a657a2ec9fa94edb976ca1a7d33afc_r.jpg" style="width: 80%"></p>
<ul>
<li><strong>混淆矩阵</strong></li>
</ul>
<p>在预测系统中,牵扯到预测值,真实值,以及真实值和预测值之间的关系,进而产生了混淆矩阵:</p>
<table>
    <tr>
        <td style="width: 40%;"><img width="100%"  src="http://pic4.zhimg.com/v2-b97dab4ad52b9d0c7dac87c9b81acebf.jpg" /></td>
        <td><img width="50%" src="http://pic3.zhimg.com/80/v2-76b9176719868e9b85bedf5192e722d3_hd.jpg" /></td>
    </tr><tr>
        <td style="color:orange;font-size:13px;color:#995;">多分类下的混淆矩阵</td>
        <td style="color:orange;font-size:13px;color:#995;">二分类下的混淆矩阵</td>
    </tr>
</table>

<p>混淆矩阵又被称为错误矩阵,在每个类别下,模型预测错误的结果数量,以及错误预测的类别和正确预测的数量都在一个矩阵下面显示出来,方便直观的评估模型分类的结果。</p>
<p>通常取预测值和真实值之间的关系、预测值对矩阵进行划分:</p>
<blockquote>
<p>True positive(TP):真实值为Positive,预测正确(预测值为Positive)</p>
<p>True negative(TN):真实值为Negative,预测正确(预测值为Negative)</p>
<p>False positive(FP):真实值为Negative,预测错误(预测值为Positive),第一类错误,Type I error。</p>
<p>False negative(FN): 真实值为Positive,预测错误(预测值为 Negative),第二类错误,Type II error。</p>
</blockquote>
<ul>
<li><strong>精确率(Precision)</strong></li>
</ul>
<p>精确率是针对我们预测结果而言的,它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了,一种就是把正类预测为正类(TP),另一种就是把负类预测为正类(FP),也就是</p>
<div>
<div class="MathJax_Preview">P = \frac{TP}{TP+FP}</div>
<script type="math/tex; mode=display">P = \frac{TP}{TP+FP}</script>
</div>
<p>精确率取值范围为[0,1],取值越大,模型预测能力越好。</p>
<ul>
<li><strong>召回率(Recall)</strong></li>
</ul>
<p>针对我们原来的样本而言的,它表示的是样本中的正例有多少被预测正确了。那也有两种可能,一种是把原来的正类预测成正类(TP),另一种就是把原来的正类预测为负类(FN)。其实就是分母不同,一个分母是预测为正的样本数,另一个是原来样本中所有的正样本数。</p>
<div>
<div class="MathJax_Preview">R=\frac{TP}{TP+FN}</div>
<script type="math/tex; mode=display">R=\frac{TP}{TP+FN}</script>
</div>
<p>在信息检索领域,精确率和召回率又被称为查准率和查全率:</p>
<blockquote>
<p>查准率＝检索出的相关信息量 / 检索出的信息总量</p>
<p>查全率＝检索出的相关信息量 / 系统中的相关信息总量</p>
</blockquote>
<ul>
<li><strong>准确率(Accuracy)</strong></li>
</ul>
<p>针对所有的样本,样本预测正确的数量占总数据:</p>
<div>
<div class="MathJax_Preview">
Acc=\frac{TP+TN}{TP+FN+FP+TN}
</div>
<script type="math/tex; mode=display">
Acc=\frac{TP+TN}{TP+FN+FP+TN}
</script>
</div>
<ul>
<li><strong>引申指标</strong></li>
</ul>
<p>用样本中的正类和负类进行计算的定义</p>
<table>
<thead>
<tr>
<th>缩写</th>
<th>全称</th>
<th>等价称呼</th>
<th>计算公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>TPR</td>
<td>True Positive Rate</td>
<td>真正类率,Recall Sensitivity</td>
<td><span><span class="MathJax_Preview">\frac{TP}{TP+FN}</span><script type="math/tex">\frac{TP}{TP+FN}</script></span></td>
</tr>
<tr>
<td>FNR</td>
<td>False Negative Rate</td>
<td>假负类率,Miss rate Type rs error</td>
<td><span><span class="MathJax_Preview">\frac{TN}{TP+FN}</span><script type="math/tex">\frac{TN}{TP+FN}</script></span></td>
</tr>
<tr>
<td>FPR</td>
<td>False Positive Rate</td>
<td>假正类率,fall-out Type 1 error</td>
<td><span><span class="MathJax_Preview">\frac{FP}{FP+FN}=1-TNR</span><script type="math/tex">\frac{FP}{FP+FN}=1-TNR</script></span></td>
</tr>
<tr>
<td>TNR</td>
<td>True Negative Rate</td>
<td>真负类率,Specificity</td>
<td><span><span class="MathJax_Preview">\frac{TN}{TN+FP}</span><script type="math/tex">\frac{TN}{TN+FP}</script></span></td>
</tr>
</tbody>
</table>
<p>用预测结果的正类和负类进行计算的定义</p>
<table>
<thead>
<tr>
<th>缩写</th>
<th>全称</th>
<th>等价称呼</th>
<th>计算公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>PPV</td>
<td>Positive Predictive Value</td>
<td>正类预测率,Precision</td>
<td><span><span class="MathJax_Preview">\frac{TP}{TP+FP}</span><script type="math/tex">\frac{TP}{TP+FP}</script></span></td>
</tr>
<tr>
<td>FOR</td>
<td>False Omission Rata</td>
<td>假错误率</td>
<td><span><span class="MathJax_Preview">\frac{FN}{TN+FN}=1-NPV</span><script type="math/tex">\frac{FN}{TN+FN}=1-NPV</script></span></td>
</tr>
<tr>
<td>FDR</td>
<td>False Discovery Rate</td>
<td>假发现率</td>
<td><span><span class="MathJax_Preview">\frac{FP}{TP+FP}</span><script type="math/tex">\frac{FP}{TP+FP}</script></span></td>
</tr>
<tr>
<td>NPV</td>
<td>Negative Predictive Value</td>
<td>负类预测率</td>
<td><span><span class="MathJax_Preview">\frac{TN}{TN+FN}</span><script type="math/tex">\frac{TN}{TN+FN}</script></span></td>
</tr>
</tbody>
</table>
<p>其他定义概念</p>
<table>
<thead>
<tr>
<th>缩写</th>
<th>全称</th>
<th>等价称呼</th>
<th>计算公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>ACC</td>
<td>Accuracy</td>
<td>准确率</td>
<td><span><span class="MathJax_Preview">\frac{TP+TN}{TP+FN+FP+TN}</span><script type="math/tex">\frac{TP+TN}{TP+FN+FP+TN}</script></span></td>
</tr>
<tr>
<td>LR+</td>
<td>Positive Likelihood Ratio</td>
<td>正类似然比</td>
<td><span><span class="MathJax_Preview">\frac{TPR}{FPR}</span><script type="math/tex">\frac{TPR}{FPR}</script></span></td>
</tr>
<tr>
<td>LR-</td>
<td>Negative likelihood ratio</td>
<td>负类似然比</td>
<td><span><span class="MathJax_Preview">\frac{FNR}{TNR}</span><script type="math/tex">\frac{FNR}{TNR}</script></span></td>
</tr>
<tr>
<td>DOR</td>
<td>Diagnostic odds ratio</td>
<td>诊断胜算比</td>
<td><span><span class="MathJax_Preview">\frac{LR+}{LR-}</span><script type="math/tex">\frac{LR+}{LR-}</script></span></td>
</tr>
<tr>
<td>F1 score</td>
<td>F1 test measure</td>
<td>F1值</td>
<td><span><span class="MathJax_Preview">\frac{2*Recall*Precision}{Recall+Precision}</span><script type="math/tex">\frac{2*Recall*Precision}{Recall+Precision}</script></span></td>
</tr>
<tr>
<td>MCC</td>
<td>Matthews Correlation coefficient</td>
<td>马修斯相关性系数</td>
<td><span><span class="MathJax_Preview">\frac{TP*TN-FP*FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}</span><script type="math/tex">\frac{TP*TN-FP*FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}</script></span></td>
</tr>
<tr>
<td>BM</td>
<td>Bookmaker Informedness</td>
<td>Informedness</td>
<td><span><span class="MathJax_Preview">TPR+TNR-1</span><script type="math/tex">TPR+TNR-1</script></span></td>
</tr>
</tbody>
</table>
<p>LR+/-指的是似然比,LR+越大表示模型对正类的分类越好,LR-越大表示模型对负类的分类效果越好。F1值是精确值和召回率的调和均值,其实原公式是<span><span class="MathJax_Preview">F_{\beta}=(1+\beta^2)\times\frac{\text{P}\times\text{R}}{\beta^2\times\text{P}+\text{R}}</span><script type="math/tex">F_{\beta}=(1+\beta^2)\times\frac{\text{P}\times\text{R}}{\beta^2\times\text{P}+\text{R}}</script></span>,这里的<span><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>表示:<strong>召回率的权重是准确率的<span><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>倍</strong>。即F值是一种精确率和召回率的综合指标,权重由<span><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>决定。MCC值在[-1,1]之间,靠近1表示完全预测正确m靠近-1表示完全悖论,0表示随机预测</p>
<pre><code class="python">def _check_zero_division(zero_division):
    if isinstance(zero_division, str) and zero_division == &quot;warn&quot;:
        return
    elif isinstance(zero_division, (int, float)) and zero_division in [0, 1]:
        return
    raise ValueError('Got zero_division={0}. Must be one of [&quot;warn&quot;, 0, 1]'.format(zero_division))

# 权重求和
def _weighted_sum(sample_score, sample_weight, normalize=False):
    if normalize:
        return np.average(sample_score, weights=sample_weight)
    elif sample_weight is not None:
        return np.dot(sample_score, sample_weight)
    else:
        return sample_score.sum()

########################### 计算准确率 #################################
def accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None):
    &quot;&quot;&quot;Accuracy classification score.
    Parameters
      y_true : 1d array-like, or label indicator array / sparse matrix
        Ground truth (correct) labels.
      y_pred : 1d array-like, or label indicator array / sparse matrix
        Predicted labels, as returned by a classifier.
      normalize : bool, optional (default=True)
          If False, return the number of correctly classified samples.
          Otherwise, return the fraction of correctly classified samples.
      sample_weight : array-like of shape (n_samples,), default=None
        Sample weights.
    Returns
      score : float
        If normalize == True, return the fraction of correctly classified samples (float), else returns the number of correctly classified samples (int).

    Examples
        &gt;&gt;&gt; from sklearn.metrics import accuracy_score
        &gt;&gt;&gt; y_pred = [0, 2, 1, 3]
        &gt;&gt;&gt; y_true = [0, 1, 2, 3]
        &gt;&gt;&gt; accuracy_score(y_true, y_pred)
        0.5
        &gt;&gt;&gt; accuracy_score(y_true, y_pred, normalize=False)
        2
        In the multilabel case with binary label indicators:
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
        0.5
    &quot;&quot;&quot;

    # Compute accuracy for each possible representation
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)    # 检核类型
    check_consistent_length(y_true, y_pred, sample_weight)     # 检验长度

    # 以下为核心代码
    if y_type.startswith('multilabel'):
        differing_labels = count_nonzero(y_true - y_pred, axis=1)
        score = differing_labels == 0
    else:
        score = y_true == y_pred

    return _weighted_sum(score, sample_weight, normalize)

########################### 计算多分类的混淆矩阵 #######################################
def confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None):
  &quot;&quot;&quot;Compute confusion matrix to evaluate the accuracy of a classification.
    Parameters
      y_true : array-like of shape (n_samples,)
        Ground truth (correct) target values.
      y_pred : array-like of shape (n_samples,)
        Estimated targets as returned by a classifier.
      labels : array-like of shape (n_classes), default=None
        List of labels to index the matrix. This may be used to reorder or select a subset of labels.
      sample_weight : array-like of shape (n_samples,), default=None Sample weights.
      normalize : {'true', 'pred', 'all'}, default=None
        Normalizes confusion matrix over the true (rows), predicted (columns)
        conditions or all the population. If None, confusion matrix will not be
        normalized.
    Returns
      C : ndarray of shape (n_classes, n_classes)
        Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and prediced label being j-th class.
    Examples
        &gt;&gt;&gt; from sklearn.metrics import confusion_matrix
        &gt;&gt;&gt; y_true = [2, 0, 2, 2, 0, 1]
        &gt;&gt;&gt; y_pred = [0, 0, 2, 2, 0, 2]
        &gt;&gt;&gt; confusion_matrix(y_true, y_pred)
        array([[2, 0, 0],
               [0, 0, 1],
               [1, 0, 2]])
        &gt;&gt;&gt; y_true = [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;]
        &gt;&gt;&gt; y_pred = [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;]
        &gt;&gt;&gt; confusion_matrix(y_true, y_pred, labels=[&quot;ant&quot;, &quot;bird&quot;, &quot;cat&quot;])
        array([[2, 0, 0],
               [0, 0, 1],
               [1, 0, 2]])
        In the binary case, we can extract true positives, etc as follows:
        &gt;&gt;&gt; tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()
        &gt;&gt;&gt; (tn, fp, fn, tp)
        (0, 2, 1, 1)
    &quot;&quot;&quot;
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    if y_type not in (&quot;binary&quot;, &quot;multiclass&quot;):
        raise ValueError(&quot;%s is not supported&quot; % y_type)

    if labels is None:
        labels = unique_labels(y_true, y_pred)
    else:
        labels = np.asarray(labels)
        n_labels = labels.size
        if n_labels == 0:
            raise ValueError(&quot;'labels' should contains at least one label.&quot;)
        elif y_true.size == 0:
            return np.zeros((n_labels, n_labels), dtype=np.int)
        elif np.all([l not in y_true for l in labels]):
            raise ValueError(&quot;At least one label specified must be in y_true&quot;)

    if sample_weight is None:
        sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
    else:
        sample_weight = np.asarray(sample_weight)

    check_consistent_length(y_true, y_pred, sample_weight)

    if normalize not in ['true', 'pred', 'all', None]:
        raise ValueError(&quot;normalize must be one of {'true', 'pred', 'all', None}&quot;)

    n_labels = labels.size
    label_to_ind = {y: x for x, y in enumerate(labels)}
    # convert yt, yp into index
    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])

    # intersect y_pred, y_true with labels, eliminate items not in labels
    ind = np.logical_and(y_pred &lt; n_labels, y_true &lt; n_labels)
    y_pred = y_pred[ind]
    y_true = y_true[ind]
    # also eliminate weights of eliminated items
    sample_weight = sample_weight[ind]

    # Choose the accumulator dtype to always have high precision
    if sample_weight.dtype.kind in {'i', 'u', 'b'}:
        dtype = np.int64
    else:
        dtype = np.float64

    cm = coo_matrix((sample_weight, (y_true, y_pred)),shape=(n_labels, n_labels), dtype=dtype,).toarray()      # scipy.sparse.coo_matrix

    with np.errstate(all='ignore'):
        if normalize == 'true':
            cm = cm / cm.sum(axis=1, keepdims=True)
        elif normalize == 'pred':
            cm = cm / cm.sum(axis=0, keepdims=True)
        elif normalize == 'all':
            cm = cm / cm.sum()
        cm = np.nan_to_num(cm)
    return cm


########################### 计算多标签的混淆矩阵 #######################################
def multilabel_confusion_matrix(y_true, y_pred, *, sample_weight=None,
    labels=None, samplewise=False):
  &quot;&quot;&quot;Compute a confusion matrix for each class or sample
    Parameters
      y_true : 1d array-like, or label indicator array / sparse matrix of shape (n_samples, n_outputs) or (n_samples,)
        Ground truth (correct) target values.
      y_pred : 1d array-like, or label indicator array / sparse matrix of shape (n_samples, n_outputs) or (n_samples,)
        Estimated targets as returned by a classifier
      sample_weight : array-like of shape (n_samples,), default=None Sample weights
      labels : array-like
        A list of classes or column indices to select some (or to force inclusion of classes absent from the data)
      samplewise : bool, default=False
        In the multilabel case, this calculates a confusion matrix per sample
    Returns
      multi_confusion : array, shape (n_outputs, 2, 2)
        A 2x2 confusion matrix corresponding to each output in the input.
        When calculating class-wise multi_confusion (default), then
        n_outputs = n_labels; when calculating sample-wise multi_confusion
        (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,
        the results will be returned in the order specified in ``labels``,
        otherwise the results will be returned in sorted order by default.
    Multilabel-indicator case:
    &gt;&gt;&gt; import numpy as np
    &gt;&gt;&gt; from sklearn.metrics import multilabel_confusion_matrix
    &gt;&gt;&gt; y_true = np.array([[1, 0, 1],
    ...                    [0, 1, 0]])
    &gt;&gt;&gt; y_pred = np.array([[1, 0, 0],
    ...                    [0, 1, 1]])
    &gt;&gt;&gt; multilabel_confusion_matrix(y_true, y_pred)
    array([[[1, 0],
            [0, 1]],
    &lt;BLANKLINE&gt;
           [[1, 0],
            [0, 1]],
    &lt;BLANKLINE&gt;
           [[0, 1],
            [1, 0]]])
    Multiclass case:
    &gt;&gt;&gt; y_true = [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;]
    &gt;&gt;&gt; y_pred = [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;]
    &gt;&gt;&gt; multilabel_confusion_matrix(y_true, y_pred,
    ...                             labels=[&quot;ant&quot;, &quot;bird&quot;, &quot;cat&quot;])
    array([[[3, 1],
            [0, 2]],
    &lt;BLANKLINE&gt;
           [[5, 0],
            [1, 0]],
    &lt;BLANKLINE&gt;
           [[2, 1],
            [1, 2]]])
    &quot;&quot;&quot;
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    if sample_weight is not None:
        sample_weight = column_or_1d(sample_weight)
    check_consistent_length(y_true, y_pred, sample_weight)

    if y_type not in (&quot;binary&quot;, &quot;multiclass&quot;, &quot;multilabel-indicator&quot;):
        raise ValueError(&quot;%s is not supported&quot; % y_type)

    present_labels = unique_labels(y_true, y_pred)
    if labels is None:
        labels = present_labels
        n_labels = None
    else:
        n_labels = len(labels)
        labels = np.hstack([labels, np.setdiff1d(present_labels, labels,assume_unique=True)])

    if y_true.ndim == 1:
        if samplewise:
            raise ValueError(&quot;Samplewise metrics are not available outside of multilabel classification.&quot;)

        le = LabelEncoder()
        le.fit(labels)
        y_true = le.transform(y_true)
        y_pred = le.transform(y_pred)
        sorted_labels = le.classes_

        # labels are now from 0 to len(labels) - 1 -&gt; use bincount
        tp = y_true == y_pred
        tp_bins = y_true[tp]
        if sample_weight is not None:
            tp_bins_weights = np.asarray(sample_weight)[tp]
        else:
            tp_bins_weights = None

        if len(tp_bins):
            tp_sum = np.bincount(tp_bins, weights=tp_bins_weights,
                                 minlength=len(labels))
        else:
            # Pathological case
            true_sum = pred_sum = tp_sum = np.zeros(len(labels))
        if len(y_pred):
            pred_sum = np.bincount(y_pred, weights=sample_weight,
                                   minlength=len(labels))
        if len(y_true):
            true_sum = np.bincount(y_true, weights=sample_weight,
                                   minlength=len(labels))

        # Retain only selected labels
        indices = np.searchsorted(sorted_labels, labels[:n_labels])
        tp_sum = tp_sum[indices]
        true_sum = true_sum[indices]
        pred_sum = pred_sum[indices]

    else:
        sum_axis = 1 if samplewise else 0

        # All labels are index integers for multilabel.
        # Select labels:
        if not np.array_equal(labels, present_labels):
            if np.max(labels) &gt; np.max(present_labels):
                raise ValueError('All labels must be in [0, n labels) for '
                                 'multilabel targets. '
                                 'Got %d &gt; %d' %
                                 (np.max(labels), np.max(present_labels)))
            if np.min(labels) &lt; 0:
                raise ValueError('All labels must be in [0, n labels) for '
                                 'multilabel targets. '
                                 'Got %d &lt; 0' % np.min(labels))

        if n_labels is not None:
            y_true = y_true[:, labels[:n_labels]]
            y_pred = y_pred[:, labels[:n_labels]]

        # calculate weighted counts
        true_and_pred = y_true.multiply(y_pred)
        tp_sum = count_nonzero(true_and_pred, axis=sum_axis,
                               sample_weight=sample_weight)
        pred_sum = count_nonzero(y_pred, axis=sum_axis,
                                 sample_weight=sample_weight)
        true_sum = count_nonzero(y_true, axis=sum_axis,
                                 sample_weight=sample_weight)

    fp = pred_sum - tp_sum
    fn = true_sum - tp_sum
    tp = tp_sum

    if sample_weight is not None and samplewise:
        sample_weight = np.array(sample_weight)
        tp = np.array(tp)
        fp = np.array(fp)
        fn = np.array(fn)
        tn = sample_weight * y_true.shape[1] - tp - fp - fn
    elif sample_weight is not None:
        tn = sum(sample_weight) - tp - fp - fn
    elif samplewise:
        tn = y_true.shape[1] - tp - fp - fn
    else:
        tn = y_true.shape[0] - tp - fp - fn

    return np.array([tn, fp, fn, tp]).T.reshape(-1, 2, 2)


########################### 计算多标签的精确率/召回率/F分数 #######################################
def precision_recall_fscore_support(y_true, y_pred, *, beta=1.0, labels=None, pos_label=1, 
        average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None,
        zero_division=&quot;warn&quot;):
    &quot;&quot;&quot;Compute precision, recall, F-measure and support for each class
    Parameters
        y_true : 1d array-like, or label indicator array / sparse matrix
            Ground truth (correct) target values.
        y_pred : 1d array-like, or label indicator array / sparse matrix
            Estimated targets as returned by a classifier.
        beta : float, 1.0 by default
            The strength of recall versus precision in the F-score.
        labels : list, optional
            The set of labels to include when average != 'binary', and their
            order if average is None. Labels present in the data can be
            excluded, for example to calculate a multiclass average ignoring a
            majority negative class, while labels not present in the data will
            result in 0 components in a macro average. For multilabel targets,
            labels are column indices. By default, all labels in ``y_true`` and
            y_pred are used in sorted order.
        pos_label : str or int, 1 by default
            The class to report if ``average='binary'`` and the data is binary.
            If the data are multiclass or multilabel, this will be ignored;
            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report
            scores for that label only.
        average : string, [None (default), 'binary', 'micro', 'macro', 'samples', 'weighted']
            If ``None``, the scores for each class are returned. Otherwise, this
            determines the type of averaging performed on the data:
            ``'binary'``:
                Only report results for the class specified by ``pos_label``.
                This is applicable only if targets (``y_{true,pred}``) are binary.
            ``'micro'``:
                Calculate metrics globally by counting the total true positives,
                false negatives and false positives.
            ``'macro'``:
                Calculate metrics for each label, and find their unweighted
                mean.  This does not take label imbalance into account.
            ``'weighted'``:
                Calculate metrics for each label, and find their average weighted
                by support (the number of true instances for each label). This
                alters 'macro' to account for label imbalance; it can result in an
                F-score that is not between precision and recall.
            ``'samples'``:
                Calculate metrics for each instance, and find their average (only
                meaningful for multilabel classification where this differs from
                :func:`accuracy_score`).
        warn_for : tuple or set, for internal use
            This determines which warnings will be made in the case that this
            function is being used to return only one of its metrics.
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights.
        zero_division : &quot;warn&quot;, 0 or 1, default=&quot;warn&quot;
            Sets the value to return when there is a zero division:
               - recall: when there are no positive labels
               - precision: when there are no positive predictions
               - f-score: both
            If set to &quot;warn&quot;, this acts as 0, but warnings are also raised.
    Returns
        precision : float (if average is not None) or array of float, shape =\
            [n_unique_labels]
        recall : float (if average is not None) or array of float, , shape =\
            [n_unique_labels]
        fbeta_score : float (if average is not None) or array of float, shape =\
            [n_unique_labels]
        support : None (if average is not None) or array of int, shape =\
            [n_unique_labels]
            The number of occurrences of each label in ``y_true``.
    Examples
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; from sklearn.metrics import precision_recall_fscore_support
        &gt;&gt;&gt; y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])
        &gt;&gt;&gt; y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])
        &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average='macro')
        (0.22..., 0.33..., 0.26..., None)
        &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average='micro')
        (0.33..., 0.33..., 0.33..., None)
        &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average='weighted')
        (0.22..., 0.33..., 0.26..., None)
        It is possible to compute per-label precisions, recalls, F1-scores and
        supports instead of averaging:
        &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average=None,
        ... labels=['pig', 'dog', 'cat'])
        (array([0.        , 0.        , 0.66...]),
         array([0., 0., 1.]), array([0. , 0. , 0.8]),
         array([2, 2, 2]))
    &quot;&quot;&quot;
    _check_zero_division(zero_division)
    if beta &lt; 0:
        raise ValueError(&quot;beta should be &gt;=0 in the F-beta score&quot;)
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)

    # Calculate tp_sum, pred_sum, true_sum ###
    samplewise = average == 'samples'
    MCM = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight,
        labels=labels, samplewise=samplewise)
    tp_sum = MCM[:, 1, 1]
    pred_sum = tp_sum + MCM[:, 0, 1]
    true_sum = tp_sum + MCM[:, 1, 0]

    if average == 'micro':
        tp_sum = np.array([tp_sum.sum()])
        pred_sum = np.array([pred_sum.sum()])
        true_sum = np.array([true_sum.sum()])

    # Finally, we have all our sufficient statistics. Divide! #
    beta2 = beta ** 2

    # Divide, and on zero-division, set scores and/or warn according to
    # zero_division:
    precision = _prf_divide(tp_sum, pred_sum, 'precision','predicted', 
        average, warn_for, zero_division)
    recall = _prf_divide(tp_sum, true_sum, 'recall', 'true', 
        average, warn_for, zero_division)

    # warn for f-score only if zero_division is warn, it is in warn_for
    # and BOTH prec and rec are ill-defined
    if zero_division == &quot;warn&quot; and (&quot;f-score&quot;,) == warn_for:
        if (pred_sum[true_sum == 0] == 0).any():
            _warn_prf(average, &quot;true nor predicted&quot;, 'F-score is', len(true_sum))

    # if tp == 0 F will be 1 only if all predictions are zero, all labels are
    # zero, and zero_division=1. In all other case, 0
    if np.isposinf(beta):
        f_score = recall
    else:
        denom = beta2 * precision + recall

        denom[denom == 0.] = 1  # avoid division by 0
        f_score = (1 + beta2) * precision * recall / denom

    # Average the results
    if average == 'weighted':
        weights = true_sum
        if weights.sum() == 0:
            zero_division_value = 0.0 if zero_division in [&quot;warn&quot;, 0] else 1.0
            # precision is zero_division if there are no positive predictions
            # recall is zero_division if there are no positive labels
            # fscore is zero_division if all labels AND predictions are
            # negative
            return (zero_division_value if pred_sum.sum() == 0 else 0,
                    zero_division_value,
                    zero_division_value if pred_sum.sum() == 0 else 0,
                    None)

    elif average == 'samples':
        weights = sample_weight
    else:
        weights = None

    if average is not None:
        assert average != 'binary' or len(precision) == 1
        precision = np.average(precision, weights=weights)
        recall = np.average(recall, weights=weights)
        f_score = np.average(f_score, weights=weights)
        true_sum = None  # return no support

    return precision, recall, f_score, true_sum
</code></pre>

<ul>
<li><strong>AP与mAP</strong></li>
</ul>
<p>二分类问题的P-R曲线(precision-recall curve),P-R曲线下面与x轴围成的面积称为average precision(AP)。显然通过积分来计算</p>
<div>
<div class="MathJax_Preview">
AP=\int_0^1P(r)dr
</div>
<script type="math/tex; mode=display">
AP=\int_0^1P(r)dr
</script>
</div>
<p>但通常情况下都是使用估算或者插值的方式计算。</p>
<p>mAP(mean average precision)的意义是为了评估你整个目标检测模型的准确度。方法是:计算每个分类的AP,求和再平均,得到的就是mAP</p>
<ul>
<li><strong><span><span class="MathJax_Preview">F_\beta</span><script type="math/tex">F_\beta</script></span> Score</strong></li>
</ul>
<p>Precision和Recall是互相影响的,理想情况下肯定是做到两者都高,但是一般情况下Precision高、Recall就低,Recall高、Precision就低。为了均衡两个指标,我们可以采用Precision和Recall的加权调和平均(weighted harmonic mean)来衡量,即<span><span class="MathJax_Preview">F_\beta</span><script type="math/tex">F_\beta</script></span> Score,公式如下:</p>
<div>
<div class="MathJax_Preview">
F_{\beta}=(1+\beta^2)\times\frac{\text{P}\times\text{R}}{\beta^2\times\text{P}+\text{R}}
</div>
<script type="math/tex; mode=display">
F_{\beta}=(1+\beta^2)\times\frac{\text{P}\times\text{R}}{\beta^2\times\text{P}+\text{R}}
</script>
</div>
<p><span><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>表示权重,</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
F_{\beta}&amp; = \frac{(1+\beta^2)\times\text{P}\times\text{R}}{\beta^2\times\text{P}+\text{R}} \\
&amp; = \frac {1}{\frac{\beta^2}{(1+\beta^2)\times R}+\frac{1}{(1+\beta^2)\times P}}\\ 
&amp; = \frac {1}{\frac{1}{(1+\frac{1}{\beta^2})\times R}+\frac{1}{(1+\beta^2)\times P}} 
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
F_{\beta}& = \frac{(1+\beta^2)\times\text{P}\times\text{R}}{\beta^2\times\text{P}+\text{R}} \\
& = \frac {1}{\frac{\beta^2}{(1+\beta^2)\times R}+\frac{1}{(1+\beta^2)\times P}}\\ 
& = \frac {1}{\frac{1}{(1+\frac{1}{\beta^2})\times R}+\frac{1}{(1+\beta^2)\times P}} 
\end{aligned}
</script>
</div>
<p>当<span><span class="MathJax_Preview">\beta \rightarrow 0 : F_{\beta} \approx P</span><script type="math/tex">\beta \rightarrow 0 : F_{\beta} \approx P</script></span>;当<span><span class="MathJax_Preview">\beta \rightarrow \infty:F_{\beta} \approx R</span><script type="math/tex">\beta \rightarrow \infty:F_{\beta} \approx R</script></span>。通俗的语言就是:<span><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>越大,Recall的权重越大,<span><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>越小,Precision的权重越大。</p>
<p>随着如<span><span class="MathJax_Preview">\beta=1</span><script type="math/tex">\beta=1</script></span>为<span><span class="MathJax_Preview">F_\beta</span><script type="math/tex">F_\beta</script></span>此时Precision和Recall的权重相等,公式如下:</p>
<div>
<div class="MathJax_Preview">
F_{\beta}=F_{1}=\frac{2\times\text{P}\times\text{R}}{\text{P}+\text{R}} 
</div>
<script type="math/tex; mode=display">
F_{\beta}=F_{1}=\frac{2\times\text{P}\times\text{R}}{\text{P}+\text{R}} 
</script>
</div>
<p>由于<span><span class="MathJax_Preview">F_\beta</span><script type="math/tex">F_\beta</script></span> Score无法直观反映数据的情况,同时业务含义相对较弱,实际工作用到的不多。</p>
<ul>
<li><strong>ROC和AUC</strong></li>
</ul>
<p>AUC是一种模型分类指标,且仅仅是二分类模型的评价指标。AUC是Area Under Curve的简称,那么Curve就是ROC(Receiver Operating Characteristic),翻译为"接受者操作特性曲线"。也就是说ROC是一条曲线,AUC是一个面积值。</p>
<p><strong>ROC</strong>:ROC曲线为FPR与TPR之间的关系曲线,这个组合以 FPR 对TPR,即是以代价(costs)对收益(benefits),显然收益越高,代价越低,模型的性能就越好。</p>
<p>x轴为假阳性率(FPR):在所有的负样本中,分类器预测错误的比例</p>
<div>
<div class="MathJax_Preview">
FPR = \frac {FP}{FP+TN}
</div>
<script type="math/tex; mode=display">
FPR = \frac {FP}{FP+TN}
</script>
</div>
<p>y轴为真阳性率(TPR):在所有的正样本中,分类器预测正确的比例(等于Recall)</p>
<div>
<div class="MathJax_Preview">
TPR = \frac {TP}{TP+FN}
</div>
<script type="math/tex; mode=display">
TPR = \frac {TP}{TP+FN}
</script>
</div>
<p>为了更好地理解ROC曲线,我们使用具体的实例来说明:</p>
<p>如在医学诊断的主要任务是尽量把生病的人群都找出来,也就是TPR越高越好。而尽量降低没病误诊为有病的人数,也就是FPR越低越好。不难发现,这两个指标之间是相互制约的。如果某个医生对于有病的症状比较敏感,稍微的小症状都判断为有病,那么他的TPR应该会很高,但是FPR也就相应地变高。最极端的情况下,他把所有的样本都看做有病,那么TPR达到1,FPR也为1。以FPR为横轴,TPR为纵轴,得到如下ROC空间。</p>
<p><img src="http://pic1.zhimg.com/v2-6d498ba1302f3c073c6c8ce1cd014e50_r.jpg" style="width:50%"></p>
<p>可以看出,左上角的点(TPR=1,FPR=0)为完美分类,也就是这个医生医术高明,诊断全对。点A(TPR>FPR),医生A的判断大体是正确的。中线上的点B(TPR=FPR),也就是医生B全都是蒙的,蒙对一半,蒙错一半;下半平面的点C(TPR&lt;FPR),这个医生说你有病,那么你很可能没有病,医生C的话我们要反着听,为真庸医。上图中一个阈值,得到一个点。现在我们需要一个独立于阈值的评价指标来衡量这个医生的医术如何,也就是遍历所有的阈值,得到ROC曲线。</p>
<p>假设下图是某医生的诊断统计图,为未得病人群(上图)和得病人群(下图)的模型输出概率分布图(横坐标表示模型输出概率,纵坐标表示概率对应的人群的数量),显然未得病人群的概率值普遍低于得病人群的输出概率值(即正常人诊断出疾病的概率小于得病人群诊断出疾病的概率)。</p>
<p><img src="http://pic2.zhimg.com/v2-b6a3d08daaf2136ff6a072b90f8fe151_r.jpg" style="width: 50%" ></p>
<p>竖线代表阈值。显然,图中给出了某个阈值对应的混淆矩阵,通过改变不同的阈值<span><span class="MathJax_Preview">1.0 \rightarrow 0</span><script type="math/tex">1.0 \rightarrow 0</script></span>,得到一系列的混淆矩阵,进而得到一系列的TPR和FPR,绘制出ROC曲线。</p>
<p>阈值为1时,不管你什么症状,医生均未诊断出疾病(预测值都为N,此时绿色和红色区域的面积为0,因此FPR=TPR=0,位于左下。随着阈值的减小,红色和绿色区域增大,紫色和蓝色区域减小。阈值为0时,不管你什么症状,医生都诊断结果都是得病(预测值都为P),此时绿色和红色区域均占整个区域,即紫色和蓝色区域的面积为0,此时 FPR=TPR=1,位于右上。</p>
<p><strong>AUC</strong>:AUC值为ROC曲线所覆盖的区域面积,显然,AUC越大,分类器分类效果越好。</p>
<blockquote>
<p>AUC=1,是完美分类器。</p>
<p>0.5&lt;AUC&lt;1,优于随机猜测,有预测价值。</p>
<p>AUC=0.5,跟随机猜测一样(例:丢铜板),没有预测价值。</p>
<p>AUC &lt; 0.5,比随机猜测还差;但只要总是反预测而行,就优于随机猜测。</p>
</blockquote>
<p><strong>注:</strong> 对于AUC小于0.5的模型,我们可以考虑取反(模型预测为positive,那我们就取negtive),这样就可以保证模型的性能不可能比随机猜测差。</p>
<p>以下为ROC曲线和AUC值的实例:</p>
<p><img src="http://pic4.zhimg.com/v2-92a524d44d915a4a043b267c238643b3_r.jpg" style="width:50%" > </p>
<p>AUC的物理意义:正样本的预测结果大于负样本的预测结果的概率。所以AUC反应的是分类器对样本的排序能力。另外值得注意的是,AUC对样本类别是否均衡并不敏感,这也是不均衡样本通常用AUC评价分类器性能的一个原因。AUC只关注正负样本之间的排序,并不关心正样本内部,或者负样本内部的排序。这也体现了AUC的本质:任意个正样本的概率都大于负样本的概率的能力</p>
<p>AUC的计算:</p>
<blockquote>
<p>(1). AUC为ROC曲线下的面积,那我们直接计算面积可得。面积为一个个小的梯形面积(曲线)之和。计算的精度与阈值的精度有关。</p>
<p>(2). 根据AUC的物理意义,我们计算正样本预测结果大于负样本预测结果的概率。取<span><span class="MathJax_Preview">n_1*n_0</span><script type="math/tex">n_1*n_0</script></span>(<span><span class="MathJax_Preview">n_1</span><script type="math/tex">n_1</script></span>为正样本数,<span><span class="MathJax_Preview">n_0</span><script type="math/tex">n_0</script></span>为负样本数)个二元组,比较score(预测结果),最后得到AUC。时间复杂度为<span><span class="MathJax_Preview">O(N*M)</span><script type="math/tex">O(N*M)</script></span>。</p>
<p>(3). 首先把所有样本按照score排序,依次用rank表示他们,如最大score的样本,<span><span class="MathJax_Preview">rank=n(n=n_0+n_1</span><script type="math/tex">rank=n(n=n_0+n_1</script></span>,其中<span><span class="MathJax_Preview">n_0</span><script type="math/tex">n_0</script></span>为负样本个数,<span><span class="MathJax_Preview">n_1</span><script type="math/tex">n_1</script></span>为正样本个数),其次为n-1。对于正样本中rank最大的样本<span><span class="MathJax_Preview">rank\_max</span><script type="math/tex">rank\_max</script></span>,有<span><span class="MathJax_Preview">n_1-1</span><script type="math/tex">n_1-1</script></span>个其他正样本比它小。有<span><span class="MathJax_Preview">rank\_max-n_1</span><script type="math/tex">rank\_max-n_1</script></span>个负样本比它小。其次为<span><span class="MathJax_Preview">rank\_second-(n_1-1)</span><script type="math/tex">rank\_second-(n_1-1)</script></span>。最后我们得到正样本大于负样本的概率为:</p>
<div>
<div class="MathJax_Preview">AUC=\frac{\sum_{\text{正样本}}rank(core)-n_1*(n_1+2)/2}{n_0*n_1}</div>
<script type="math/tex; mode=display">AUC=\frac{\sum_{\text{正样本}}rank(core)-n_1*(n_1+2)/2}{n_0*n_1}</script>
</div>
<p>(4). 计算复杂度为<span><span class="MathJax_Preview">O(N+M)</span><script type="math/tex">O(N+M)</script></span>。</p>
</blockquote>
<p><strong>ROC和AUC都能应用于非均衡的分类问题</strong>:ROC曲线只与横坐标(FPR)和纵坐标(TPR)有关系,其中:</p>
<div>
<div class="MathJax_Preview">FPR = \frac {FP}{FP+TN}</div>
<script type="math/tex; mode=display">FPR = \frac {FP}{FP+TN}</script>
</div>
<div>
<div class="MathJax_Preview">TPR = \frac {TP}{TP+FN}</div>
<script type="math/tex; mode=display">TPR = \frac {TP}{TP+FN}</script>
</div>
<p>以及混淆矩阵:</p>
<p><img alt="" src="http://pic2.zhimg.com/v2-971343041d399b14e4ba379fce0c6d25_r.jpg" /></p>
<p>可以发现TPR只是正样本中(第一行)预测正确的概率,在正样本内部进行,并没有牵扯到负样本。而FPR只是负样本中(第二行)预测错误的概率,在负样本内部进行,并没有牵扯到正样本。TPR和FPR的计算并没有涉及正负样本的互动(也就是没有跨行)。和正负样本的比例没有关系。因此ROC的值与实际的正负样本比例无关,因此既可以用于均衡问题,也可以用于非均衡问题。而AUC的几何意义为ROC曲线下的面积,因此也和实际的正负样本比例无关。</p>
<ul>
<li><strong>KS(Kolmogorov-Smirnov)</strong></li>
</ul>
<p>KS值是在模型中用于区分预测正负样本分隔程度的评价指标,一般应用于金融风控领域。与ROC曲线相似,ROC是以FPR作为横坐标,TPR作为纵坐标,通过改变不同阈值,从而得到ROC曲线。而在KS曲线中,则是以阈值作为横坐标,以FPR和TPR作为纵坐标,ks曲线则为TPR-FPR,ks曲线的最大值通常为ks值。</p>
<p>为什么这样求KS值呢?我们知道,当阈值减小时,TPR和FPR会同时减小,当阈值增大时,TPR和FPR会同时增大。而在实际工程中,我们希望TPR更大一些,FPR更小一些,即TPR-FPR越大越好,即ks值越大越好。KS值的取值范围是[0,1]。通常来说,值越大,模型区分正负样本的能力越强(一般0.3以上,说明模型的效果比较好)。</p>
<p>以下为KS曲线的实例 :</p>
<p><img src="http://pic3.zhimg.com/80/v2-d84017e07e95e3b43f11b04663c3cb1e_r.jpg" style="width: 35%" > </p>
<ul>
<li><strong>micro与macro</strong></li>
</ul>
<p>假如我们有n个二分类混淆矩阵,评价模型通常有两种方式一种叫macro,一种叫micro。</p>
<blockquote>
<p><strong>macro方法</strong></p>
</blockquote>
<p>计算出各混淆矩阵的Recall,Precision,记为<span><span class="MathJax_Preview">(P_1,R_1),(P_2,R_2),\cdots,(P_n,R_n)</span><script type="math/tex">(P_1,R_1),(P_2,R_2),\cdots,(P_n,R_n)</script></span>:</p>
<div>
<div class="MathJax_Preview">P_i = \frac{TP_i}{TP_i + FP_i}</div>
<script type="math/tex; mode=display">P_i = \frac{TP_i}{TP_i + FP_i}</script>
</div>
<div>
<div class="MathJax_Preview">R_i = \frac{TP_i}{TP_i + FN_i}</div>
<script type="math/tex; mode=display">R_i = \frac{TP_i}{TP_i + FN_i}</script>
</div>
<p>对各个混淆矩阵的Recall,Precision求平均,然后再根据求得的Recall,Precision计算F1。</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
&amp; P_{macro} = \frac1n\sum\limits_{i=1}^n P_i \\
&amp; R_{macro} = \frac1n\sum\limits_{i=1}^n R_i \\
&amp; F1_{macro} = \frac{2\times P_{macro} \times R_{macro}}{ P_{macro} + R_{macro}} 
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
& P_{macro} = \frac1n\sum\limits_{i=1}^n P_i \\
& R_{macro} = \frac1n\sum\limits_{i=1}^n R_i \\
& F1_{macro} = \frac{2\times P_{macro} \times R_{macro}}{ P_{macro} + R_{macro}} 
\end{aligned}
</script>
</div>
<blockquote>
<p><strong>micro方法</strong></p>
</blockquote>
<p>将各混淆矩阵对应的元素进行平均,得到平均混淆矩阵:</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
&amp; \overline {TP} = \frac1n\sum\limits_{i=1}^n (TP)_i  \\
&amp; \overline {TN} = \frac1n\sum\limits_{i=1}^n (TN)_i \\
&amp; \overline {FP} = \frac1n\sum\limits_{i=1}^n (FP)_i \\
&amp; \overline {FN} = \frac1n\sum\limits_{i=1}^n (FN)_i 
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
& \overline {TP} = \frac1n\sum\limits_{i=1}^n (TP)_i  \\
& \overline {TN} = \frac1n\sum\limits_{i=1}^n (TN)_i \\
& \overline {FP} = \frac1n\sum\limits_{i=1}^n (FP)_i \\
& \overline {FN} = \frac1n\sum\limits_{i=1}^n (FN)_i 
\end{aligned}
</script>
</div>
<p>再基于平均混淆矩阵计算Recall,Precision,然后再根据求得的Recall,Precision计算F1:</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
    &amp; {P_{micro}} = \frac{\overline {TP}}{\overline{TP} + \overline{FP}} \\
    &amp; {R_{micro}} = \frac{\overline {TP}}{\overline{TP} + \overline{FN}} \\
    &amp; F1_{micro} = \frac{2\times P_{micro} \times R_{micro}}{ P_{micro} + R_{micro}} \\
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
    & {P_{micro}} = \frac{\overline {TP}}{\overline{TP} + \overline{FP}} \\
    & {R_{micro}} = \frac{\overline {TP}}{\overline{TP} + \overline{FN}} \\
    & F1_{micro} = \frac{2\times P_{micro} \times R_{micro}}{ P_{micro} + R_{micro}} \\
\end{aligned}
</script>
</div>
<h3 id="53">5.3 聚类指标</h3>
<ul>
<li><strong>兰德指数</strong></li>
<li><strong>轮廓系数</strong></li>
<li><strong>互信息</strong></li>
</ul>
<p>https://www.zhihu.com/question/312556066/answer/600228264</p>
<p>https://www.zhihu.com/question/291032522/answer/605843215</p>
<p>https://cloud.tencent.com/developer/article/1352583</p>
<p>https://zhuanlan.zhihu.com/p/44106492</p>
<h2 id="_3">三、经典网络</h2>
<h3 id="31-lenet">3.1 lenet网络</h3>
<p>卷积神经网络的开山之作,麻雀虽小五脏俱全,卷积层、池化层、全链接层一直沿用至今。</p>
<p><img alt="" src="http://cuijiahua.com/wp-content/uploads/2018/01/dl_3_4.jpg" /></p>
<pre><code class="python"># Contains a variant of the LeNet model definition.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow.compat.v1 as tf
import tf_slim as slim

def lenet(images, num_classes=10, is_training=False, dropout_keep_prob=0.5,
          prediction_fn=slim.softmax, scope='LeNet'):
    &quot;&quot;&quot;
    Creates a variant of the LeNet model.

    Note that since the output is a set of 'logits', the values fall in the
    interval of (-infinity, infinity). Consequently, to convert the outputs to a
    probability distribution over the characters, one will need to convert them
    using the softmax function:

        logits = lenet.lenet(images, is_training=False)
        probabilities = tf.nn.softmax(logits)
        predictions = tf.argmax(logits, 1)

    Args:
        images: A batch of `Tensors` of size [batch_size, height, width, channels].
        num_classes: the number of classes in the dataset. If 0 or None, the logits
            layer is omitted and the input features to the logits layer are returned instead.
        is_training: specifies whether or not we're currently training the model.
            This variable will determine the behaviour of the dropout layer.
        dropout_keep_prob: the percentage of activation values that are retained.
        prediction_fn: a function to get predictions out of logits.
        scope: Optional variable_scope.
    Returns:
        net: a 2D Tensor with the logits (pre-softmax activations) if num_classes
            is a non-zero integer, or the inon-dropped-out nput to the logits layer
            if num_classes is 0 or None.
        end_points: a dictionary from components of the network to the corresponding
            activation.
    &quot;&quot;&quot;
    end_points = {}

    with tf.variable_scope(scope, 'LeNet', [images]):
        net = end_points['conv1'] = slim.conv2d(images, 32, [5, 5], scope='conv1')
        net = end_points['pool1'] = slim.max_pool2d(net, [2, 2], 2, scope='pool1')
        net = end_points['conv2'] = slim.conv2d(net, 64, [5, 5], scope='conv2')
        net = end_points['pool2'] = slim.max_pool2d(net, [2, 2], 2, scope='pool2')
        net = slim.flatten(net)
        end_points['Flatten'] = net

        net = end_points['fc3'] = slim.fully_connected(net, 1024, scope='fc3')
        if not num_classes:
            return net, end_points
        net = end_points['dropout3'] = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='dropout3')
        logits = end_points['Logits'] = slim.fully_connected(net, num_classes, activation_fn=None, scope='fc4')

    end_points['Predictions'] = prediction_fn(logits, scope='Predictions')
    return logits, end_points

lenet.default_image_size = 28
def lenet_arg_scope(weight_decay=0.0):
    &quot;&quot;&quot;
    Defines the default lenet argument scope.

    Args:
        weight_decay: The weight decay to use for regularizing the model.

    Returns:
        An `arg_scope` to use for the inception v3 model.
    &quot;&quot;&quot;
    with slim.arg_scope([slim.conv2d, slim.fully_connected],
        weights_regularizer=slim.l2_regularizer(weight_decay),
        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),
        activation_fn=tf.nn.relu) as sc:
    return sc
</code></pre>

  <br>
    <style>
blockquote{
    font-size: 99%;
}
</style>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  CommonHTML: {
    scale: 100
  }
});
</script>
<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    
    
      
    

    <br>
</div>

</body>
</html>