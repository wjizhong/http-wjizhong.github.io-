<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="author" content="wjizhong">
    <link rel="canonical" href="https://wjizhong.github.io/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">
    <link rel="shortcut icon" href="https://pic.pngsucai.com/00/18/26/4a7884c36067e596.jpg">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>深度学习基础 - 图像/视频算法</title>
    <link href="../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../js/jquery-3.2.1.min.js"></script>
    <script src="../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840", url: "#_top", level:1, children: [
              {title: "\u4e00\u3001\u8bc4\u4f30\u6307\u6807", url: "#_2", level:2, children: [
                  {title: "1.1 \u56de\u5f52(Regression)\u7b97\u6cd5\u6307\u6807", url: "#11-regression", level:3, children: [
                  ]},
                  {title: "1.2 \u5e38\u89c1\u7684\u8ddd\u79bb", url: "#12", level:3, children: [
                  ]},
                  {title: "1.3 \u5206\u7c7b(Classification)\u6307\u6807", url: "#13-classification", level:3, children: [
                  ]},
                  {title: "1.4 \u805a\u7c7b\u6307\u6807", url: "#14", level:3, children: [
                  ]},
                  {title: "1.5 \u68c0\u6d4b\u6307\u6807", url: "#15", level:3, children: [
                  ]}, 
              ]},
              {title: "\u4e8c\u3001\u57fa\u672c\u7ed3\u6784", url: "#_3", level:2, children: [
                  {title: "2.1 \u5377\u79ef\u7c7b\u578b", url: "#21", level:3, children: [
                  ]}, 
              ]},
              {title: "\u4e09\u3001\u7ecf\u5178\u7f51\u7edc", url: "#_4", level:2, children: [
                  {title: "3.1 lenet\u7f51\u7edc", url: "#31-lenet", level:3, children: [
                  ]},
                  {title: "Alexnet\u7f51\u7edc", url: "#alexnet", level:3, children: [
                  ]},
                  {title: "VGG\u7f51\u7edc", url: "#vgg", level:3, children: [
                  ]}, 
              ]},
          ]},
        ];
    </script>
    <script src="../../js/base.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    <style>
blockquote{
    font-size: 99%;
}
</style>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  CommonHTML: {
    scale: 100
  }
});
</script>
<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    
    
      
    

    

    <h1 id="_1">深度学习基础</h1>
<h2 id="_2">一、评估指标</h2>
<p>代价函数:<span><span class="MathJax_Preview">f(\theta,y)</span><script type="math/tex">f(\theta,y)</script></span>,又称Cost function,loss function objective function。一般用在训练过程中,用来定义预测值和真实值之间的距离(也就是衡量模型在训练集上的性能),作为模型调整参数的反馈。代价函数越小,模型性能越好。</p>
<p>评判指标:<span><span class="MathJax_Preview">f(\hat y,y)</span><script type="math/tex">f(\hat y,y)</script></span>,一般用于训练和测试过程中,用于评估模型好坏。评判指标越大(或越小),模型越好。</p>
<p>本质上代价函数和评判指标都是一家人,只他们的应用场景不同,分工不同。代价函数是用来优化模型参数的,评价指标是用来评判模型好坏的。</p>
<p>作为代价函数所具备的条件:</p>
<pre><code>函数光滑且可导:可用梯度下降求解极值
函数为凸函数:可用梯度下降求解最优解
......
</code></pre>

<p>例如我们经常使用的分类器评判指标AUC就不能直接被优化,因此我们常采用交叉熵来代替AUC进行优化。一般情况下,交叉熵越小,AUC就会越大。</p>
<h3 id="11-regression">1.1 回归(Regression)算法指标</h3>
<p><img src="http://pic3.zhimg.com/v2-32c48e2a576d9c2157a071f05e2d6d7a_r.jpg" style="width: 35%"></p>
<ul>
<li><strong>平均绝对误差(Mean Absolute Error)</strong></li>
</ul>
<p>平均绝对误差MAE(Mean Absolute Error)又被称为l1范数损失(l1-norm loss):</p>
<div>
<div class="MathJax_Preview">{\rm MAE}(y, \hat{y})=\frac{1}{m}\sum\limits_{i=1}^{n}|y_i-\hat{y}_i|</div>
<script type="math/tex; mode=display">{\rm MAE}(y, \hat{y})=\frac{1}{m}\sum\limits_{i=1}^{n}|y_i-\hat{y}_i|</script>
</div>
<p>MAE不足:MAE虽能较好衡量回归模型的好坏,但是绝对值的存在导致函数不光滑,在某些点上不能求导,可以考虑将绝对值改为残差的平方,这就是均方误差。</p>
<pre><code class="python">def mean_absolute_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average'):
    &quot;&quot;&quot;Mean absolute error regression loss
    Parameters
        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
            Ground truth (correct) target values.
        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
            Estimated target values.
        sample_weight : array-like of shape (n_samples,), optional
            Sample weights.
        multioutput : string in ['raw_values', 'uniform_average'] or array-like of shape (n_outputs)
            Defines aggregating of multiple output values.
            'raw_values' : Returns a full set of errors in case of multioutput input.
            'uniform_average' : Errors of all outputs are averaged with uniform weight.
    Returns
        loss : float or ndarray of floats
            If multioutput is 'raw_values', then mean absolute error is returned for each output separately.
            If multioutput is 'uniform_average' or an ndarray of weights, then the weighted average of all output errors is returned.
            MAE output is non-negative floating point. The best value is 0.0.
    Examples
        &gt;&gt;&gt; from sklearn.metrics import mean_absolute_error
        &gt;&gt;&gt; y_true = [3, -0.5, 2, 7]
        &gt;&gt;&gt; y_pred = [2.5, 0.0, 2, 8]
        &gt;&gt;&gt; mean_absolute_error(y_true, y_pred)
        0.5
        &gt;&gt;&gt; y_true = [[0.5, 1], [-1, 1], [7, -6]]
        &gt;&gt;&gt; y_pred = [[0, 2], [-1, 2], [8, -5]]
        &gt;&gt;&gt; mean_absolute_error(y_true, y_pred)
        0.75
        &gt;&gt;&gt; mean_absolute_error(y_true, y_pred, multioutput='raw_values')
        array([0.5, 1. ])
        &gt;&gt;&gt; mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])
        0.85...
    &quot;&quot;&quot;
    y_type, y_true, y_pred, multioutput = _check_reg_targets(y_true, y_pred, multioutput)
    check_consistent_length(y_true, y_pred, sample_weight)
    output_errors = np.average(np.abs(y_pred - y_true), weights=sample_weight, axis=0)
    if isinstance(multioutput, str):
        if multioutput == 'raw_values':
            return output_errors
        elif multioutput == 'uniform_average':
            # pass None as weights to np.average: uniform mean
            multioutput = None

    return np.average(output_errors, weights=multioutput)
</code></pre>

<ul>
<li><strong>均方误差(Mean Squared Error)</strong></li>
</ul>
<p>均方误差MSE(Mean Squared Error)又被称为l2范数损失(l2-norm loss):</p>
<div>
<div class="MathJax_Preview">{\rm MSE}(y, \hat{y})=\frac{1}{m}\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2</div>
<script type="math/tex; mode=display">{\rm MSE}(y, \hat{y})=\frac{1}{m}\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2</script>
</div>
<p>MSE和方差的性质比较类似,与我们的目标变量的量纲不一致,为了保证量纲一致性,我们需要对MSE进行开方得到RMSE。</p>
<ul>
<li><strong>均方根误差(Root Mean Squared Error)</strong></li>
</ul>
<p>开方之后的MSE称为RMSE,是标准差的表兄弟,如下式所示:</p>
<div>
<div class="MathJax_Preview">{\rm RMSE}(y, \hat{y})=\sqrt {\frac{1}{m}\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2}</div>
<script type="math/tex; mode=display">{\rm RMSE}(y, \hat{y})=\sqrt {\frac{1}{m}\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2}</script>
</div>
<p>上面的几种衡量标准的取值大小与具体的应用场景有关系,很难定义统一的规则来衡量模型的好坏。比如说利用机器学习算法预测上海的房价RMSE在2000元,我们是可以接受的,但是当四五线城市的房价RMSE为2000元,我们还可以接受吗?</p>
<pre><code class="python">def mean_squared_error(y_true, y_pred, *, sample_weight=None,multioutput='uniform_average', squared=True):
    &quot;&quot;&quot;Mean squared error regression loss
    Parameters
        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)
            Ground truth (correct) target values.
        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)
            Estimated target values.
        sample_weight : array-like of shape (n_samples,), optional
            Sample weights.
        multioutput : string in ['raw_values', 'uniform_average'] or array-like of shape (n_outputs)
            Defines aggregating of multiple output values.
                Array-like value defines weights used to average errors.
                'raw_values' : Returns a full set of errors in case of multioutput input.
                'uniform_average' : Errors of all outputs are averaged with uniform weight.
        squared : boolean value, optional (default = True)
            If True returns MSE value, if False returns RMSE value.
    Returns
        loss : float or ndarray of floats
            A non-negative floating point value (the best value is 0.0), or an array of floating point values, one for each individual target.
    Examples
        &gt;&gt;&gt; from sklearn.metrics import mean_squared_error
        &gt;&gt;&gt; y_true = [3, -0.5, 2, 7]
        &gt;&gt;&gt; y_pred = [2.5, 0.0, 2, 8]
        &gt;&gt;&gt; mean_squared_error(y_true, y_pred)
        0.375
        &gt;&gt;&gt; y_true = [3, -0.5, 2, 7]
        &gt;&gt;&gt; y_pred = [2.5, 0.0, 2, 8]
        &gt;&gt;&gt; mean_squared_error(y_true, y_pred, squared=False)
        0.612...
        &gt;&gt;&gt; y_true = [[0.5, 1],[-1, 1],[7, -6]]
        &gt;&gt;&gt; y_pred = [[0, 2],[-1, 2],[8, -5]]
        &gt;&gt;&gt; mean_squared_error(y_true, y_pred)
        0.708...
        &gt;&gt;&gt; mean_squared_error(y_true, y_pred, multioutput='raw_values')
        array([0.41666667, 1.        ])
        &gt;&gt;&gt; mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])
        0.825...
    &quot;&quot;&quot;
    y_type, y_true, y_pred, multioutput = _check_reg_targets(y_true, y_pred, multioutput)
    check_consistent_length(y_true, y_pred, sample_weight)
    output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)
    if isinstance(multioutput, str):
        if multioutput == 'raw_values':
            return output_errors if squared else np.sqrt(output_errors)
        elif multioutput == 'uniform_average':
            # pass None as weights to np.average: uniform mean
            multioutput = None

    mse = np.average(output_errors, weights=multioutput)
    return mse if squared else np.sqrt(mse)
</code></pre>

<ul>
<li><strong>决定系数(Coefficient of determination)</strong></li>
</ul>
<p>变量之所以有价值,就是因为变量是变化的。什么意思呢?比如说一组因变量为[0,0,0,0,0],显然该因变量的结果是一个常数0,我们也没有必要建模对该因变量进行预测。假如一组的因变量为[1,3,7,10,12],该因变量是变化的,也就是有变异,因此需要通过建立回归模型进行预测。这里的变异可以理解为一组数据的方差不为0。</p>
<p>决定系数又称为<span><span class="MathJax_Preview">R^2</span><script type="math/tex">R^2</script></span> score,反应因变量的全部变异能通过回归关系被自变量解释的比例。</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
    &amp; \text{SST} = \sum \limits_i^m(y_i - \bar y)^2 \qquad \text{SST = total sum of squares} \\
    &amp; \text{SSR} = \sum \limits_i^m(\hat y_i - \bar y)^2 \qquad \text{SSR = sum of due to regression} \\
    &amp; \text{SSE} = \sum \limits_i^m(\hat y_i - y_i)^2 \qquad \text{SSE = sum of due to erros} \\
    &amp; \text{SST = SSR + SSE} \\
    &amp; R^2(y,\hat{y})= \frac{\rm SSR}{\rm SST}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
    & \text{SST} = \sum \limits_i^m(y_i - \bar y)^2 \qquad \text{SST = total sum of squares} \\
    & \text{SSR} = \sum \limits_i^m(\hat y_i - \bar y)^2 \qquad \text{SSR = sum of due to regression} \\
    & \text{SSE} = \sum \limits_i^m(\hat y_i - y_i)^2 \qquad \text{SSE = sum of due to erros} \\
    & \text{SST = SSR + SSE} \\
    & R^2(y,\hat{y})= \frac{\rm SSR}{\rm SST}
\end{aligned}
</script>
</div>
<p>如果结果是0,就说明模型预测不能预测因变量。如果结果是1。就说明是函数关系。如果结果是0~1之间的数,就是我们模型的好坏程度。化简上面的公式,分子就变成了我们的均方误差MSE,下面分母就变成了方差:</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
    R^2(y,\hat{y}) &amp;= 1 - \frac{\rm SSE}{\rm SST}=1-\frac{\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2}{\sum\limits_{i=1}^{m}(y_i-\bar{y})^2} \\
    &amp;=1-\frac{\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2/m}{\sum\limits_{i=1}^{m}(y_i-\bar{y})^2/m}= 1 - \frac{\rm MSE(\hat y, y)}{\rm Var(y)} 
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
    R^2(y,\hat{y}) &= 1 - \frac{\rm SSE}{\rm SST}=1-\frac{\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2}{\sum\limits_{i=1}^{m}(y_i-\bar{y})^2} \\
    &=1-\frac{\sum\limits_{i=1}^{m}(y_i-\hat{y}_i)^2/m}{\sum\limits_{i=1}^{m}(y_i-\bar{y})^2/m}= 1 - \frac{\rm MSE(\hat y, y)}{\rm Var(y)} 
\end{aligned}
</script>
</div>
<p>以上的评估指标是基于误差的均值对进行评估的,均值对异常点(outliers)较敏感,如果样本中有一些异常值出现,会对以上指标的值有较大影响,即均值是非鲁棒的。</p>
<ul>
<li><strong>解决评估指标鲁棒性问题</strong></li>
</ul>
<p>我们通常用一下两种方法解决评估指标的鲁棒性问题:</p>
<blockquote>
<p>剔除异常值:设定一个相对误差<span><span class="MathJax_Preview">\frac{|y_i-\hat{y_i}|}{y_i}</span><script type="math/tex">\frac{|y_i-\hat{y_i}|}{y_i}</script></span>,当该值超过一定的阈值时,则认为其是一个异常点,剔除这个异常点,将异常点剔除之后。再计算平均误差来对模型进行评价。</p>
<p>使用误差的分位数来代替:如利用中位数来代替平均数。例如MAPE:<span><span class="MathJax_Preview">MAPE=median(|y_i-\hat{y_i}|/y_i)</span><script type="math/tex">MAPE=median(|y_i-\hat{y_i}|/y_i)</script></span>,MAPE是一个相对误差的中位数,当然也可以使用别的分位数。</p>
</blockquote>
<p>https://zhuanlan.zhihu.com/p/38529433</p>
<h3 id="12">1.2 常见的距离</h3>
<p>在机器学习里,我们的运算一般都是基于向量的,一条用户具有100个特征,那么他对应的就是一个100维的向量,通过计算两个用户对应向量之间的距离值大小,有时候能反映出这两个用户的相似程度。这在后面的KNN算法和K-means算法中很明显。</p>
<p>一般而言,定义一个距离函数<span><span class="MathJax_Preview">d(x,y)</span><script type="math/tex">d(x,y)</script></span>,需要满足下面几个准则:</p>
<blockquote>
<ol>
<li>
<p><span><span class="MathJax_Preview">d(x,x)=0</span><script type="math/tex">d(x,x)=0</script></span>,到自己的距离为0</p>
</li>
<li>
<p><span><span class="MathJax_Preview">d(x,y)&gt;= 0</span><script type="math/tex">d(x,y)>= 0</script></span>,距离非负</p>
</li>
<li>
<p><span><span class="MathJax_Preview">d(x,y)=d(y,x)</span><script type="math/tex">d(x,y)=d(y,x)</script></span>,对称性,如果A到B距离是a,那么B到A的距离也应该是a</p>
</li>
<li>
<p><span><span class="MathJax_Preview">d(x,k)+d(k,y)&gt;= d(x,y)</span><script type="math/tex">d(x,k)+d(k,y)>= d(x,y)</script></span>,三角形法则:(两边之和大于第三边)</p>
</li>
</ol>
</blockquote>
<p>设有两个n维变量<span><span class="MathJax_Preview">A=\left[ x_{11}, x_{12},...,x_{1n} \right]</span><script type="math/tex">A=\left[ x_{11}, x_{12},...,x_{1n} \right]</script></span>和<span><span class="MathJax_Preview">B=\left[ x_{21} ,x_{22} ,...,x_{2n} \right]</span><script type="math/tex">B=\left[ x_{21} ,x_{22} ,...,x_{2n} \right]</script></span>,则一些常用的距离公式定义如下:</p>
<ul>
<li><strong>曼哈顿距离</strong></li>
</ul>
<p>曼哈顿距离也称为城市街区距离,数学定义如下:</p>
<div>
<div class="MathJax_Preview">d_{12} =\sum_{k=1}^{n}{\left| x_{1k}-x_{2k} \right| } </div>
<script type="math/tex; mode=display">d_{12} =\sum_{k=1}^{n}{\left| x_{1k}-x_{2k} \right| } </script>
</div>
<p>曼哈顿距离的python实现:</p>
<pre><code class="python">from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print(sum(abs(vector1-vector2)))
</code></pre>

<ul>
<li><strong>欧氏距离</strong></li>
</ul>
<p>欧氏距离其实就是L2范数,数学定义如下:</p>
<div>
<div class="MathJax_Preview">
d_{12} =\sqrt{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{2} } } 
</div>
<script type="math/tex; mode=display">
d_{12} =\sqrt{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{2} } } 
</script>
</div>
<p>欧氏距离的Python实现:</p>
<pre><code class="python">from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print(sqrt((vector1-vector2)*(vector1-vector2).T))
</code></pre>

<ul>
<li><strong>切比雪夫距离</strong></li>
</ul>
<p>切比雪夫距离就是<span><span class="MathJax_Preview">L_{\infty}</span><script type="math/tex">L_{\infty}</script></span>,即无穷范数,数学表达式如下:</p>
<div>
<div class="MathJax_Preview">
d_{12} =max\left( \left| x_{1k}-x_{2k} \right| \right)
</div>
<script type="math/tex; mode=display">
d_{12} =max\left( \left| x_{1k}-x_{2k} \right| \right)
</script>
</div>
<p>切比雪夫距离额Python实现如下:</p>
<pre><code class="python">from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print(sqrt(abs(vector1-vector2).max))
</code></pre>

<ul>
<li><strong>闵可夫斯基距离</strong></li>
</ul>
<p>从严格意义上讲,闵可夫斯基距离不是一种距离,而是一组距离的定义:</p>
<div>
<div class="MathJax_Preview">
d_{12} =\sqrt[p]{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{p} } } 
</div>
<script type="math/tex; mode=display">
d_{12} =\sqrt[p]{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{p} } } 
</script>
</div>
<p>该距离最常用的p是2和1,前者是欧几里得距离(Euclidean distance),后者是曼哈顿距离(Manhattan distance)。假设在曼哈顿街区乘坐出租车从P点到Q点,白色表示高楼大厦,灰色表示街道:</p>
<p><img alt="" src="http://images.cnitblog.com/blog/533521/201308/07220530-1c87c470c5984305932cb5f5fc91656f.png" /></p>
<p>绿色的斜线表示欧几里得距离,在现实中是不可能的。其他三条折线表示了曼哈顿距离,这三条折线的长度是相等的。当p趋近于无穷大时,闵可夫斯基距离转化成切比雪夫距离(Chebyshev distance)。</p>
<p>我们知道平面上到原点欧几里得距离(p=2)为1的点所组成的形状是一个圆,当p取其他数值的时候呢?</p>
<p><img alt="" src="http://images.cnitblog.com/blog/533521/201308/07220559-ae662025d1394f90bfd62f7c21c3d895.png" /></p>
<p>注意,当p&lt;1时,闵可夫斯基距离不再符合三角形法则,举个例子:当p&lt;1,(0,0)到(1,1)的距离等于(1+1)^{1/p}&gt;2,而(0,1)到这两个点的距离都是1。</p>
<p>闵可夫斯基距离比较直观,但是它与数据的分布有关,具有一定的局限性,如果x方向的幅值远远大于y方向的值,这个距离公式就会过度放大x维度的作用。所以,在计算距离之前,我们可能还需要对数据进行z-transform 处理,即减去均值,除以标准差:</p>
<div>
<div class="MathJax_Preview">
(x_1,y_1)\rightarrow (\frac{x_1-\mu_x}{\sigma_x},\frac{y_1-\mu_y}{\sigma_y})
</div>
<script type="math/tex; mode=display">
(x_1,y_1)\rightarrow (\frac{x_1-\mu_x}{\sigma_x},\frac{y_1-\mu_y}{\sigma_y})
</script>
</div>
<p>其中<span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span>为该维度上的均值,<span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>为该维度上的标准差。</p>
<p>可以看到,上述处理开始体现数据的统计特性了。这种方法在假设数据各个维度不相关的情况下利用数据分布的特性计算出不同的距离。如果维度相互之间数据相关(例如:身高较高的信息很有可能会带来体重较重的信息,因为两者是有关联的),这时候就要用到马氏距离(Mahalanobis distance)了。</p>
<p>可以看到,上述处理开始体现数据的统计特性了。这种方法在假设数据各个维度不相关的情况下利用数据分布的特性计算出不同的距离。如果维度相互之间数据相关(例如:身高较高的信息很有可能会带来体重较重的信息,因为两者是有关联的),这时候就要用到马氏距离（Mahalanobis distance)了。</p>
<ul>
<li><strong>马氏距离</strong></li>
</ul>
<p>马氏距离实际上是利用Cholesky transformation来消除不同维度之间的相关性和尺度不同的性质。假设样本点(列向量)之间的协方差对称矩阵是<span><span class="MathJax_Preview">\Sigma</span><script type="math/tex">\Sigma</script></span>, 通过Cholesky Decomposition(实际上是对称矩阵LU分解的一种特殊形式)可以转化为下三角矩阵和上三角矩阵的乘积:<span><span class="MathJax_Preview">\Sigma=LL^T</span><script type="math/tex">\Sigma=LL^T</script></span>。消除不同维度之间的相关性和尺度不同,只需要对样本点x做如下处理:<span><span class="MathJax_Preview">z=L^{-1}(x-\mu)</span><script type="math/tex">z=L^{-1}(x-\mu)</script></span>。处理之后的欧几里得距离就是原样本的马氏距离):</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
z^Tz &amp;=(L^{-1}(x-\mu))^T(L^{-1}(x-\mu)) \\
     &amp;=(x-\mu)^T\Sigma^{-1}(x-\mu)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
z^Tz &=(L^{-1}(x-\mu))^T(L^{-1}(x-\mu)) \\
     &=(x-\mu)^T\Sigma^{-1}(x-\mu)
\end{aligned}
</script>
</div>
<p><strong>马氏距离的问题</strong>:</p>
<blockquote>
<p>协方差矩阵必须满秩:里面有求逆矩阵的过程,不满秩不行,要求数据要有原维度个特征值,如果没有可以考虑先进行PCA,这种情况下PCA不会损失信息</p>
<p>不能处理非线性流形(manifold)上的问题:只对线性空间有效,如果要处理流形,只能在局部定义,可以用来建立KNN图</p>
</blockquote>
<p>python代码:</p>
<pre><code class="python">import numpy as np
import pylab as pl
import scipy.spatial.distance as dist

def plotSamples(x, y, z=None):
    stars = np.matrix([[3., -2., 0.], [3., 2., 0.]])
    if z is not None:
        x, y = z * np.matrix([x, y])
        stars = z * stars
    pl.scatter(x, y, s=10)    # 画 gaussian 随机点
    pl.scatter(np.array(stars[0]), np.array(stars[1]), s=200, marker='*', color='r')  # 画三个指定点
    pl.axhline(linewidth=2, color='g') # 画 x 轴
    pl.axvline(linewidth=2, color='g')  # 画 y 轴

    pl.axis('equal')
    pl.axis([-5, 5, -5, 5])
    pl.show()

# 产生高斯分布的随机点
mean = [0, 0]      # 平均值
cov = [[2, 1], [1, 2]]   # 协方差
x, y = np.random.multivariate_normal(mean, cov, 1000).T
plotSamples(x, y)

covMat = np.matrix(np.cov(x, y))    # 求 x 与 y 的协方差矩阵
Z = np.linalg.cholesky(covMat).I  # 仿射矩阵
plotSamples(x, y, Z)

# 求马氏距离 
print('\n到原点的马氏距离分别是:')
print(dist.mahalanobis([0,0], [3,3], covMat.I), dist.mahalanobis([0,0], [-2,2], covMat.I))

# 求变换后的欧几里得距离
dots = (Z * np.matrix([[3, -2, 0], [3, 2, 0]])).T
print('\n变换后到原点的欧几里得距离分别是:')
print(dist.minkowski([0, 0], np.array(dots[0]), 2), dist.minkowski([0, 0], np.array(dots[1]), 2))
</code></pre>

<ul>
<li><strong>夹角余弦</strong></li>
</ul>
<p>夹角余弦的取值范围为[-1,1],可以用来衡量两个向量方向的差异;夹角余弦越大,表示两个向量的夹角越小;当两个向量的方向重合时,夹角余弦取最大值1;当两个向量的方向完全相反时,夹角余弦取最小值-1。</p>
<p>机器学习中用这一概念来衡量样本向量之间的差异,其数学表达式如下:</p>
<div>
<div class="MathJax_Preview">
cos\theta =\frac{AB}{\left| A \right| \left|B \right| } =\frac{\sum_{k=1}^{n}{x_{1k}x_{2k} } }{\sqrt{\sum_{k=1}^{n}{x_{1k}^{2} } } \sqrt{\sum_{k=1}^{n}{x_{2k}^{2} } } } 
</div>
<script type="math/tex; mode=display">
cos\theta =\frac{AB}{\left| A \right| \left|B \right| } =\frac{\sum_{k=1}^{n}{x_{1k}x_{2k} } }{\sqrt{\sum_{k=1}^{n}{x_{1k}^{2} } } \sqrt{\sum_{k=1}^{n}{x_{2k}^{2} } } } 
</script>
</div>
<p>夹角余弦的python实现:</p>
<pre><code class="python">from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print(dot(vector1,vector2)/(linalg.norm(vector1)*linalg.norm(vector2)))
</code></pre>

<ul>
<li><strong>汉明距离</strong></li>
</ul>
<p>汉明距离定义的是两个字符串中不相同位数的数目。例如:字符串‘1111’与‘1001’之间的汉明距离为2。信息编码中一般应使得编码间的汉明距离尽可能的小。</p>
<p>汉明距离的python实现:</p>
<pre><code class="python">from numpy import *
matV = mat([1,1,1,1],[1,0,0,1])
smstr = nonzero(matV[0]-matV[1])
print(smstr)
</code></pre>

<ul>
<li><strong>杰卡德相似系数</strong></li>
</ul>
<p>两个集合A和B的交集元素在A和B的并集中所占的比例称为两个集合的杰卡德相似系数,用符号<span><span class="MathJax_Preview">J(A,B)</span><script type="math/tex">J(A,B)</script></span>表示,数学表达式为:</p>
<div>
<div class="MathJax_Preview">
J\left( A,B \right) =\frac{\left| A\cap B\right| }{\left|A\cup B \right| } 
</div>
<script type="math/tex; mode=display">
J\left( A,B \right) =\frac{\left| A\cap B\right| }{\left|A\cup B \right| } 
</script>
</div>
<p>杰卡德相似系数是衡量两个集合的相似度的一种指标。一般可以将其用在衡量样本的相似度上。</p>
<ul>
<li><strong>杰卡德距离</strong></li>
</ul>
<p>与杰卡德相似系数相反的概念是杰卡德距离,其定义式为:</p>
<div>
<div class="MathJax_Preview">
J_{\sigma} =1-J\left( A,B \right) =\frac{\left| A\cup B \right| -\left| A\cap B \right| }{\left| A\cup B \right| } 
</div>
<script type="math/tex; mode=display">
J_{\sigma} =1-J\left( A,B \right) =\frac{\left| A\cup B \right| -\left| A\cap B \right| }{\left| A\cup B \right| } 
</script>
</div>
<p>杰卡德距离的python实现:</p>
<pre><code class="python">from numpy import *
import scipy.spatial.distance as dist
matV = mat([1,1,1,1],[1,0,0,1])
print(dist.pdist(matV,'jaccard'))
</code></pre>

<ul>
<li><strong>KL散度</strong></li>
</ul>
<p>KL散度不对称。如果分布P和Q，KL(P||Q)很大而KL(Q||P)很小</p>
<h3 id="13-classification">1.3 分类(Classification)指标</h3>
<p><img src="http://pic1.zhimg.com/v2-49a657a2ec9fa94edb976ca1a7d33afc_r.jpg" style="width: 80%"></p>
<ul>
<li><strong>混淆矩阵</strong></li>
</ul>
<p>在预测系统中,牵扯到预测值,真实值,以及真实值和预测值之间的关系,进而产生了混淆矩阵:</p>
<table>
    <tr>
        <td style="width: 40%;"><img width="100%"  src="http://pic4.zhimg.com/v2-b97dab4ad52b9d0c7dac87c9b81acebf.jpg" /></td>
        <td><img width="50%" src="http://pic3.zhimg.com/80/v2-76b9176719868e9b85bedf5192e722d3_hd.jpg" /></td>
    </tr><tr>
        <td style="color:orange;font-size:13px;color:#995;">多分类下的混淆矩阵</td>
        <td style="color:orange;font-size:13px;color:#995;">二分类下的混淆矩阵</td>
    </tr>
</table>

<p>混淆矩阵又被称为错误矩阵,在每个类别下,模型预测错误的结果数量,以及错误预测的类别和正确预测的数量都在一个矩阵下面显示出来,方便直观的评估模型分类的结果。</p>
<p>通常取预测值和真实值之间的关系、预测值对矩阵进行划分:</p>
<blockquote>
<p>True positive(TP):真实值为Positive,预测正确(预测值为Positive)</p>
<p>True negative(TN):真实值为Negative,预测正确(预测值为Negative)</p>
<p>False positive(FP):真实值为Negative,预测错误(预测值为Positive),第一类错误,Type I error。</p>
<p>False negative(FN): 真实值为Positive,预测错误(预测值为 Negative),第二类错误,Type II error。</p>
</blockquote>
<ul>
<li><strong>精确率(Precision)</strong></li>
</ul>
<p>精确率是针对我们预测结果而言的,它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了,一种就是把正类预测为正类(TP),另一种就是把负类预测为正类(FP),也就是</p>
<div>
<div class="MathJax_Preview">P = \frac{TP}{TP+FP}</div>
<script type="math/tex; mode=display">P = \frac{TP}{TP+FP}</script>
</div>
<p>精确率取值范围为[0,1],取值越大,模型预测能力越好。</p>
<ul>
<li><strong>召回率(Recall)</strong></li>
</ul>
<p>针对我们原来的样本而言的,它表示的是样本中的正例有多少被预测正确了。那也有两种可能,一种是把原来的正类预测成正类(TP),另一种就是把原来的正类预测为负类(FN)。其实就是分母不同,一个分母是预测为正的样本数,另一个是原来样本中所有的正样本数。</p>
<div>
<div class="MathJax_Preview">R=\frac{TP}{TP+FN}</div>
<script type="math/tex; mode=display">R=\frac{TP}{TP+FN}</script>
</div>
<p>在信息检索领域,精确率和召回率又被称为查准率和查全率:</p>
<blockquote>
<p>查准率＝检索出的相关信息量 / 检索出的信息总量</p>
<p>查全率＝检索出的相关信息量 / 系统中的相关信息总量</p>
</blockquote>
<ul>
<li><strong>准确率(Accuracy)</strong></li>
</ul>
<p>针对所有的样本,样本预测正确的数量占总数据:</p>
<div>
<div class="MathJax_Preview">
Acc=\frac{TP+TN}{TP+FN+FP+TN}
</div>
<script type="math/tex; mode=display">
Acc=\frac{TP+TN}{TP+FN+FP+TN}
</script>
</div>
<ul>
<li><strong>引申指标</strong></li>
</ul>
<p>用样本中的正类和负类进行计算的定义</p>
<table>
<thead>
<tr>
<th>缩写</th>
<th>全称</th>
<th>等价称呼</th>
<th>计算公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>TPR</td>
<td>True Positive Rate</td>
<td>真正类率,Recall Sensitivity</td>
<td><span><span class="MathJax_Preview">\frac{TP}{TP+FN}</span><script type="math/tex">\frac{TP}{TP+FN}</script></span></td>
</tr>
<tr>
<td>FNR</td>
<td>False Negative Rate</td>
<td>假负类率,Miss rate Type rs error</td>
<td><span><span class="MathJax_Preview">\frac{TN}{TP+FN}</span><script type="math/tex">\frac{TN}{TP+FN}</script></span></td>
</tr>
<tr>
<td>FPR</td>
<td>False Positive Rate</td>
<td>假正类率,fall-out Type 1 error</td>
<td><span><span class="MathJax_Preview">\frac{FP}{FP+FN}=1-TNR</span><script type="math/tex">\frac{FP}{FP+FN}=1-TNR</script></span></td>
</tr>
<tr>
<td>TNR</td>
<td>True Negative Rate</td>
<td>真负类率,Specificity</td>
<td><span><span class="MathJax_Preview">\frac{TN}{TN+FP}</span><script type="math/tex">\frac{TN}{TN+FP}</script></span></td>
</tr>
</tbody>
</table>
<p>用预测结果的正类和负类进行计算的定义</p>
<table>
<thead>
<tr>
<th>缩写</th>
<th>全称</th>
<th>等价称呼</th>
<th>计算公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>PPV</td>
<td>Positive Predictive Value</td>
<td>正类预测率,Precision</td>
<td><span><span class="MathJax_Preview">\frac{TP}{TP+FP}</span><script type="math/tex">\frac{TP}{TP+FP}</script></span></td>
</tr>
<tr>
<td>FOR</td>
<td>False Omission Rata</td>
<td>假错误率</td>
<td><span><span class="MathJax_Preview">\frac{FN}{TN+FN}=1-NPV</span><script type="math/tex">\frac{FN}{TN+FN}=1-NPV</script></span></td>
</tr>
<tr>
<td>FDR</td>
<td>False Discovery Rate</td>
<td>假发现率</td>
<td><span><span class="MathJax_Preview">\frac{FP}{TP+FP}</span><script type="math/tex">\frac{FP}{TP+FP}</script></span></td>
</tr>
<tr>
<td>NPV</td>
<td>Negative Predictive Value</td>
<td>负类预测率</td>
<td><span><span class="MathJax_Preview">\frac{TN}{TN+FN}</span><script type="math/tex">\frac{TN}{TN+FN}</script></span></td>
</tr>
</tbody>
</table>
<p>其他定义概念</p>
<table>
<thead>
<tr>
<th>缩写</th>
<th>全称</th>
<th>等价称呼</th>
<th>计算公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>ACC</td>
<td>Accuracy</td>
<td>准确率</td>
<td><span><span class="MathJax_Preview">\frac{TP+TN}{TP+FN+FP+TN}</span><script type="math/tex">\frac{TP+TN}{TP+FN+FP+TN}</script></span></td>
</tr>
<tr>
<td>LR+</td>
<td>Positive Likelihood Ratio</td>
<td>正类似然比</td>
<td><span><span class="MathJax_Preview">\frac{TPR}{FPR}</span><script type="math/tex">\frac{TPR}{FPR}</script></span></td>
</tr>
<tr>
<td>LR-</td>
<td>Negative likelihood ratio</td>
<td>负类似然比</td>
<td><span><span class="MathJax_Preview">\frac{FNR}{TNR}</span><script type="math/tex">\frac{FNR}{TNR}</script></span></td>
</tr>
<tr>
<td>DOR</td>
<td>Diagnostic odds ratio</td>
<td>诊断胜算比</td>
<td><span><span class="MathJax_Preview">\frac{LR+}{LR-}</span><script type="math/tex">\frac{LR+}{LR-}</script></span></td>
</tr>
<tr>
<td>F1 score</td>
<td>F1 test measure</td>
<td>F1值</td>
<td><span><span class="MathJax_Preview">\frac{2*Recall*Precision}{Recall+Precision}</span><script type="math/tex">\frac{2*Recall*Precision}{Recall+Precision}</script></span></td>
</tr>
<tr>
<td>MCC</td>
<td>Matthews Correlation coefficient</td>
<td>马修斯相关性系数</td>
<td><span><span class="MathJax_Preview">\frac{TP*TN-FP*FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}</span><script type="math/tex">\frac{TP*TN-FP*FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}</script></span></td>
</tr>
<tr>
<td>BM</td>
<td>Bookmaker Informedness</td>
<td>Informedness</td>
<td><span><span class="MathJax_Preview">TPR+TNR-1</span><script type="math/tex">TPR+TNR-1</script></span></td>
</tr>
</tbody>
</table>
<p>LR+/-指的是似然比,LR+越大表示模型对正类的分类越好,LR-越大表示模型对负类的分类效果越好。F1值是精确值和召回率的调和均值,其实原公式是<span><span class="MathJax_Preview">F_{\beta}=(1+\beta^2)\times\frac{\text{P}\times\text{R}}{\beta^2\times\text{P}+\text{R}}</span><script type="math/tex">F_{\beta}=(1+\beta^2)\times\frac{\text{P}\times\text{R}}{\beta^2\times\text{P}+\text{R}}</script></span>,这里的<span><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>表示:<strong>召回率的权重是准确率的<span><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>倍</strong>。即F值是一种精确率和召回率的综合指标,权重由<span><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>决定。MCC值在[-1,1]之间,靠近1表示完全预测正确m靠近-1表示完全悖论,0表示随机预测</p>
<pre><code class="python">def _check_zero_division(zero_division):
    if isinstance(zero_division, str) and zero_division == &quot;warn&quot;:
        return
    elif isinstance(zero_division, (int, float)) and zero_division in [0, 1]:
        return
    raise ValueError('Got zero_division={0}. Must be one of [&quot;warn&quot;, 0, 1]'.format(zero_division))

# 权重求和
def _weighted_sum(sample_score, sample_weight, normalize=False):
    if normalize:
        return np.average(sample_score, weights=sample_weight)
    elif sample_weight is not None:
        return np.dot(sample_score, sample_weight)
    else:
        return sample_score.sum()

########################### 计算准确率 #################################
def accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None):
    &quot;&quot;&quot;Accuracy classification score.
    Parameters
      y_true : 1d array-like, or label indicator array / sparse matrix
        Ground truth (correct) labels.
      y_pred : 1d array-like, or label indicator array / sparse matrix
        Predicted labels, as returned by a classifier.
      normalize : bool, optional (default=True)
          If False, return the number of correctly classified samples.
          Otherwise, return the fraction of correctly classified samples.
      sample_weight : array-like of shape (n_samples,), default=None
        Sample weights.
    Returns
      score : float
        If normalize == True, return the fraction of correctly classified samples (float), else returns the number of correctly classified samples (int).

    Examples
        &gt;&gt;&gt; from sklearn.metrics import accuracy_score
        &gt;&gt;&gt; y_pred = [0, 2, 1, 3]
        &gt;&gt;&gt; y_true = [0, 1, 2, 3]
        &gt;&gt;&gt; accuracy_score(y_true, y_pred)
        0.5
        &gt;&gt;&gt; accuracy_score(y_true, y_pred, normalize=False)
        2
        In the multilabel case with binary label indicators:
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
        0.5
    &quot;&quot;&quot;

    # Compute accuracy for each possible representation
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)    # 检核类型
    check_consistent_length(y_true, y_pred, sample_weight)     # 检验长度

    # 以下为核心代码
    if y_type.startswith('multilabel'):
        differing_labels = count_nonzero(y_true - y_pred, axis=1)
        score = differing_labels == 0
    else:
        score = y_true == y_pred

    return _weighted_sum(score, sample_weight, normalize)

########################### 计算多分类的混淆矩阵 #######################################
def confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None):
  &quot;&quot;&quot;Compute confusion matrix to evaluate the accuracy of a classification.
    Parameters
      y_true : array-like of shape (n_samples,)
        Ground truth (correct) target values.
      y_pred : array-like of shape (n_samples,)
        Estimated targets as returned by a classifier.
      labels : array-like of shape (n_classes), default=None
        List of labels to index the matrix. This may be used to reorder or select a subset of labels.
      sample_weight : array-like of shape (n_samples,), default=None Sample weights.
      normalize : {'true', 'pred', 'all'}, default=None
        Normalizes confusion matrix over the true (rows), predicted (columns)
        conditions or all the population. If None, confusion matrix will not be
        normalized.
    Returns
      C : ndarray of shape (n_classes, n_classes)
        Confusion matrix whose i-th row and j-th column entry indicates the number of samples with true label being i-th class and prediced label being j-th class.
    Examples
        &gt;&gt;&gt; from sklearn.metrics import confusion_matrix
        &gt;&gt;&gt; y_true = [2, 0, 2, 2, 0, 1]
        &gt;&gt;&gt; y_pred = [0, 0, 2, 2, 0, 2]
        &gt;&gt;&gt; confusion_matrix(y_true, y_pred)
        array([[2, 0, 0],
               [0, 0, 1],
               [1, 0, 2]])
        &gt;&gt;&gt; y_true = [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;]
        &gt;&gt;&gt; y_pred = [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;]
        &gt;&gt;&gt; confusion_matrix(y_true, y_pred, labels=[&quot;ant&quot;, &quot;bird&quot;, &quot;cat&quot;])
        array([[2, 0, 0],
               [0, 0, 1],
               [1, 0, 2]])
        In the binary case, we can extract true positives, etc as follows:
        &gt;&gt;&gt; tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()
        &gt;&gt;&gt; (tn, fp, fn, tp)
        (0, 2, 1, 1)
    &quot;&quot;&quot;
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    if y_type not in (&quot;binary&quot;, &quot;multiclass&quot;):
        raise ValueError(&quot;%s is not supported&quot; % y_type)

    if labels is None:
        labels = unique_labels(y_true, y_pred)
    else:
        labels = np.asarray(labels)
        n_labels = labels.size
        if n_labels == 0:
            raise ValueError(&quot;'labels' should contains at least one label.&quot;)
        elif y_true.size == 0:
            return np.zeros((n_labels, n_labels), dtype=np.int)
        elif np.all([l not in y_true for l in labels]):
            raise ValueError(&quot;At least one label specified must be in y_true&quot;)

    if sample_weight is None:
        sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
    else:
        sample_weight = np.asarray(sample_weight)

    check_consistent_length(y_true, y_pred, sample_weight)

    if normalize not in ['true', 'pred', 'all', None]:
        raise ValueError(&quot;normalize must be one of {'true', 'pred', 'all', None}&quot;)

    n_labels = labels.size
    label_to_ind = {y: x for x, y in enumerate(labels)}
    # convert yt, yp into index
    y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
    y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])

    # intersect y_pred, y_true with labels, eliminate items not in labels
    ind = np.logical_and(y_pred &lt; n_labels, y_true &lt; n_labels)
    y_pred = y_pred[ind]
    y_true = y_true[ind]
    # also eliminate weights of eliminated items
    sample_weight = sample_weight[ind]

    # Choose the accumulator dtype to always have high precision
    if sample_weight.dtype.kind in {'i', 'u', 'b'}:
        dtype = np.int64
    else:
        dtype = np.float64

    cm = coo_matrix((sample_weight, (y_true, y_pred)),shape=(n_labels, n_labels), dtype=dtype,).toarray()      # scipy.sparse.coo_matrix

    with np.errstate(all='ignore'):
        if normalize == 'true':
            cm = cm / cm.sum(axis=1, keepdims=True)
        elif normalize == 'pred':
            cm = cm / cm.sum(axis=0, keepdims=True)
        elif normalize == 'all':
            cm = cm / cm.sum()
        cm = np.nan_to_num(cm)
    return cm


########################### 计算多标签的混淆矩阵 #######################################
def multilabel_confusion_matrix(y_true, y_pred, *, sample_weight=None,
    labels=None, samplewise=False):
  &quot;&quot;&quot;Compute a confusion matrix for each class or sample
    Parameters
      y_true : 1d array-like, or label indicator array / sparse matrix of shape (n_samples, n_outputs) or (n_samples,)
        Ground truth (correct) target values.
      y_pred : 1d array-like, or label indicator array / sparse matrix of shape (n_samples, n_outputs) or (n_samples,)
        Estimated targets as returned by a classifier
      sample_weight : array-like of shape (n_samples,), default=None Sample weights
      labels : array-like
        A list of classes or column indices to select some (or to force inclusion of classes absent from the data)
      samplewise : bool, default=False
        In the multilabel case, this calculates a confusion matrix per sample
    Returns
      multi_confusion : array, shape (n_outputs, 2, 2)
        A 2x2 confusion matrix corresponding to each output in the input.
        When calculating class-wise multi_confusion (default), then
        n_outputs = n_labels; when calculating sample-wise multi_confusion
        (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,
        the results will be returned in the order specified in ``labels``,
        otherwise the results will be returned in sorted order by default.
    Multilabel-indicator case:
    &gt;&gt;&gt; import numpy as np
    &gt;&gt;&gt; from sklearn.metrics import multilabel_confusion_matrix
    &gt;&gt;&gt; y_true = np.array([[1, 0, 1],
    ...                    [0, 1, 0]])
    &gt;&gt;&gt; y_pred = np.array([[1, 0, 0],
    ...                    [0, 1, 1]])
    &gt;&gt;&gt; multilabel_confusion_matrix(y_true, y_pred)
    array([[[1, 0],
            [0, 1]],
    &lt;BLANKLINE&gt;
           [[1, 0],
            [0, 1]],
    &lt;BLANKLINE&gt;
           [[0, 1],
            [1, 0]]])
    Multiclass case:
    &gt;&gt;&gt; y_true = [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;]
    &gt;&gt;&gt; y_pred = [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;]
    &gt;&gt;&gt; multilabel_confusion_matrix(y_true, y_pred,
    ...                             labels=[&quot;ant&quot;, &quot;bird&quot;, &quot;cat&quot;])
    array([[[3, 1],
            [0, 2]],
    &lt;BLANKLINE&gt;
           [[5, 0],
            [1, 0]],
    &lt;BLANKLINE&gt;
           [[2, 1],
            [1, 2]]])
    &quot;&quot;&quot;
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    if sample_weight is not None:
        sample_weight = column_or_1d(sample_weight)
    check_consistent_length(y_true, y_pred, sample_weight)

    if y_type not in (&quot;binary&quot;, &quot;multiclass&quot;, &quot;multilabel-indicator&quot;):
        raise ValueError(&quot;%s is not supported&quot; % y_type)

    present_labels = unique_labels(y_true, y_pred)
    if labels is None:
        labels = present_labels
        n_labels = None
    else:
        n_labels = len(labels)
        labels = np.hstack([labels, np.setdiff1d(present_labels, labels,assume_unique=True)])

    if y_true.ndim == 1:
        if samplewise:
            raise ValueError(&quot;Samplewise metrics are not available outside of multilabel classification.&quot;)

        le = LabelEncoder()
        le.fit(labels)
        y_true = le.transform(y_true)
        y_pred = le.transform(y_pred)
        sorted_labels = le.classes_

        # labels are now from 0 to len(labels) - 1 -&gt; use bincount
        tp = y_true == y_pred
        tp_bins = y_true[tp]
        if sample_weight is not None:
            tp_bins_weights = np.asarray(sample_weight)[tp]
        else:
            tp_bins_weights = None

        if len(tp_bins):
            tp_sum = np.bincount(tp_bins, weights=tp_bins_weights,
                                 minlength=len(labels))
        else:
            # Pathological case
            true_sum = pred_sum = tp_sum = np.zeros(len(labels))
        if len(y_pred):
            pred_sum = np.bincount(y_pred, weights=sample_weight,
                                   minlength=len(labels))
        if len(y_true):
            true_sum = np.bincount(y_true, weights=sample_weight,
                                   minlength=len(labels))

        # Retain only selected labels
        indices = np.searchsorted(sorted_labels, labels[:n_labels])
        tp_sum = tp_sum[indices]
        true_sum = true_sum[indices]
        pred_sum = pred_sum[indices]

    else:
        sum_axis = 1 if samplewise else 0

        # All labels are index integers for multilabel.
        # Select labels:
        if not np.array_equal(labels, present_labels):
            if np.max(labels) &gt; np.max(present_labels):
                raise ValueError('All labels must be in [0, n labels) for '
                                 'multilabel targets. '
                                 'Got %d &gt; %d' %
                                 (np.max(labels), np.max(present_labels)))
            if np.min(labels) &lt; 0:
                raise ValueError('All labels must be in [0, n labels) for '
                                 'multilabel targets. '
                                 'Got %d &lt; 0' % np.min(labels))

        if n_labels is not None:
            y_true = y_true[:, labels[:n_labels]]
            y_pred = y_pred[:, labels[:n_labels]]

        # calculate weighted counts
        true_and_pred = y_true.multiply(y_pred)
        tp_sum = count_nonzero(true_and_pred, axis=sum_axis,
                               sample_weight=sample_weight)
        pred_sum = count_nonzero(y_pred, axis=sum_axis,
                                 sample_weight=sample_weight)
        true_sum = count_nonzero(y_true, axis=sum_axis,
                                 sample_weight=sample_weight)

    fp = pred_sum - tp_sum
    fn = true_sum - tp_sum
    tp = tp_sum

    if sample_weight is not None and samplewise:
        sample_weight = np.array(sample_weight)
        tp = np.array(tp)
        fp = np.array(fp)
        fn = np.array(fn)
        tn = sample_weight * y_true.shape[1] - tp - fp - fn
    elif sample_weight is not None:
        tn = sum(sample_weight) - tp - fp - fn
    elif samplewise:
        tn = y_true.shape[1] - tp - fp - fn
    else:
        tn = y_true.shape[0] - tp - fp - fn

    return np.array([tn, fp, fn, tp]).T.reshape(-1, 2, 2)


########################### 计算多标签的精确率/召回率/F分数 #######################################
def precision_recall_fscore_support(y_true, y_pred, *, beta=1.0, labels=None, pos_label=1, 
        average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None,
        zero_division=&quot;warn&quot;):
    &quot;&quot;&quot;Compute precision, recall, F-measure and support for each class
    Parameters
        y_true : 1d array-like, or label indicator array / sparse matrix
            Ground truth (correct) target values.
        y_pred : 1d array-like, or label indicator array / sparse matrix
            Estimated targets as returned by a classifier.
        beta : float, 1.0 by default
            The strength of recall versus precision in the F-score.
        labels : list, optional
            The set of labels to include when average != 'binary', and their
            order if average is None. Labels present in the data can be
            excluded, for example to calculate a multiclass average ignoring a
            majority negative class, while labels not present in the data will
            result in 0 components in a macro average. For multilabel targets,
            labels are column indices. By default, all labels in ``y_true`` and
            y_pred are used in sorted order.
        pos_label : str or int, 1 by default
            The class to report if ``average='binary'`` and the data is binary.
            If the data are multiclass or multilabel, this will be ignored;
            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report
            scores for that label only.
        average : string, [None (default), 'binary', 'micro', 'macro', 'samples', 'weighted']
            If ``None``, the scores for each class are returned. Otherwise, this
            determines the type of averaging performed on the data:
            ``'binary'``:
                Only report results for the class specified by ``pos_label``.
                This is applicable only if targets (``y_{true,pred}``) are binary.
            ``'micro'``:
                Calculate metrics globally by counting the total true positives,
                false negatives and false positives.
            ``'macro'``:
                Calculate metrics for each label, and find their unweighted
                mean.  This does not take label imbalance into account.
            ``'weighted'``:
                Calculate metrics for each label, and find their average weighted
                by support (the number of true instances for each label). This
                alters 'macro' to account for label imbalance; it can result in an
                F-score that is not between precision and recall.
            ``'samples'``:
                Calculate metrics for each instance, and find their average (only
                meaningful for multilabel classification where this differs from
                :func:`accuracy_score`).
        warn_for : tuple or set, for internal use
            This determines which warnings will be made in the case that this
            function is being used to return only one of its metrics.
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights.
        zero_division : &quot;warn&quot;, 0 or 1, default=&quot;warn&quot;
            Sets the value to return when there is a zero division:
               - recall: when there are no positive labels
               - precision: when there are no positive predictions
               - f-score: both
            If set to &quot;warn&quot;, this acts as 0, but warnings are also raised.
    Returns
        precision : float (if average is not None) or array of float, shape =\
            [n_unique_labels]
        recall : float (if average is not None) or array of float, , shape =\
            [n_unique_labels]
        fbeta_score : float (if average is not None) or array of float, shape =\
            [n_unique_labels]
        support : None (if average is not None) or array of int, shape =\
            [n_unique_labels]
            The number of occurrences of each label in ``y_true``.
    Examples
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; from sklearn.metrics import precision_recall_fscore_support
        &gt;&gt;&gt; y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])
        &gt;&gt;&gt; y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])
        &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average='macro')
        (0.22..., 0.33..., 0.26..., None)
        &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average='micro')
        (0.33..., 0.33..., 0.33..., None)
        &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average='weighted')
        (0.22..., 0.33..., 0.26..., None)
        It is possible to compute per-label precisions, recalls, F1-scores and
        supports instead of averaging:
        &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average=None,
        ... labels=['pig', 'dog', 'cat'])
        (array([0.        , 0.        , 0.66...]),
         array([0., 0., 1.]), array([0. , 0. , 0.8]),
         array([2, 2, 2]))
    &quot;&quot;&quot;
    _check_zero_division(zero_division)
    if beta &lt; 0:
        raise ValueError(&quot;beta should be &gt;=0 in the F-beta score&quot;)
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)

    # Calculate tp_sum, pred_sum, true_sum ###
    samplewise = average == 'samples'
    MCM = multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight,
        labels=labels, samplewise=samplewise)
    tp_sum = MCM[:, 1, 1]
    pred_sum = tp_sum + MCM[:, 0, 1]
    true_sum = tp_sum + MCM[:, 1, 0]

    if average == 'micro':
        tp_sum = np.array([tp_sum.sum()])
        pred_sum = np.array([pred_sum.sum()])
        true_sum = np.array([true_sum.sum()])

    # Finally, we have all our sufficient statistics. Divide! #
    beta2 = beta ** 2

    # Divide, and on zero-division, set scores and/or warn according to
    # zero_division:
    precision = _prf_divide(tp_sum, pred_sum, 'precision','predicted', 
        average, warn_for, zero_division)
    recall = _prf_divide(tp_sum, true_sum, 'recall', 'true', 
        average, warn_for, zero_division)

    # warn for f-score only if zero_division is warn, it is in warn_for
    # and BOTH prec and rec are ill-defined
    if zero_division == &quot;warn&quot; and (&quot;f-score&quot;,) == warn_for:
        if (pred_sum[true_sum == 0] == 0).any():
            _warn_prf(average, &quot;true nor predicted&quot;, 'F-score is', len(true_sum))

    # if tp == 0 F will be 1 only if all predictions are zero, all labels are
    # zero, and zero_division=1. In all other case, 0
    if np.isposinf(beta):
        f_score = recall
    else:
        denom = beta2 * precision + recall

        denom[denom == 0.] = 1  # avoid division by 0
        f_score = (1 + beta2) * precision * recall / denom

    # Average the results
    if average == 'weighted':
        weights = true_sum
        if weights.sum() == 0:
            zero_division_value = 0.0 if zero_division in [&quot;warn&quot;, 0] else 1.0
            # precision is zero_division if there are no positive predictions
            # recall is zero_division if there are no positive labels
            # fscore is zero_division if all labels AND predictions are
            # negative
            return (zero_division_value if pred_sum.sum() == 0 else 0,
                    zero_division_value,
                    zero_division_value if pred_sum.sum() == 0 else 0,
                    None)

    elif average == 'samples':
        weights = sample_weight
    else:
        weights = None

    if average is not None:
        assert average != 'binary' or len(precision) == 1
        precision = np.average(precision, weights=weights)
        recall = np.average(recall, weights=weights)
        f_score = np.average(f_score, weights=weights)
        true_sum = None  # return no support

    return precision, recall, f_score, true_sum
</code></pre>

<ul>
<li><strong>AP与mAP</strong></li>
</ul>
<p>二分类问题的P-R曲线(precision-recall curve),P-R曲线下面与x轴围成的面积称为average precision(AP)。显然通过积分来计算</p>
<div>
<div class="MathJax_Preview">
AP=\int_0^1P(r)dr
</div>
<script type="math/tex; mode=display">
AP=\int_0^1P(r)dr
</script>
</div>
<p>但通常情况下都是使用估算或者插值的方式计算。</p>
<p>mAP(mean average precision)的意义是为了评估你整个目标检测模型的准确度。方法是:计算每个分类的AP,求和再平均,得到的就是mAP</p>
<ul>
<li><strong><span><span class="MathJax_Preview">F_\beta</span><script type="math/tex">F_\beta</script></span> Score</strong></li>
</ul>
<p>Precision和Recall是互相影响的,理想情况下肯定是做到两者都高,但是一般情况下Precision高、Recall就低,Recall高、Precision就低。为了均衡两个指标,我们可以采用Precision和Recall的加权调和平均(weighted harmonic mean)来衡量,即<span><span class="MathJax_Preview">F_\beta</span><script type="math/tex">F_\beta</script></span> Score,公式如下:</p>
<div>
<div class="MathJax_Preview">
F_{\beta}=(1+\beta^2)\times\frac{\text{P}\times\text{R}}{\beta^2\times\text{P}+\text{R}}
</div>
<script type="math/tex; mode=display">
F_{\beta}=(1+\beta^2)\times\frac{\text{P}\times\text{R}}{\beta^2\times\text{P}+\text{R}}
</script>
</div>
<p><span><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>表示权重,</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
F_{\beta}&amp; = \frac{(1+\beta^2)\times\text{P}\times\text{R}}{\beta^2\times\text{P}+\text{R}} \\
&amp; = \frac {1}{\frac{\beta^2}{(1+\beta^2)\times R}+\frac{1}{(1+\beta^2)\times P}}\\ 
&amp; = \frac {1}{\frac{1}{(1+\frac{1}{\beta^2})\times R}+\frac{1}{(1+\beta^2)\times P}} 
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
F_{\beta}& = \frac{(1+\beta^2)\times\text{P}\times\text{R}}{\beta^2\times\text{P}+\text{R}} \\
& = \frac {1}{\frac{\beta^2}{(1+\beta^2)\times R}+\frac{1}{(1+\beta^2)\times P}}\\ 
& = \frac {1}{\frac{1}{(1+\frac{1}{\beta^2})\times R}+\frac{1}{(1+\beta^2)\times P}} 
\end{aligned}
</script>
</div>
<p>当<span><span class="MathJax_Preview">\beta \rightarrow 0 : F_{\beta} \approx P</span><script type="math/tex">\beta \rightarrow 0 : F_{\beta} \approx P</script></span>;当<span><span class="MathJax_Preview">\beta \rightarrow \infty:F_{\beta} \approx R</span><script type="math/tex">\beta \rightarrow \infty:F_{\beta} \approx R</script></span>。通俗的语言就是:<span><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>越大,Recall的权重越大,<span><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>越小,Precision的权重越大。</p>
<p>随着如<span><span class="MathJax_Preview">\beta=1</span><script type="math/tex">\beta=1</script></span>为<span><span class="MathJax_Preview">F_\beta</span><script type="math/tex">F_\beta</script></span>此时Precision和Recall的权重相等,公式如下:</p>
<div>
<div class="MathJax_Preview">
F_{\beta}=F_{1}=\frac{2\times\text{P}\times\text{R}}{\text{P}+\text{R}} 
</div>
<script type="math/tex; mode=display">
F_{\beta}=F_{1}=\frac{2\times\text{P}\times\text{R}}{\text{P}+\text{R}} 
</script>
</div>
<p>由于<span><span class="MathJax_Preview">F_\beta</span><script type="math/tex">F_\beta</script></span> Score无法直观反映数据的情况,同时业务含义相对较弱,实际工作用到的不多。</p>
<ul>
<li><strong>ROC和AUC</strong></li>
</ul>
<p>AUC是一种模型分类指标,且仅仅是二分类模型的评价指标。AUC是Area Under Curve的简称,那么Curve就是ROC(Receiver Operating Characteristic),翻译为"接受者操作特性曲线"。也就是说ROC是一条曲线,AUC是一个面积值。</p>
<p><strong>ROC</strong>:ROC曲线为FPR与TPR之间的关系曲线,这个组合以 FPR 对TPR,即是以代价(costs)对收益(benefits),显然收益越高,代价越低,模型的性能就越好。</p>
<p>x轴为假阳性率(FPR):在所有的负样本中,分类器预测错误的比例</p>
<div>
<div class="MathJax_Preview">
FPR = \frac {FP}{FP+TN}
</div>
<script type="math/tex; mode=display">
FPR = \frac {FP}{FP+TN}
</script>
</div>
<p>y轴为真阳性率(TPR):在所有的正样本中,分类器预测正确的比例(等于Recall)</p>
<div>
<div class="MathJax_Preview">
TPR = \frac {TP}{TP+FN}
</div>
<script type="math/tex; mode=display">
TPR = \frac {TP}{TP+FN}
</script>
</div>
<p>为了更好地理解ROC曲线,我们使用具体的实例来说明:</p>
<p>如在医学诊断的主要任务是尽量把生病的人群都找出来,也就是TPR越高越好。而尽量降低没病误诊为有病的人数,也就是FPR越低越好。不难发现,这两个指标之间是相互制约的。如果某个医生对于有病的症状比较敏感,稍微的小症状都判断为有病,那么他的TPR应该会很高,但是FPR也就相应地变高。最极端的情况下,他把所有的样本都看做有病,那么TPR达到1,FPR也为1。以FPR为横轴,TPR为纵轴,得到如下ROC空间。</p>
<p><img src="http://pic1.zhimg.com/v2-6d498ba1302f3c073c6c8ce1cd014e50_r.jpg" style="width:50%"></p>
<p>可以看出,左上角的点(TPR=1,FPR=0)为完美分类,也就是这个医生医术高明,诊断全对。点A(TPR>FPR),医生A的判断大体是正确的。中线上的点B(TPR=FPR),也就是医生B全都是蒙的,蒙对一半,蒙错一半;下半平面的点C(TPR&lt;FPR),这个医生说你有病,那么你很可能没有病,医生C的话我们要反着听,为真庸医。上图中一个阈值,得到一个点。现在我们需要一个独立于阈值的评价指标来衡量这个医生的医术如何,也就是遍历所有的阈值,得到ROC曲线。</p>
<p>假设下图是某医生的诊断统计图,为未得病人群(上图)和得病人群(下图)的模型输出概率分布图(横坐标表示模型输出概率,纵坐标表示概率对应的人群的数量),显然未得病人群的概率值普遍低于得病人群的输出概率值(即正常人诊断出疾病的概率小于得病人群诊断出疾病的概率)。</p>
<p><img src="http://pic2.zhimg.com/v2-b6a3d08daaf2136ff6a072b90f8fe151_r.jpg" style="width: 50%" ></p>
<p>竖线代表阈值。显然,图中给出了某个阈值对应的混淆矩阵,通过改变不同的阈值<span><span class="MathJax_Preview">1.0 \rightarrow 0</span><script type="math/tex">1.0 \rightarrow 0</script></span>,得到一系列的混淆矩阵,进而得到一系列的TPR和FPR,绘制出ROC曲线。</p>
<p>阈值为1时,不管你什么症状,医生均未诊断出疾病(预测值都为N,此时绿色和红色区域的面积为0,因此FPR=TPR=0,位于左下。随着阈值的减小,红色和绿色区域增大,紫色和蓝色区域减小。阈值为0时,不管你什么症状,医生都诊断结果都是得病(预测值都为P),此时绿色和红色区域均占整个区域,即紫色和蓝色区域的面积为0,此时 FPR=TPR=1,位于右上。</p>
<p><strong>AUC</strong>:AUC值为ROC曲线所覆盖的区域面积,显然,AUC越大,分类器分类效果越好。</p>
<blockquote>
<p>AUC=1,是完美分类器。</p>
<p>0.5&lt;AUC&lt;1,优于随机猜测,有预测价值。</p>
<p>AUC=0.5,跟随机猜测一样(例:丢铜板),没有预测价值。</p>
<p>AUC &lt; 0.5,比随机猜测还差;但只要总是反预测而行,就优于随机猜测。</p>
</blockquote>
<p><strong>注:</strong> 对于AUC小于0.5的模型,我们可以考虑取反(模型预测为positive,那我们就取negtive),这样就可以保证模型的性能不可能比随机猜测差。</p>
<p>以下为ROC曲线和AUC值的实例:</p>
<p><img src="http://pic4.zhimg.com/v2-92a524d44d915a4a043b267c238643b3_r.jpg" style="width:50%" > </p>
<p>AUC的物理意义:正样本的预测结果大于负样本的预测结果的概率。所以AUC反应的是分类器对样本的排序能力。另外值得注意的是,AUC对样本类别是否均衡并不敏感,这也是不均衡样本通常用AUC评价分类器性能的一个原因。AUC只关注正负样本之间的排序,并不关心正样本内部,或者负样本内部的排序。这也体现了AUC的本质:任意个正样本的概率都大于负样本的概率的能力</p>
<p>AUC的计算:</p>
<blockquote>
<p>(1). AUC为ROC曲线下的面积,那我们直接计算面积可得。面积为一个个小的梯形面积(曲线)之和。计算的精度与阈值的精度有关。</p>
<p>(2). 根据AUC的物理意义,我们计算正样本预测结果大于负样本预测结果的概率。取<span><span class="MathJax_Preview">n_1*n_0</span><script type="math/tex">n_1*n_0</script></span>(<span><span class="MathJax_Preview">n_1</span><script type="math/tex">n_1</script></span>为正样本数,<span><span class="MathJax_Preview">n_0</span><script type="math/tex">n_0</script></span>为负样本数)个二元组,比较score(预测结果),最后得到AUC。时间复杂度为<span><span class="MathJax_Preview">O(N*M)</span><script type="math/tex">O(N*M)</script></span>。</p>
<p>(3). 首先把所有样本按照score排序,依次用rank表示他们,如最大score的样本,<span><span class="MathJax_Preview">rank=n(n=n_0+n_1</span><script type="math/tex">rank=n(n=n_0+n_1</script></span>,其中<span><span class="MathJax_Preview">n_0</span><script type="math/tex">n_0</script></span>为负样本个数,<span><span class="MathJax_Preview">n_1</span><script type="math/tex">n_1</script></span>为正样本个数),其次为n-1。对于正样本中rank最大的样本<span><span class="MathJax_Preview">rank\_max</span><script type="math/tex">rank\_max</script></span>,有<span><span class="MathJax_Preview">n_1-1</span><script type="math/tex">n_1-1</script></span>个其他正样本比它小。有<span><span class="MathJax_Preview">rank\_max-n_1</span><script type="math/tex">rank\_max-n_1</script></span>个负样本比它小。其次为<span><span class="MathJax_Preview">rank\_second-(n_1-1)</span><script type="math/tex">rank\_second-(n_1-1)</script></span>。最后我们得到正样本大于负样本的概率为:</p>
<div>
<div class="MathJax_Preview">AUC=\frac{\sum_{\text{正样本}}rank(core)-n_1*(n_1+2)/2}{n_0*n_1}</div>
<script type="math/tex; mode=display">AUC=\frac{\sum_{\text{正样本}}rank(core)-n_1*(n_1+2)/2}{n_0*n_1}</script>
</div>
<p>(4). 计算复杂度为<span><span class="MathJax_Preview">O(N+M)</span><script type="math/tex">O(N+M)</script></span>。</p>
</blockquote>
<p><strong>ROC和AUC都能应用于非均衡的分类问题</strong>:ROC曲线只与横坐标(FPR)和纵坐标(TPR)有关系,其中:</p>
<div>
<div class="MathJax_Preview">FPR = \frac {FP}{FP+TN}</div>
<script type="math/tex; mode=display">FPR = \frac {FP}{FP+TN}</script>
</div>
<div>
<div class="MathJax_Preview">TPR = \frac {TP}{TP+FN}</div>
<script type="math/tex; mode=display">TPR = \frac {TP}{TP+FN}</script>
</div>
<p>以及混淆矩阵:</p>
<p><img alt="" src="http://pic2.zhimg.com/v2-971343041d399b14e4ba379fce0c6d25_r.jpg" /></p>
<p>可以发现TPR只是正样本中(第一行)预测正确的概率,在正样本内部进行,并没有牵扯到负样本。而FPR只是负样本中(第二行)预测错误的概率,在负样本内部进行,并没有牵扯到正样本。TPR和FPR的计算并没有涉及正负样本的互动(也就是没有跨行)。和正负样本的比例没有关系。因此ROC的值与实际的正负样本比例无关,因此既可以用于均衡问题,也可以用于非均衡问题。而AUC的几何意义为ROC曲线下的面积,因此也和实际的正负样本比例无关。</p>
<ul>
<li><strong>KS(Kolmogorov-Smirnov)</strong></li>
</ul>
<p>KS值是在模型中用于区分预测正负样本分隔程度的评价指标,一般应用于金融风控领域。与ROC曲线相似,ROC是以FPR作为横坐标,TPR作为纵坐标,通过改变不同阈值,从而得到ROC曲线。而在KS曲线中,则是以阈值作为横坐标,以FPR和TPR作为纵坐标,ks曲线则为TPR-FPR,ks曲线的最大值通常为ks值。</p>
<p>为什么这样求KS值呢?我们知道,当阈值减小时,TPR和FPR会同时减小,当阈值增大时,TPR和FPR会同时增大。而在实际工程中,我们希望TPR更大一些,FPR更小一些,即TPR-FPR越大越好,即ks值越大越好。KS值的取值范围是[0,1]。通常来说,值越大,模型区分正负样本的能力越强(一般0.3以上,说明模型的效果比较好)。</p>
<p>以下为KS曲线的实例 :</p>
<p><img src="http://pic3.zhimg.com/80/v2-d84017e07e95e3b43f11b04663c3cb1e_r.jpg" style="width: 35%" > </p>
<ul>
<li><strong>micro与macro</strong></li>
</ul>
<p>假如我们有n个二分类混淆矩阵,评价模型通常有两种方式一种叫macro,一种叫micro。</p>
<blockquote>
<p><strong>macro方法</strong></p>
</blockquote>
<p>计算出各混淆矩阵的Recall,Precision,记为<span><span class="MathJax_Preview">(P_1,R_1),(P_2,R_2),\cdots,(P_n,R_n)</span><script type="math/tex">(P_1,R_1),(P_2,R_2),\cdots,(P_n,R_n)</script></span>:</p>
<div>
<div class="MathJax_Preview">P_i = \frac{TP_i}{TP_i + FP_i}</div>
<script type="math/tex; mode=display">P_i = \frac{TP_i}{TP_i + FP_i}</script>
</div>
<div>
<div class="MathJax_Preview">R_i = \frac{TP_i}{TP_i + FN_i}</div>
<script type="math/tex; mode=display">R_i = \frac{TP_i}{TP_i + FN_i}</script>
</div>
<p>对各个混淆矩阵的Recall,Precision求平均,然后再根据求得的Recall,Precision计算F1。</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
&amp; P_{macro} = \frac1n\sum\limits_{i=1}^n P_i \\
&amp; R_{macro} = \frac1n\sum\limits_{i=1}^n R_i \\
&amp; F1_{macro} = \frac{2\times P_{macro} \times R_{macro}}{ P_{macro} + R_{macro}} 
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
& P_{macro} = \frac1n\sum\limits_{i=1}^n P_i \\
& R_{macro} = \frac1n\sum\limits_{i=1}^n R_i \\
& F1_{macro} = \frac{2\times P_{macro} \times R_{macro}}{ P_{macro} + R_{macro}} 
\end{aligned}
</script>
</div>
<blockquote>
<p><strong>micro方法</strong></p>
</blockquote>
<p>将各混淆矩阵对应的元素进行平均,得到平均混淆矩阵:</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
&amp; \overline {TP} = \frac1n\sum\limits_{i=1}^n (TP)_i  \\
&amp; \overline {TN} = \frac1n\sum\limits_{i=1}^n (TN)_i \\
&amp; \overline {FP} = \frac1n\sum\limits_{i=1}^n (FP)_i \\
&amp; \overline {FN} = \frac1n\sum\limits_{i=1}^n (FN)_i 
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
& \overline {TP} = \frac1n\sum\limits_{i=1}^n (TP)_i  \\
& \overline {TN} = \frac1n\sum\limits_{i=1}^n (TN)_i \\
& \overline {FP} = \frac1n\sum\limits_{i=1}^n (FP)_i \\
& \overline {FN} = \frac1n\sum\limits_{i=1}^n (FN)_i 
\end{aligned}
</script>
</div>
<p>再基于平均混淆矩阵计算Recall,Precision,然后再根据求得的Recall,Precision计算F1:</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
    &amp; {P_{micro}} = \frac{\overline {TP}}{\overline{TP} + \overline{FP}} \\
    &amp; {R_{micro}} = \frac{\overline {TP}}{\overline{TP} + \overline{FN}} \\
    &amp; F1_{micro} = \frac{2\times P_{micro} \times R_{micro}}{ P_{micro} + R_{micro}} \\
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
    & {P_{micro}} = \frac{\overline {TP}}{\overline{TP} + \overline{FP}} \\
    & {R_{micro}} = \frac{\overline {TP}}{\overline{TP} + \overline{FN}} \\
    & F1_{micro} = \frac{2\times P_{micro} \times R_{micro}}{ P_{micro} + R_{micro}} \\
\end{aligned}
</script>
</div>
<h3 id="14">1.4 聚类指标</h3>
<ul>
<li><strong>兰德指数</strong></li>
<li><strong>轮廓系数</strong></li>
<li><strong>互信息</strong></li>
</ul>
<h3 id="15">1.5 检测指标</h3>
<ul>
<li><strong>IOU</strong></li>
</ul>
<p>IOU定义了两个bounding box的重叠度,可以说,当算法给出的框(bounding box)和人工标注的框(ground truth)差异很小时,即重叠度很大时,那么算法产生的boundingbox就很准确。</p>
<p>IOU正是表达这种bounding box和ground truth的差异的指标。</p>
<p><img alt="" src="http://pic2.zhimg.com/80/v2-2747aafdf1498dd0807e4b010f1f52b1_1440w.jpg" /></p>
<p>如上图,矩形框A、B的一个重合度IOU计算公式为(交集除以并集): <span><span class="MathJax_Preview">IoU=(A\cap B)/(A\cup B)</span><script type="math/tex">IoU=(A\cap B)/(A\cup B)</script></span></p>
<pre><code class="python">def bb_intersection_over_union(boxA, boxB):
    boxA = [int(x) for x in boxA]
    boxB = [int(x) for x in boxB]

    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])

    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)

    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)
    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)

    iou = interArea / float(boxAArea + boxBArea - interArea)

    return iou
</code></pre>

<ul>
<li><strong>GIou</strong></li>
</ul>
<p>参考论文:<a href="http://arxiv.org/pdf/1902.09630.pdf">Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression</a></p>
<p>IoU是检测任务中最常用的指标,由于IoU是比值的概念,对目标物体的scale是不敏感的。然而检测任务中的BBox的回归损失(MSE loss, l1-smooth loss等)优化和IoU优化不是完全等价的(见下图)。而且<span><span class="MathJax_Preview">L_n</span><script type="math/tex">L_n</script></span>范数对物体的scale也比较敏感。这篇论文提出可以直接把IoU设为回归的loss。然而有个问题是IoU无法直接优化没有重叠的部分,为了解决这个问题这篇paper提出了GIoU的思想。</p>
<p><img alt="" src="http://pic1.zhimg.com/80/v2-9b4e7f33344b9260d0ba903a8c11eeac_1440w.jpg" /></p>
<p>要将IoU设计为损失，主要需要解决两个问题:</p>
<blockquote>
<p>预测值和Ground truth没有重叠的话，IoU始终为0且无法优化</p>
<p>IoU无法辨别不同方式的对齐，比如方向不一致等。</p>
</blockquote>
<p>假如现在有两个任意性质A,B,我们找到一个最小的封闭形状C,让C可以把A,B包含在内,然后我们计算C中没有覆盖A和B的面积占C总面积的比值,然后用A与B的IoU减去这个比值:</p>
<div>
<div class="MathJax_Preview">GIoU=IoU-\frac{|C\(A\cup B)|}{|C|}</div>
<script type="math/tex; mode=display">GIoU=IoU-\frac{|C\(A\cup B)|}{|C|}</script>
</div>
<p>GIoU有如下性质:</p>
<blockquote>
<p>与IoU类似，GIoU也可以作为一个距离，loss可以用 [公式] 来计算
同原始IoU类似，GIoU对物体的大小不敏感
GIoU总是小于等于IoU，对于IoU，有 [公式] ,GIoU则是 [公式] 。在两个形状完全重合时，有 [公式]
由于GIoU引入了包含A，B两个形状的C，所以当A，B不重合时，依然可以进行优化。</p>
</blockquote>
<p>总之就是保留了IoU的原始性质同时弱化了它的缺点,于是论文认为可以将其作为IoU的替代。</p>
<p>我们以2D detecation计算GIoU损失为例:</p>
<p>假设我们现在有预测的Bbox和groud truth的Bbox的坐标，分别记为: <span><span class="MathJax_Preview">B^p=\{x_1^p,y_1^p,x_2^p,y_2^p\},B^g=\{x_1^g,y_1^g,x_2^g,y_2^g\}</span><script type="math/tex">B^p=\{x_1^p,y_1^p,x_2^p,y_2^p\},B^g=\{x_1^g,y_1^g,x_2^g,y_2^g\}</script></span>。注意我们规定对于预测的BBox来说,有<span><span class="MathJax_Preview">x_2^p&gt;x_1^p,y_2^p&gt;y_1^p</span><script type="math/tex">x_2^p>x_1^p,y_2^p>y_1^p</script></span>,主要是为了方便之后点的对应关系。</p>
<blockquote>
<ol>
<li>计算<span><span class="MathJax_Preview">B^g</span><script type="math/tex">B^g</script></span>的面积:<span><span class="MathJax_Preview">A^g=(x_2^g-x_1^g)*(y_2^g-y_1^g)</span><script type="math/tex">A^g=(x_2^g-x_1^g)*(y_2^g-y_1^g)</script></span></li>
<li>计算<span><span class="MathJax_Preview">B^p</span><script type="math/tex">B^p</script></span>的面积:<span><span class="MathJax_Preview">A^p=(x_2^p-x_1^p)*(y_2^p-y_1^p)</span><script type="math/tex">A^p=(x_2^p-x_1^p)*(y_2^p-y_1^p)</script></span></li>
<li>计算<span><span class="MathJax_Preview">B^g,B^p</span><script type="math/tex">B^g,B^p</script></span>的重叠面积:</li>
</ol>
<div>
<div class="MathJax_Preview">x_1^I = max(\hat x_1^p, x_1^g), x_2^I = min(\hat x_2^p, x_2^g) \\ y_1^I = max(\hat y_1^p,y_1^g), x_2^I = min(\hat y_2^p, y_2^g) \\ I = \begin{equation}   \left\{                \begin{array}{**lr**}                (x_2^I-x_1^I)*(y_2^I-y_1^I) &amp;x_2^I&gt;x_1^I, y_2^I&gt;y_1^I\\                0 &amp;otherwise&amp;   \end{array}   \right.   \end{equation}  </div>
<script type="math/tex; mode=display">x_1^I = max(\hat x_1^p, x_1^g), x_2^I = min(\hat x_2^p, x_2^g) \\ y_1^I = max(\hat y_1^p,y_1^g), x_2^I = min(\hat y_2^p, y_2^g) \\ I = \begin{equation}   \left\{                \begin{array}{**lr**}                (x_2^I-x_1^I)*(y_2^I-y_1^I) &x_2^I>x_1^I, y_2^I>y_1^I\\                0 &otherwise&   \end{array}   \right.   \end{equation}  </script>
</div>
<ol>
<li>找到可以包含<span><span class="MathJax_Preview">B^p,B^g</span><script type="math/tex">B^p,B^g</script></span>的最小box<span><span class="MathJax_Preview">B^c</span><script type="math/tex">B^c</script></span></li>
</ol>
<div>
<div class="MathJax_Preview">x_1^c = min(\hat x_1^p, x_1^g), x_2^c = max(\hat x_2^p, x_2^g)\\ y_1^c = min(\hat y_1^p,y_1^g), x_2^c = max(\hat y_2^p, y_2^g) </div>
<script type="math/tex; mode=display">x_1^c = min(\hat x_1^p, x_1^g), x_2^c = max(\hat x_2^p, x_2^g)\\ y_1^c = min(\hat y_1^p,y_1^g), x_2^c = max(\hat y_2^p, y_2^g) </script>
</div>
<ol>
<li>
<p>计算<span><span class="MathJax_Preview">B^c</span><script type="math/tex">B^c</script></span>的面积: <span><span class="MathJax_Preview">A^c = (x_2^c-x_1^c) *(y_2^c-y_1^c)</span><script type="math/tex">A^c = (x_2^c-x_1^c) *(y_2^c-y_1^c)</script></span></p>
</li>
<li>
<p>计算IoU:<span><span class="MathJax_Preview">IoU = \frac{I}{U}=\frac{I}{A^p+A^g - I}</span><script type="math/tex">IoU = \frac{I}{U}=\frac{I}{A^p+A^g - I}</script></span></p>
</li>
<li>
<p>计算<span><span class="MathJax_Preview">GIoU = IoU - \frac{A^c - U}{A^c}</span><script type="math/tex">GIoU = IoU - \frac{A^c - U}{A^c}</script></span></p>
</li>
</ol>
<p>8.计算最终的损失:<span><span class="MathJax_Preview">L_{GIoU} = 1 - GIoU</span><script type="math/tex">L_{GIoU} = 1 - GIoU</script></span></p>
</blockquote>
<pre><code class="python">def Giou(rec1,rec2):
  x1,x2,y1,y2 = rec1 #分别是第一个矩形左右上下的坐标
  x3,x4,y3,y4 = rec2
  iou = Iou(rec1,rec2)
  area_C = (max(x1,x2,x3,x4)-min(x1,x2,x3,x4))*(max(y1,y2,y3,y4)-min(y1,y2,y3,y4))
  area_1 = (x2-x1)*(y1-y2)
  area_2 = (x4-x3)*(y3-y4)
  sum_area = area_1 + area_2
  w1 = x2 - x1#第一个矩形的宽
  w2 = x4 - x3#第二个矩形的宽
  h1 = y1 - y2
  h2 = y3 - y4
  W = min(x1,x2,x3,x4)+w1+w2-max(x1,x2,x3,x4)#交叉部分的宽
  H = min(y1,y2,y3,y4)+h1+h2-max(y1,y2,y3,y4)#交叉部分的高
  Area = W*H#交叉的面积
  add_area = sum_area - Area #两矩形并集的面积
  end_area = (area_C - add_area)/area_C #(c/(AUB))/c的面积
  giou = iou - end_area
  return giou 
</code></pre>

<ul>
<li><strong>NMS</strong></li>
</ul>
<p>非极大值抑制(non maximum suppression,nms)是通常用于目标检测算法,作用是去除重复的区域,就是抑制不是极大值的元素,在这里就是去除和想要的框重叠部分过大的框。</p>
<p>NMS的基本思想是将所有框按得分进行排序,然后无条件保留其中得分最高的框,然后遍历其余框找到和当前最高分的框的重叠面积(IOU)大于一定阈值的框,并删除。然后继续这个过程,找另一个得分高的框,再删除和其IOU大于阈值的框,一直循环直到所有的框都被处理。</p>
<p>在目标检测中,常用非极大值抑制算法(NMS)对生成的大量候选框进行后处理,在faster R-CNN中,每一个bbox都有一个得分,然后使用NMS去除冗余的候选框,得到最具代表性的bbox以加快目标检测的效率。</p>
<p><img alt="" src="http://pic2.zhimg.com/80/v2-322e883d277481276d0af294073662f5_1440w.jpg" /></p>
<p>NMS的具体实现流程为:</p>
<blockquote>
<ol>
<li>根据候选框的类别分类概率(得分),按最高到最低将BBox排序,例如:A&gt;B&gt;C&gt;D&gt;E&gt;F</li>
<li>先标记最大概率矩形框A是要保留下来的,即A的分数最高,则无条件保留</li>
<li>将B~E分别与A求重叠率IoU(两框的交并比),假设B、D与A的IoU大于设定的阈值,那么B和D可以认为是重复标记被剔除</li>
<li>继续从剩下的矩形框C、E、F中选择概率最大的C,标记为要无条件保留下来的框,然后分别计算C与E、F的重叠度,扔掉重叠度超过设定阈值的矩形框</li>
<li>就这样一直重复下去,直到剩下的矩形框没有了,得到所有要保留下来的矩形框</li>
</ol>
</blockquote>
<pre><code class="python">import numpy as np

def non_max_suppression_fast(boxes, overlapThresh):
    &quot;&quot;&quot;
    boxes: boxes为一个m*n的矩阵，m为bbox的个数，n的前4列为每个bbox的坐标，
           格式为（x1,y1,x2,y2），有时会有第5列，该列为每一类的置信
    overlapThresh: 最大允许重叠率
    &quot;&quot;&quot;
    # if there are no boxes, return an empty list
    if len(boxes) == 0:
        return []

    # if the bounding boxes are integers, convert them to floats
    # this is important since we'll be doing a bunch of divisions
    if boxes.dtype.kind == &quot;i&quot;:
        boxes = boxes.astype(&quot;float&quot;)

    # initialize the list of picked indexes
    pick = []

    # grab the coordinates of all bounding boxes respectively
    x1 = boxes[:,0]  # startX
    y1 = boxes[:,1]  # startY
    x2 = boxes[:,2]  # endX
    y2 = boxes[:,3]  # endY
    # probs = boxes[:,4]

    # compute the area of the bounding boxes and sort the bboxes 
    # by the bottom y-coordinate of the bboxes by ascending order
    # and grab the indexes of the sorted coordinates of bboxes
    area = (x2 - x1 + 1) * (y2 - y1 + 1)
    idxs = np.argsort(y2)

    # if probabilities are provided, sort by them instead
    # idxs = np.argsort(probs)

    # keep looping while some indexes still remain in the idxs list
    while len(idxs) &gt; 0:
        # grab the last index in the idxs list (the bottom-right box)
        # and add the index value to the list of picked indexes
        last = len(idxs) - 1
        i = idxs[last]
        pick.append(i)

        # find the largest coordinates for the start of the bbox 
        # and the smallest coordinates for the end of the bbox
        # in the rest of bounding boxes.
        xx1 = np.maximum(x1[i], x1[idxs[:last]])
        yy1 = np.maximum(y1[i], y1[idxs[:last]])
        xx2 = np.minimum(x2[i], x2[idxs[:last]])
        yy2 = np.minimum(y2[i], y2[idxs[:last]])

        # the width and height of the bounding box
        w = np.maximum(0, xx2 - xx1 + 1)
        h = np.maximum(0, yy2 - yy1 + 1)
        # the ratio of overlap in the bounding box
        overlap = (w * h) / area[idxs[:last]]

        # delete all indexes from the index list that overlap is larger than overlapThresh
        idxs = np.delete(idxs, np.concatenate(([last], np.where(overlap &gt; overlapThresh)[0])))

    # return only the bounding boxes that were picked using the
    # integer data type
    return boxes[pick].astype(&quot;int&quot;)
</code></pre>

<ul>
<li><strong>soft_nms</strong></li>
</ul>
<p>这是对nms的改进,它没有删除所有IOU大于阈值的框,而是降低其置信度。</p>
<pre><code class="python">def cpu_soft_nms(np.ndarray[float, ndim=2] boxes, float sigma=0.5, float Nt=0.3, float threshold=0.001, unsigned int method=0):
    cdef unsigned int N = boxes.shape[0]
    cdef float iw, ih, box_area
    cdef float ua
    cdef int pos = 0
    cdef float maxscore = 0
    cdef int maxpos = 0
    cdef float x1,x2,y1,y2,tx1,tx2,ty1,ty2,ts,area,weight,ov

    for i in range(N):　　　　　　　　　　　　　　每次找最大的得分和相应的box
        maxscore = boxes[i, 4]
        maxpos = i

        tx1 = boxes[i,0]
        ty1 = boxes[i,1]
        tx2 = boxes[i,2]
        ty2 = boxes[i,3]
        ts = boxes[i,4]

        pos = i + 1
    # get max box
        while pos &lt; N:
            if maxscore &lt; boxes[pos, 4]:
                maxscore = boxes[pos, 4]
                maxpos = pos
            pos = pos + 1

    # add max box as a detection 
        boxes[i,0] = boxes[maxpos,0]
        boxes[i,1] = boxes[maxpos,1]
        boxes[i,2] = boxes[maxpos,2]
        boxes[i,3] = boxes[maxpos,3]
        boxes[i,4] = boxes[maxpos,4]

    # swap ith box with position of max box　　　　　　把得分最大的放到当前第一个位置
        boxes[maxpos,0] = tx1
        boxes[maxpos,1] = ty1
        boxes[maxpos,2] = tx2
        boxes[maxpos,3] = ty2
        boxes[maxpos,4] = ts

        tx1 = boxes[i,0]
        ty1 = boxes[i,1]
        tx2 = boxes[i,2]
        ty2 = boxes[i,3]
        ts = boxes[i,4]

        pos = i + 1
    # NMS iterations, note that N changes if detection boxes fall below threshold
        while pos &lt; N:　　　　　　　　　　　　　　　　　　　　　　当前第一个，也就是得分最高的一个，和后面所有的box进行nms操作
            x1 = boxes[pos, 0]
            y1 = boxes[pos, 1]
            x2 = boxes[pos, 2]
            y2 = boxes[pos, 3]
            s = boxes[pos, 4]

            area = (x2 - x1 + 1) * (y2 - y1 + 1)
            iw = (min(tx2, x2) - max(tx1, x1) + 1)　　　　　　width的重叠部分长度
            if iw &gt; 0:
                ih = (min(ty2, y2) - max(ty1, y1) + 1)　　　 height的重叠部分长度
                if ih &gt; 0:
                    ua = float((tx2 - tx1 + 1) * (ty2 - ty1 + 1) + area - iw * ih)
                    ov = iw * ih / ua #iou between max box and detection box

                    if method == 1: # linear
                        if ov &gt; Nt: 
                            weight = 1 - ov
                        else:
                            weight = 1
                    elif method == 2: # gaussian
                        weight = np.exp(-(ov * ov)/sigma)
                    else: # original NMS
                        if ov &gt; Nt: 
                            weight = 0
                        else:
                            weight = 1

                    boxes[pos, 4] = weight*boxes[pos, 4]

            # if box score falls below threshold, discard the box by swapping with last box
            # update N
                    if boxes[pos, 4] &lt; threshold:
                        boxes[pos,0] = boxes[N-1, 0]
                        boxes[pos,1] = boxes[N-1, 1]
                        boxes[pos,2] = boxes[N-1, 2]
                        boxes[pos,3] = boxes[N-1, 3]
                        boxes[pos,4] = boxes[N-1, 4]
                        N = N - 1
                        pos = pos - 1

            pos = pos + 1

    keep = [i for i in range(N)]
    return keep
</code></pre>

<ul>
<li><strong>MSER</strong></li>
</ul>
<p>MSER全称叫做最大稳定极值区域(MSER-Maximally Stable Extremal Regions),是一种检测图像中文本区域的传统图像算法,主要是基于分水岭的思想来对图像进行斑点(blob)区域检测。</p>
<p>就准确率来说,MSER对文本区域的检测效果是不能和深度学习如CTPN、Pixellink等相比的,但是可以用于自然场景的文本检测的前期阶段,产生尽可能多的proposals。</p>
<p>MSER对灰度图像取阈值进行二值化处理,阈值从0到255依次进行递增，阈值的递增类似于分水岭算法中的水平面的上升,随着水平面的上升,有一些山谷和较矮的丘陵会被淹没，如果从天空往下看，则整个区域被分为陆地和水域两个部分，这类似于二值图像。图像中灰度值的不同就对应地势高低的不同，每个阈值都都会生成一个二值图。</p>
<p>随着阈值的增加，首先会看到一个全白图像，然后出现小黑点，随着阈值的增加，黑色部分会逐渐增大， 这些黑色区域最终会融合，直到整个图像变成黑色。在得到的所有二值图像中，图像中的某些连通区域变化很小，甚至没有变化，则该区域就被称为最大稳定极值区域。</p>
<p><img alt="" src="http://pic1.zhimg.com/80/v2-37d31b9bdbf2e03a9260102578f94bcc_1440w.jpg" /></p>
<p>在一幅含有文字的图像上，由于文字区域的灰度值是一致的，而且和文字周边像素的灰度值差别较大，因此在水平面（阈值）持续增长的一段时间内它们都不会被覆盖，直到阈值涨到文字本身的灰度值时才会被淹没，所以文字区域可以作为最大稳定极值区域。</p>
<p>所以如果一个区域在给定的阈值范围内保持其形状和大小基本稳定不变，而不会与其他区域合并，该区域被认为是稳定的。</p>
<p>上述做法只能检测出灰度图像的黑色区域，不能检测出白色区域，因此还需要对原图进行反转（负片图像），然后再进行阈值从0～255递增的二值化处理过程。这两种操作分别被称为MSER+和MSER-。</p>
<p>MSER 的公式如下:</p>
<div>
<div class="MathJax_Preview">q(i)=\dfrac {|R(i + Δ)-R(i)|}{|R(i)|}</div>
<script type="math/tex; mode=display">q(i)=\dfrac {|R(i + Δ)-R(i)|}{|R(i)|}</script>
</div>
<p><span><span class="MathJax_Preview">R(i)</span><script type="math/tex">R(i)</script></span>表示阈值为i时的某一连通区域,<span><span class="MathJax_Preview">Δ</span><script type="math/tex">Δ</script></span>为灰度阈值的微小增加量,<span><span class="MathJax_Preview">q(i)</span><script type="math/tex">q(i)</script></span>为阈值是i时的区域<span><span class="MathJax_Preview">R(i)</span><script type="math/tex">R(i)</script></span>的变化率,<span><span class="MathJax_Preview">|R(i)|</span><script type="math/tex">|R(i)|</script></span>表示区域<span><span class="MathJax_Preview">R(i)</span><script type="math/tex">R(i)</script></span>的面积。当<span><span class="MathJax_Preview">q(i)</span><script type="math/tex">q(i)</script></span>为局部极小值时,说明<span><span class="MathJax_Preview">R(i)</span><script type="math/tex">R(i)</script></span>区域的变化非常小,则<span><span class="MathJax_Preview">R(i)</span><script type="math/tex">R(i)</script></span>为可以被认为是最大稳定极值区域。</p>
<p>该算法可以用来粗略地寻找图像中的文字区域，虽然算法思想简单，但要做到效果又快又好还是需要一定基础的，OpenCV直接提供了该算法的接口。</p>
<pre><code class="python">'''
MSER+NMS实现图像文本区域检测
'''
path = '/home/zxd/Downloads/co120110121029-12.jpg'
img = cv2.imread(path)
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # 灰度图
vis_1 = img.copy()
vis_2 = img.copy()

# get mser object
mser = cv2.MSER_create(_delta=5, _min_area=10, _max_variation=0.5)
# Detect MSER regions
regions, boxes = mser.detectRegions(gray)

# 绘制文本区域（不规则轮廓）
hulls = [cv2.convexHull(p.reshape(-1, 1, 2)) for p in regions]  
cv2.polylines(img, hulls, 1, (0, 255, 0), 1)

# 两种绘制矩形轮廓
# for box in boxes:
#     x, y, w, h = box
#     cv2.rectangle(vis_1, (x,y),(x+w, y+h), (255, 0, 0), 1)

keep = []
for hull in hulls:
    x, y, w, h = cv2.boundingRect(hull)
    keep.append([x, y, x + w, y + h])
    cv2.rectangle(vis_1, (x, y), (x + w, y + h), (255, 0, 0), 1)
print(&quot;%d bounding boxes before nms&quot; % (len(keep)))

# 使用非极大值抑制获取不重复的矩形框
pick = non_max_suppression_fast(np.array(keep), overlapThresh=0.4)
print(&quot;%d bounding boxes after nms&quot; % (len(pick)))

# loop over the picked bounding boxes and draw them
for (startX, startY, endX, endY) in pick:
    cv2.rectangle(vis_2, (startX, startY), (endX, endY), (0, 0, 255), 1)

# 合并图片
boxing_list = [img, vis_1, vis_2]
boxing = np.concatenate(boxing_list, axis=0)

plt.figure(figsize=(10, 20))  # w,h
plt.imshow(boxing, cmap='gray')
plt.show()

text_mask = np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)
for contour in hulls:
    cv2.drawContours(text_mask, [contour], -1, (255, 255, 255), -1)

img = cv2.imread(path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
text_region = cv2.bitwise_and(img, img, mask=mask)
</code></pre>

<h2 id="_3">二、基本结构</h2>
<h3 id="21">2.1 卷积类型</h3>
<p>https://www.zhihu.com/question/312556066/answer/600228264</p>
<p>https://www.zhihu.com/question/291032522/answer/605843215</p>
<p>https://cloud.tencent.com/developer/article/1352583</p>
<p>https://zhuanlan.zhihu.com/p/44106492</p>
<h2 id="_4">三、经典网络</h2>
<h3 id="31-lenet">3.1 lenet网络</h3>
<p>卷积神经网络的开山之作,麻雀虽小五脏俱全,卷积层、池化层、全链接层一直沿用至今。</p>
<p><img alt="" src="http://cuijiahua.com/wp-content/uploads/2018/01/dl_3_4.jpg" /></p>
<pre><code class="python"># Contains a variant of the LeNet model definition.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow.compat.v1 as tf
import tf_slim as slim

def lenet(images, num_classes=10, is_training=False, dropout_keep_prob=0.5,
          prediction_fn=slim.softmax, scope='LeNet'):
    &quot;&quot;&quot;
    Creates a variant of the LeNet model.

    Note that since the output is a set of 'logits', the values fall in the
    interval of (-infinity, infinity). Consequently, to convert the outputs to a
    probability distribution over the characters, one will need to convert them
    using the softmax function:

        logits = lenet.lenet(images, is_training=False)
        probabilities = tf.nn.softmax(logits)
        predictions = tf.argmax(logits, 1)

    Args:
        images: A batch of `Tensors` of size [batch_size, height, width, channels].
        num_classes: the number of classes in the dataset. If 0 or None, the logits
            layer is omitted and the input features to the logits layer are returned instead.
        is_training: specifies whether or not we're currently training the model.
            This variable will determine the behaviour of the dropout layer.
        dropout_keep_prob: the percentage of activation values that are retained.
        prediction_fn: a function to get predictions out of logits.
        scope: Optional variable_scope.
    Returns:
        net: a 2D Tensor with the logits (pre-softmax activations) if num_classes
            is a non-zero integer, or the inon-dropped-out nput to the logits layer
            if num_classes is 0 or None.
        end_points: a dictionary from components of the network to the corresponding
            activation.
    &quot;&quot;&quot;
    end_points = {}

    with tf.variable_scope(scope, 'LeNet', [images]):
        net = end_points['conv1'] = slim.conv2d(images, 32, [5, 5], scope='conv1')
        net = end_points['pool1'] = slim.max_pool2d(net, [2, 2], 2, scope='pool1')
        net = end_points['conv2'] = slim.conv2d(net, 64, [5, 5], scope='conv2')
        net = end_points['pool2'] = slim.max_pool2d(net, [2, 2], 2, scope='pool2')
        net = slim.flatten(net)
        end_points['Flatten'] = net

        net = end_points['fc3'] = slim.fully_connected(net, 1024, scope='fc3')
        if not num_classes:
            return net, end_points
        net = end_points['dropout3'] = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='dropout3')
        logits = end_points['Logits'] = slim.fully_connected(net, num_classes, activation_fn=None, scope='fc4')

    end_points['Predictions'] = prediction_fn(logits, scope='Predictions')
    return logits, end_points

lenet.default_image_size = 28
def lenet_arg_scope(weight_decay=0.0):
    &quot;&quot;&quot;
    Defines the default lenet argument scope.

    Args:
        weight_decay: The weight decay to use for regularizing the model.

    Returns:
        An `arg_scope` to use for the inception v3 model.
    &quot;&quot;&quot;
    with slim.arg_scope([slim.conv2d, slim.fully_connected],
        weights_regularizer=slim.l2_regularizer(weight_decay),
        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),
        activation_fn=tf.nn.relu) as sc:
    return sc
</code></pre>

<h3 id="alexnet">Alexnet网络</h3>
<p><img alt="" src="http://pic2.zhimg.com/80/v2-5093ccb6ce6313c673c9890dc3cb47d9_1440w.png" /></p>
<p>AlexNet主要使用到的新技术点如下:</p>
<blockquote>
<ol>
<li>成功使用ReLU作为CNN的激活函数,并验证其效果在较深的网络超过了Sigmoid,成功解决了Sigmoid在网络较深时的梯度弥散问题。虽然ReLU激活函数在很久之前就被提出了,但是直到AlexNet的出现才将其发扬光大。</li>
<li>训练时使用Dropout随机忽略一部分神经元,以避免模型过拟合。Dropout虽有单独的论文论述,但是AlexNet将其实用化,通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout。</li>
<li>在CNN中使用重叠的最大池化。此前CNN中普遍使用平均池化,AlexNet全部使用最大池化,避免平均池化的模糊化效果。并且AlexNet中提出让步长比池化核的尺寸小,这样池化层的输出之间会有重叠和覆盖,提升了特征的丰富性。</li>
<li>提出了LRN层,对局部神经元的活动创建竞争机制,使得其中响应比较大的值变得相对更大,并抑制其他反馈较小的神经元,增强了模型的泛化能力。</li>
</ol>
</blockquote>
<pre><code class="python"># -*- coding:utf8 -*-
&quot;&quot;&quot;
Contains a model definition for AlexNet.

This work was first described in:
    ImageNet Classification with Deep Convolutional Neural Networks
    Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton

and later refined in:
  One weird trick for parallelizing convolutional neural networks
  Alex Krizhevsky, 2014

Here we provide the implementation proposed in &quot;One weird trick&quot; and not
&quot;ImageNet Classification&quot;, as per the paper, the LRN layers have been removed.

Usage:
    with slim.arg_scope(alexnet.alexnet_v2_arg_scope()):
        outputs, end_points = alexnet.alexnet_v2(inputs)
&quot;&quot;&quot;
import tensorflow.compat.v1 as tf
import tf_slim as slim

# pylint: disable=g-long-lambda
trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)


def alexnet_v2_arg_scope(weight_decay=0.0005):
  with slim.arg_scope([slim.conv2d, slim.fully_connected],
                      activation_fn=tf.nn.relu,
                      biases_initializer=tf.constant_initializer(0.1),
                      weights_regularizer=slim.l2_regularizer(weight_decay)):
    with slim.arg_scope([slim.conv2d], padding='SAME'):
      with slim.arg_scope([slim.max_pool2d], padding='VALID') as arg_sc:
        return arg_sc


def alexnet_v2(inputs,num_classes=1000, is_training=True,dropout_keep_prob=0.5,
        spatial_squeeze=True,scope='alexnet_v2',global_pool=False):
    &quot;&quot;&quot;
    AlexNet version 2.

    Described in: http://arxiv.org/pdf/1404.5997v2.pdf
    Parameters from: github.com/akrizhevsky/cuda-convnet2/blob/master/layers/layers-imagenet-1gpu.cfg

    Note: All the fully_connected layers have been transformed to conv2d layers.
          To use in classification mode, resize input to 224x224 or set
          global_pool=True. To use in fully convolutional mode, set
          spatial_squeeze to false.
          The LRN layers have been removed and change the initializers from
          random_normal_initializer to xavier_initializer.

    Args:
        inputs: a tensor of size [batch_size, height, width, channels].
        num_classes: the number of predicted classes. If 0 or None, the logits layer
            is omitted and the input features to the logits layer are returned instead.
        is_training: whether or not the model is being trained.
        dropout_keep_prob: the probability that activations are kept in the dropout
            layers during training.
        spatial_squeeze: whether or not should squeeze the spatial dimensions of the
            logits. Useful to remove unnecessary dimensions for classification.
        scope: Optional scope for the variables.
        global_pool: Optional boolean flag. If True, the input to the classification
            layer is avgpooled to size 1x1, for any input size. (This is not part
            of the original AlexNet.)

    Returns:
        net: the output of the logits layer (if num_classes is a non-zero integer),
            or the non-dropped-out input to the logits layer (if num_classes is 0
            or None).
        end_points: a dict of tensors with intermediate activations.
    &quot;&quot;&quot;
    with tf.variable_scope(scope, 'alexnet_v2', [inputs]) as sc:
        end_points_collection = sc.original_name_scope + '_end_points'
        # Collect outputs for conv2d, fully_connected and max_pool2d.
        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],outputs_collections=[end_points_collection]):
            net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID',scope='conv1')
            net = slim.max_pool2d(net, [3, 3], 2, scope='pool1')
            net = slim.conv2d(net, 192, [5, 5], scope='conv2')
            net = slim.max_pool2d(net, [3, 3], 2, scope='pool2')
            net = slim.conv2d(net, 384, [3, 3], scope='conv3')
            net = slim.conv2d(net, 384, [3, 3], scope='conv4')
            net = slim.conv2d(net, 256, [3, 3], scope='conv5')
            net = slim.max_pool2d(net, [3, 3], 2, scope='pool5')

            # Use conv2d instead of fully_connected layers.
            with slim.arg_scope([slim.conv2d],weights_initializer=trunc_normal(0.005),biases_initializer=tf.constant_initializer(0.1)):
                net = slim.conv2d(net, 4096, [5, 5], padding='VALID',scope='fc6')
                net = slim.dropout(net, dropout_keep_prob, is_training=is_training,scope='dropout6')
                net = slim.conv2d(net, 4096, [1, 1], scope='fc7')
                # Convert end_points_collection into a end_point dict.
                end_points = slim.utils.convert_collection_to_dict(end_points_collection)
                if global_pool:
                    net = tf.reduce_mean(input_tensor=net, axis=[1, 2], keepdims=True, name='global_pool')
                    end_points['global_pool'] = net
                if num_classes:
                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,scope='dropout7')
                    net = slim.conv2d(net,num_classes, [1, 1],activation_fn=None,normalizer_fn=None,biases_initializer=tf.zeros_initializer(),scope='fc8')
                if spatial_squeeze:
                    net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
                end_points[sc.name + '/fc8'] = net
            return net, end_points
alexnet_v2.default_image_size = 224
</code></pre>

<h3 id="vgg">VGG网络</h3>
<p>VGG优点:</p>
<blockquote>
<p>VGGNet的结构非常简洁,整个网络都使用了同样大小的卷积核尺寸(3x3)和最大池化尺寸(2x2)。</p>
<p>几个小滤波器(3x3)卷积层的组合比一个大滤波器(5x5或7x7)卷积层好</p>
<p>验证了通过不断加深网络结构可以提升性能。</p>
</blockquote>
<p>VGG缺点</p>
<blockquote>
<p>VGG耗费更多计算资源,并且使用了更多的参数(这里不是3x3卷积的锅)m导致更多的内存占用(140M)。其中绝大多数的参数都是来自于第一个全连接层。VGG可是有3个全连接层啊!</p>
</blockquote>
<p><img alt="" src="http://pic4.zhimg.com/80/v2-ea924e733676e0da534f677a97c98653_1440w.jpg" /></p>
<pre><code class="python"># -*- coding:utf8 -*-
&quot;&quot;&quot;
Contains model definitions for versions of the Oxford VGG network.

These model definitions were introduced in the following technical report:

  Very Deep Convolutional Networks For Large-Scale Image Recognition
  Karen Simonyan and Andrew Zisserman
  PDF: http://arxiv.org/pdf/1409.1556.pdf
  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf

More information can be obtained from the VGG website:
    www.robots.ox.ac.uk/~vgg/research/very_deep/

Usage:
    with slim.arg_scope(vgg.vgg_arg_scope()):
        outputs, end_points = vgg.vgg_a(inputs)

    with slim.arg_scope(vgg.vgg_arg_scope()):
        outputs, end_points = vgg.vgg_16(inputs)
&quot;&quot;&quot;
import tensorflow.compat.v1 as tf
import tf_slim as slim


def vgg_arg_scope(weight_decay=0.0005):
    &quot;&quot;&quot;
    Defines the VGG arg scope.
    Args:
        weight_decay: The l2 regularization coefficient.
    Returns:
        An arg_scope.
    &quot;&quot;&quot;
    with slim.arg_scope([slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu,
                      weights_regularizer=slim.l2_regularizer(weight_decay),
                      biases_initializer=tf.zeros_initializer()):
        with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc:
            return arg_sc


def vgg_a(inputs,num_classes=1000, is_training=True,dropout_keep_prob=0.5,spatial_squeeze=True,reuse=None,
        scope='vgg_a',fc_conv_padding='VALID',global_pool=False):
    &quot;&quot;&quot;
    Oxford Net VGG 11-Layers version A Example.

    Note: All the fully_connected layers have been transformed to conv2d layers.
            To use in classification mode, resize input to 224x224.

    Args:
        inputs: a tensor of size [batch_size, height, width, channels].
        num_classes: number of predicted classes. If 0 or None, the logits layer is
            omitted and the input features to the logits layer are returned instead.
        is_training: whether or not the model is being trained.
        dropout_keep_prob: the probability that activations are kept in the dropout
            layers during training.
        spatial_squeeze: whether or not should squeeze the spatial dimensions of the
            outputs. Useful to remove unnecessary dimensions for classification.
        reuse: whether or not the network and its variables should be reused. To be
            able to reuse 'scope' must be given.
        scope: Optional scope for the variables.
        fc_conv_padding: the type of padding to use for the fully connected layer
            that is implemented as a convolutional layer. Use 'SAME' padding if you
            are applying the network in a fully convolutional manner and want to
            get a prediction map downsampled by a factor of 32 as an output.
            Otherwise, the output prediction map will be (input / 32) - 6 in case of
            'VALID' padding.
        global_pool: Optional boolean flag. If True, the input to the classification
            layer is avgpooled to size 1x1, for any input size. (This is not part
            of the original VGG architecture.)

    Returns:
        net: the output of the logits layer (if num_classes is a non-zero integer),
            or the input to the logits layer (if num_classes is 0 or None).
        end_points: a dict of tensors with intermediate activations.
    &quot;&quot;&quot;
    with tf.variable_scope(scope, 'vgg_a', [inputs], reuse=reuse) as sc:
        end_points_collection = sc.original_name_scope + '_end_points'
        # Collect outputs for conv2d, fully_connected and max_pool2d.
        with slim.arg_scope([slim.conv2d, slim.max_pool2d],outputs_collections=end_points_collection):
            net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope='conv1')
            net = slim.max_pool2d(net, [2, 2], scope='pool1')
            net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope='conv2')
            net = slim.max_pool2d(net, [2, 2], scope='pool2')
            net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope='conv3')
            net = slim.max_pool2d(net, [2, 2], scope='pool3')
            net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope='conv4')
            net = slim.max_pool2d(net, [2, 2], scope='pool4')
            net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope='conv5')
            net = slim.max_pool2d(net, [2, 2], scope='pool5')

            # Use conv2d instead of fully_connected layers.
            net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')
            net = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='dropout6')
            net = slim.conv2d(net, 4096, [1, 1], scope='fc7')
            # Convert end_points_collection into a end_point dict.
            end_points = slim.utils.convert_collection_to_dict(end_points_collection)
            if global_pool:
                net = tf.reduce_mean(input_tensor=net, axis=[1, 2], keepdims=True, name='global_pool')
                end_points['global_pool'] = net
            if num_classes:
                net = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='dropout7')
                net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,normalizer_fn=None, scope='fc8')
                if spatial_squeeze:
                    net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
                end_points[sc.name + '/fc8'] = net
            return net, end_points
vgg_a.default_image_size = 224

def vgg_16(inputs,num_classes=1000,is_training=True,dropout_keep_prob=0.5,spatial_squeeze=True,
        reuse=None,scope='vgg_16',fc_conv_padding='VALID',global_pool=False):
    &quot;&quot;&quot;
    Oxford Net VGG 16-Layers version D Example.

    Note: All the fully_connected layers have been transformed to conv2d layers.
        To use in classification mode, resize input to 224x224.

    Args:
        inputs: a tensor of size [batch_size, height, width, channels].
        num_classes: number of predicted classes. If 0 or None, the logits layer is
            omitted and the input features to the logits layer are returned instead.
        is_training: whether or not the model is being trained.
        dropout_keep_prob: the probability that activations are kept in the dropout
            layers during training.
        spatial_squeeze: whether or not should squeeze the spatial dimensions of the
            outputs. Useful to remove unnecessary dimensions for classification.
        reuse: whether or not the network and its variables should be reused. To be
            able to reuse 'scope' must be given.
        scope: Optional scope for the variables.
        fc_conv_padding: the type of padding to use for the fully connected layer
            that is implemented as a convolutional layer. Use 'SAME' padding if you
            are applying the network in a fully convolutional manner and want to
            get a prediction map downsampled by a factor of 32 as an output.
            Otherwise, the output prediction map will be (input / 32) - 6 in case of
            'VALID' padding.
        global_pool: Optional boolean flag. If True, the input to the classification
            layer is avgpooled to size 1x1, for any input size. (This is not part
            of the original VGG architecture.)

    Returns:
        net: the output of the logits layer (if num_classes is a non-zero integer),
            or the input to the logits layer (if num_classes is 0 or None).
        end_points: a dict of tensors with intermediate activations.
    &quot;&quot;&quot;
    with tf.variable_scope(scope, 'vgg_16', [inputs], reuse=reuse) as sc:
        end_points_collection = sc.original_name_scope + '_end_points'
        # Collect outputs for conv2d, fully_connected and max_pool2d.
        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d], outputs_collections=end_points_collection):
            net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')
            net = slim.max_pool2d(net, [2, 2], scope='pool1')
            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')
            net = slim.max_pool2d(net, [2, 2], scope='pool2')
            net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')
            net = slim.max_pool2d(net, [2, 2], scope='pool3')
            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')
            net = slim.max_pool2d(net, [2, 2], scope='pool4')
            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')
            net = slim.max_pool2d(net, [2, 2], scope='pool5')

            # Use conv2d instead of fully_connected layers.
            net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')
            net = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='dropout6')
            net = slim.conv2d(net, 4096, [1, 1], scope='fc7')
            # Convert end_points_collection into a end_point dict.
            end_points = slim.utils.convert_collection_to_dict(end_points_collection)
            if global_pool:
                net = tf.reduce_mean(input_tensor=net, axis=[1, 2], keepdims=True, name='global_pool')
                end_points['global_pool'] = net
            if num_classes:
                net = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='dropout7')
                net = slim.conv2d(net, num_classes, [1, 1],activation_fn=None,normalizer_fn=None,scope='fc8')
                if spatial_squeeze:
                    net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
                end_points[sc.name + '/fc8'] = net
            return net, end_points
vgg_16.default_image_size = 224


def vgg_19(inputs,num_classes=1000,is_training=True,dropout_keep_prob=0.5,spatial_squeeze=True,
        reuse=None,scope='vgg_19',fc_conv_padding='VALID',global_pool=False):
    &quot;&quot;&quot;
    Oxford Net VGG 19-Layers version E Example.

    Note: All the fully_connected layers have been transformed to conv2d layers.
                To use in classification mode, resize input to 224x224.

    Args:
        inputs: a tensor of size [batch_size, height, width, channels].
        num_classes: number of predicted classes. If 0 or None, the logits layer is
            omitted and the input features to the logits layer are returned instead.
        is_training: whether or not the model is being trained.
        dropout_keep_prob: the probability that activations are kept in the dropout
            layers during training.
        spatial_squeeze: whether or not should squeeze the spatial dimensions of the
            outputs. Useful to remove unnecessary dimensions for classification.
        reuse: whether or not the network and its variables should be reused. To be
            able to reuse 'scope' must be given.
        scope: Optional scope for the variables.
        fc_conv_padding: the type of padding to use for the fully connected layer
            that is implemented as a convolutional layer. Use 'SAME' padding if you
            are applying the network in a fully convolutional manner and want to
            get a prediction map downsampled by a factor of 32 as an output.
            Otherwise, the output prediction map will be (input / 32) - 6 in case of
            'VALID' padding.
        global_pool: Optional boolean flag. If True, the input to the classification
            layer is avgpooled to size 1x1, for any input size. (This is not part
            of the original VGG architecture.)

    Returns:
        net: the output of the logits layer (if num_classes is a non-zero integer),
            or the non-dropped-out input to the logits layer (if num_classes is 0 or
            None).
        end_points: a dict of tensors with intermediate activations.
    &quot;&quot;&quot;
    with tf.variable_scope(scope, 'vgg_19', [inputs], reuse=reuse) as sc:
        end_points_collection = sc.original_name_scope + '_end_points'
        # Collect outputs for conv2d, fully_connected and max_pool2d.
        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],outputs_collections=end_points_collection):
            net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')
            net = slim.max_pool2d(net, [2, 2], scope='pool1')
            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')
            net = slim.max_pool2d(net, [2, 2], scope='pool2')
            net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope='conv3')
            net = slim.max_pool2d(net, [2, 2], scope='pool3')
            net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope='conv4')
            net = slim.max_pool2d(net, [2, 2], scope='pool4')
            net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope='conv5')
            net = slim.max_pool2d(net, [2, 2], scope='pool5')

            # Use conv2d instead of fully_connected layers.
            net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')
            net = slim.dropout(net, dropout_keep_prob, is_training=is_training,scope='dropout6')
            net = slim.conv2d(net, 4096, [1, 1], scope='fc7')
            # Convert end_points_collection into a end_point dict.
            end_points = slim.utils.convert_collection_to_dict(end_points_collection)
            if global_pool:
                net = tf.reduce_mean(
                        input_tensor=net, axis=[1, 2], keepdims=True, name='global_pool')
                end_points['global_pool'] = net
            if num_classes:
                net = slim.dropout(net, dropout_keep_prob, is_training=is_training,scope='dropout7')
                net = slim.conv2d(net, num_classes, [1, 1],activation_fn=None,normalizer_fn=None,scope='fc8')
                if spatial_squeeze:
                    net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
                end_points[sc.name + '/fc8'] = net
            return net, end_points
vgg_19.default_image_size = 224

# Alias
vgg_d = vgg_16
vgg_e = vgg_19
</code></pre>

  <br>
    <style>
blockquote{
    font-size: 99%;
}
</style>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  CommonHTML: {
    scale: 100
  }
});
</script>
<script type="text/javascript" async
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    
    
      
    

    <br>
</div>

</body>
</html>